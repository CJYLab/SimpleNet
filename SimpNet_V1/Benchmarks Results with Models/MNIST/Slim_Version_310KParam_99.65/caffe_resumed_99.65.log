
G:\Caffe\examples\mnist>REM go to the caffe root 

G:\Caffe\examples\mnist>cd ../../ 

G:\Caffe>set BIN=build/x64/Release 

G:\Caffe>"build/x64/Release/caffe.exe" train --solver=examples/mnist/lenet_solver.prototxt --weights=examples/mnist/snaps/lenet_iter_16800.caffemodel 
I0312 12:42:14.841737  3948 caffe.cpp:218] Using GPUs 0
I0312 12:42:15.036084  3948 caffe.cpp:223] GPU 0: GeForce GTX 980
I0312 12:42:15.701035  3948 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:15.701035  3948 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 600
base_lr: 0.01983656
display: 100
max_iter: 800000
lr_policy: "poly"
power: 1
momentum: 0.55
weight_decay: 0.0001
snapshot: 600
snapshot_prefix: "examples/mnist/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I0312 12:42:15.701539  3948 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0312 12:42:15.702038  3948 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I0312 12:42:15.702038  3948 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I0312 12:42:15.702558  3948 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0312 12:42:15.702558  3948 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0312 12:42:15.703057  3948 layer_factory.cpp:58] Creating layer mnist
I0312 12:42:15.703537  3948 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:15.704538  3948 net.cpp:100] Creating Layer mnist
I0312 12:42:15.704538  3948 net.cpp:408] mnist -> data
I0312 12:42:15.704538  3948 net.cpp:408] mnist -> label
I0312 12:42:15.706037 11968 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:15.708036 11968 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_train_lmdb_norm2
I0312 12:42:15.788034  3948 data_layer.cpp:41] output data size: 100,1,28,28
I0312 12:42:15.794535  3948 net.cpp:150] Setting up mnist
I0312 12:42:15.794535  3948 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0312 12:42:15.794535  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:15.794535  3948 net.cpp:165] Memory required for data: 314000
I0312 12:42:15.794535  3948 layer_factory.cpp:58] Creating layer label_mnist_1_split
I0312 12:42:15.794535  3948 net.cpp:100] Creating Layer label_mnist_1_split
I0312 12:42:15.794535  3948 net.cpp:434] label_mnist_1_split <- label
I0312 12:42:15.794535  3948 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0312 12:42:15.794535  3948 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0312 12:42:15.795035  3948 net.cpp:150] Setting up label_mnist_1_split
I0312 12:42:15.795035  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:15.795035  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:15.795035  3948 net.cpp:165] Memory required for data: 314800
I0312 12:42:15.795035  3948 layer_factory.cpp:58] Creating layer conv1
I0312 12:42:15.795035  3948 net.cpp:100] Creating Layer conv1
I0312 12:42:15.795035  3948 net.cpp:434] conv1 <- data
I0312 12:42:15.795035  3948 net.cpp:408] conv1 -> conv1
I0312 12:42:15.796036  7704 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:16.521739  3948 net.cpp:150] Setting up conv1
I0312 12:42:16.521739  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.521739  3948 net.cpp:165] Memory required for data: 20385200
I0312 12:42:16.521739  3948 layer_factory.cpp:58] Creating layer bn1
I0312 12:42:16.521739  3948 net.cpp:100] Creating Layer bn1
I0312 12:42:16.521739  3948 net.cpp:434] bn1 <- conv1
I0312 12:42:16.521739  3948 net.cpp:408] bn1 -> bn1
I0312 12:42:16.521739  3948 net.cpp:150] Setting up bn1
I0312 12:42:16.521739  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.521739  3948 net.cpp:165] Memory required for data: 40455600
I0312 12:42:16.521739  3948 layer_factory.cpp:58] Creating layer scale1
I0312 12:42:16.521739  3948 net.cpp:100] Creating Layer scale1
I0312 12:42:16.521739  3948 net.cpp:434] scale1 <- bn1
I0312 12:42:16.521739  3948 net.cpp:408] scale1 -> scale1
I0312 12:42:16.521739  3948 layer_factory.cpp:58] Creating layer scale1
I0312 12:42:16.521739  3948 net.cpp:150] Setting up scale1
I0312 12:42:16.521739  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.521739  3948 net.cpp:165] Memory required for data: 60526000
I0312 12:42:16.521739  3948 layer_factory.cpp:58] Creating layer relu1
I0312 12:42:16.521739  3948 net.cpp:100] Creating Layer relu1
I0312 12:42:16.521739  3948 net.cpp:434] relu1 <- scale1
I0312 12:42:16.521739  3948 net.cpp:408] relu1 -> relu1
I0312 12:42:16.531718  3948 net.cpp:150] Setting up relu1
I0312 12:42:16.531718  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.531718  3948 net.cpp:165] Memory required for data: 80596400
I0312 12:42:16.531718  3948 layer_factory.cpp:58] Creating layer conv1_0
I0312 12:42:16.531718  3948 net.cpp:100] Creating Layer conv1_0
I0312 12:42:16.531718  3948 net.cpp:434] conv1_0 <- relu1
I0312 12:42:16.531718  3948 net.cpp:408] conv1_0 -> conv1_0
I0312 12:42:16.541735  3948 net.cpp:150] Setting up conv1_0
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 90631600
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer bn1_0
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer bn1_0
I0312 12:42:16.541735  3948 net.cpp:434] bn1_0 <- conv1_0
I0312 12:42:16.541735  3948 net.cpp:408] bn1_0 -> bn1_0
I0312 12:42:16.541735  3948 net.cpp:150] Setting up bn1_0
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 100666800
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale1_0
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer scale1_0
I0312 12:42:16.541735  3948 net.cpp:434] scale1_0 <- bn1_0
I0312 12:42:16.541735  3948 net.cpp:408] scale1_0 -> scale1_0
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale1_0
I0312 12:42:16.541735  3948 net.cpp:150] Setting up scale1_0
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 110702000
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer relu1_0
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer relu1_0
I0312 12:42:16.541735  3948 net.cpp:434] relu1_0 <- scale1_0
I0312 12:42:16.541735  3948 net.cpp:408] relu1_0 -> relu1_0
I0312 12:42:16.541735  3948 net.cpp:150] Setting up relu1_0
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 120737200
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer conv2
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer conv2
I0312 12:42:16.541735  3948 net.cpp:434] conv2 <- relu1_0
I0312 12:42:16.541735  3948 net.cpp:408] conv2 -> conv2
I0312 12:42:16.541735  3948 net.cpp:150] Setting up conv2
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 130772400
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer bn2
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer bn2
I0312 12:42:16.541735  3948 net.cpp:434] bn2 <- conv2
I0312 12:42:16.541735  3948 net.cpp:408] bn2 -> bn2
I0312 12:42:16.541735  3948 net.cpp:150] Setting up bn2
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 140807600
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale2
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer scale2
I0312 12:42:16.541735  3948 net.cpp:434] scale2 <- bn2
I0312 12:42:16.541735  3948 net.cpp:408] scale2 -> scale2
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale2
I0312 12:42:16.541735  3948 net.cpp:150] Setting up scale2
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 150842800
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer relu2
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer relu2
I0312 12:42:16.541735  3948 net.cpp:434] relu2 <- scale2
I0312 12:42:16.541735  3948 net.cpp:408] relu2 -> relu2
I0312 12:42:16.541735  3948 net.cpp:150] Setting up relu2
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 160878000
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer conv2_1
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer conv2_1
I0312 12:42:16.541735  3948 net.cpp:434] conv2_1 <- relu2
I0312 12:42:16.541735  3948 net.cpp:408] conv2_1 -> conv2_1
I0312 12:42:16.541735  3948 net.cpp:150] Setting up conv2_1
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 170913200
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer bn2_1
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer bn2_1
I0312 12:42:16.541735  3948 net.cpp:434] bn2_1 <- conv2_1
I0312 12:42:16.541735  3948 net.cpp:408] bn2_1 -> bn2_1
I0312 12:42:16.541735  3948 net.cpp:150] Setting up bn2_1
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 180948400
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale2_1
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer scale2_1
I0312 12:42:16.541735  3948 net.cpp:434] scale2_1 <- bn2_1
I0312 12:42:16.541735  3948 net.cpp:408] scale2_1 -> scale2_1
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer scale2_1
I0312 12:42:16.541735  3948 net.cpp:150] Setting up scale2_1
I0312 12:42:16.541735  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.541735  3948 net.cpp:165] Memory required for data: 190983600
I0312 12:42:16.541735  3948 layer_factory.cpp:58] Creating layer relu2_1
I0312 12:42:16.541735  3948 net.cpp:100] Creating Layer relu2_1
I0312 12:42:16.551734  3948 net.cpp:434] relu2_1 <- scale2_1
I0312 12:42:16.551734  3948 net.cpp:408] relu2_1 -> relu2_1
I0312 12:42:16.551734  3948 net.cpp:150] Setting up relu2_1
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 201018800
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer pool2_1
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer pool2_1
I0312 12:42:16.551734  3948 net.cpp:434] pool2_1 <- relu2_1
I0312 12:42:16.551734  3948 net.cpp:408] pool2_1 -> pool2_1
I0312 12:42:16.551734  3948 net.cpp:150] Setting up pool2_1
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 203527600
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer conv2_2
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer conv2_2
I0312 12:42:16.551734  3948 net.cpp:434] conv2_2 <- pool2_1
I0312 12:42:16.551734  3948 net.cpp:408] conv2_2 -> conv2_2
I0312 12:42:16.551734  3948 net.cpp:150] Setting up conv2_2
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 206036400
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer bn2_2
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer bn2_2
I0312 12:42:16.551734  3948 net.cpp:434] bn2_2 <- conv2_2
I0312 12:42:16.551734  3948 net.cpp:408] bn2_2 -> bn2_2
I0312 12:42:16.551734  3948 net.cpp:150] Setting up bn2_2
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 208545200
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer scale2_2
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer scale2_2
I0312 12:42:16.551734  3948 net.cpp:434] scale2_2 <- bn2_2
I0312 12:42:16.551734  3948 net.cpp:408] scale2_2 -> scale2_2
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer scale2_2
I0312 12:42:16.551734  3948 net.cpp:150] Setting up scale2_2
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 211054000
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer relu2_2
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer relu2_2
I0312 12:42:16.551734  3948 net.cpp:434] relu2_2 <- scale2_2
I0312 12:42:16.551734  3948 net.cpp:408] relu2_2 -> relu2_2
I0312 12:42:16.551734  3948 net.cpp:150] Setting up relu2_2
I0312 12:42:16.551734  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.551734  3948 net.cpp:165] Memory required for data: 213562800
I0312 12:42:16.551734  3948 layer_factory.cpp:58] Creating layer conv3
I0312 12:42:16.551734  3948 net.cpp:100] Creating Layer conv3
I0312 12:42:16.551734  3948 net.cpp:434] conv3 <- relu2_2
I0312 12:42:16.551734  3948 net.cpp:408] conv3 -> conv3
I0312 12:42:16.561723  3948 net.cpp:150] Setting up conv3
I0312 12:42:16.561723  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.561723  3948 net.cpp:165] Memory required for data: 216071600
I0312 12:42:16.561723  3948 layer_factory.cpp:58] Creating layer bn3
I0312 12:42:16.561723  3948 net.cpp:100] Creating Layer bn3
I0312 12:42:16.561723  3948 net.cpp:434] bn3 <- conv3
I0312 12:42:16.561723  3948 net.cpp:408] bn3 -> bn3
I0312 12:42:16.561723  3948 net.cpp:150] Setting up bn3
I0312 12:42:16.561723  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.561723  3948 net.cpp:165] Memory required for data: 218580400
I0312 12:42:16.561723  3948 layer_factory.cpp:58] Creating layer scale3
I0312 12:42:16.561723  3948 net.cpp:100] Creating Layer scale3
I0312 12:42:16.561723  3948 net.cpp:434] scale3 <- bn3
I0312 12:42:16.561723  3948 net.cpp:408] scale3 -> scale3
I0312 12:42:16.561723  3948 layer_factory.cpp:58] Creating layer scale3
I0312 12:42:16.561723  3948 net.cpp:150] Setting up scale3
I0312 12:42:16.561723  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.561723  3948 net.cpp:165] Memory required for data: 221089200
I0312 12:42:16.561723  3948 layer_factory.cpp:58] Creating layer relu3
I0312 12:42:16.561723  3948 net.cpp:100] Creating Layer relu3
I0312 12:42:16.561723  3948 net.cpp:434] relu3 <- scale3
I0312 12:42:16.561723  3948 net.cpp:408] relu3 -> relu3
I0312 12:42:16.561723  3948 net.cpp:150] Setting up relu3
I0312 12:42:16.561723  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.561723  3948 net.cpp:165] Memory required for data: 223598000
I0312 12:42:16.561723  3948 layer_factory.cpp:58] Creating layer conv4
I0312 12:42:16.561723  3948 net.cpp:100] Creating Layer conv4
I0312 12:42:16.561723  3948 net.cpp:434] conv4 <- relu3
I0312 12:42:16.561723  3948 net.cpp:408] conv4 -> conv4
I0312 12:42:16.574259  3948 net.cpp:150] Setting up conv4
I0312 12:42:16.574259  3948 net.cpp:157] Top shape: 100 64 14 14 (1254400)
I0312 12:42:16.574259  3948 net.cpp:165] Memory required for data: 228615600
I0312 12:42:16.574259  3948 layer_factory.cpp:58] Creating layer pool4
I0312 12:42:16.574259  3948 net.cpp:100] Creating Layer pool4
I0312 12:42:16.574259  3948 net.cpp:434] pool4 <- conv4
I0312 12:42:16.574259  3948 net.cpp:408] pool4 -> pool4
I0312 12:42:16.574259  3948 net.cpp:150] Setting up pool4
I0312 12:42:16.574259  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.574259  3948 net.cpp:165] Memory required for data: 229870000
I0312 12:42:16.574259  3948 layer_factory.cpp:58] Creating layer bn4
I0312 12:42:16.574259  3948 net.cpp:100] Creating Layer bn4
I0312 12:42:16.574259  3948 net.cpp:434] bn4 <- pool4
I0312 12:42:16.574259  3948 net.cpp:408] bn4 -> bn4
I0312 12:42:16.574740  3948 net.cpp:150] Setting up bn4
I0312 12:42:16.574740  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.574740  3948 net.cpp:165] Memory required for data: 231124400
I0312 12:42:16.574740  3948 layer_factory.cpp:58] Creating layer scale4
I0312 12:42:16.574740  3948 net.cpp:100] Creating Layer scale4
I0312 12:42:16.574740  3948 net.cpp:434] scale4 <- bn4
I0312 12:42:16.574740  3948 net.cpp:408] scale4 -> scale4
I0312 12:42:16.574740  3948 layer_factory.cpp:58] Creating layer scale4
I0312 12:42:16.574740  3948 net.cpp:150] Setting up scale4
I0312 12:42:16.574740  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.574740  3948 net.cpp:165] Memory required for data: 232378800
I0312 12:42:16.574740  3948 layer_factory.cpp:58] Creating layer relu4
I0312 12:42:16.574740  3948 net.cpp:100] Creating Layer relu4
I0312 12:42:16.574740  3948 net.cpp:434] relu4 <- scale4
I0312 12:42:16.574740  3948 net.cpp:408] relu4 -> relu4
I0312 12:42:16.578753  3948 net.cpp:150] Setting up relu4
I0312 12:42:16.579254  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.579254  3948 net.cpp:165] Memory required for data: 233633200
I0312 12:42:16.579254  3948 layer_factory.cpp:58] Creating layer conv4_1
I0312 12:42:16.579254  3948 net.cpp:100] Creating Layer conv4_1
I0312 12:42:16.579254  3948 net.cpp:434] conv4_1 <- relu4
I0312 12:42:16.579254  3948 net.cpp:408] conv4_1 -> conv4_1
I0312 12:42:16.582239  3948 net.cpp:150] Setting up conv4_1
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 234887600
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer bn4_1
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer bn4_1
I0312 12:42:16.582239  3948 net.cpp:434] bn4_1 <- conv4_1
I0312 12:42:16.582239  3948 net.cpp:408] bn4_1 -> bn4_1
I0312 12:42:16.582239  3948 net.cpp:150] Setting up bn4_1
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 236142000
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer scale4_1
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer scale4_1
I0312 12:42:16.582239  3948 net.cpp:434] scale4_1 <- bn4_1
I0312 12:42:16.582239  3948 net.cpp:408] scale4_1 -> scale4_1
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer scale4_1
I0312 12:42:16.582239  3948 net.cpp:150] Setting up scale4_1
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 237396400
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer relu4_1
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer relu4_1
I0312 12:42:16.582239  3948 net.cpp:434] relu4_1 <- scale4_1
I0312 12:42:16.582239  3948 net.cpp:408] relu4_1 -> relu4_1
I0312 12:42:16.582239  3948 net.cpp:150] Setting up relu4_1
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 238650800
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer conv4_2
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer conv4_2
I0312 12:42:16.582239  3948 net.cpp:434] conv4_2 <- relu4_1
I0312 12:42:16.582239  3948 net.cpp:408] conv4_2 -> conv4_2
I0312 12:42:16.582239  3948 net.cpp:150] Setting up conv4_2
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 239905200
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer bn4_2
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer bn4_2
I0312 12:42:16.582239  3948 net.cpp:434] bn4_2 <- conv4_2
I0312 12:42:16.582239  3948 net.cpp:408] bn4_2 -> bn4_2
I0312 12:42:16.582239  3948 net.cpp:150] Setting up bn4_2
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 241159600
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer scale4_2
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer scale4_2
I0312 12:42:16.582239  3948 net.cpp:434] scale4_2 <- bn4_2
I0312 12:42:16.582239  3948 net.cpp:408] scale4_2 -> scale4_2
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer scale4_2
I0312 12:42:16.582239  3948 net.cpp:150] Setting up scale4_2
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 242414000
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer relu4_2
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer relu4_2
I0312 12:42:16.582239  3948 net.cpp:434] relu4_2 <- scale4_2
I0312 12:42:16.582239  3948 net.cpp:408] relu4_2 -> relu4_2
I0312 12:42:16.582239  3948 net.cpp:150] Setting up relu4_2
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 243668400
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer pool4_2
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer pool4_2
I0312 12:42:16.582239  3948 net.cpp:434] pool4_2 <- relu4_2
I0312 12:42:16.582239  3948 net.cpp:408] pool4_2 -> pool4_2
I0312 12:42:16.582239  3948 net.cpp:150] Setting up pool4_2
I0312 12:42:16.582239  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.582239  3948 net.cpp:165] Memory required for data: 244078000
I0312 12:42:16.582239  3948 layer_factory.cpp:58] Creating layer conv4_0
I0312 12:42:16.582239  3948 net.cpp:100] Creating Layer conv4_0
I0312 12:42:16.582239  3948 net.cpp:434] conv4_0 <- pool4_2
I0312 12:42:16.582239  3948 net.cpp:408] conv4_0 -> conv4_0
I0312 12:42:16.592265  3948 net.cpp:150] Setting up conv4_0
I0312 12:42:16.592265  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.592265  3948 net.cpp:165] Memory required for data: 244897200
I0312 12:42:16.592265  3948 layer_factory.cpp:58] Creating layer bn4_0
I0312 12:42:16.592265  3948 net.cpp:100] Creating Layer bn4_0
I0312 12:42:16.592265  3948 net.cpp:434] bn4_0 <- conv4_0
I0312 12:42:16.592265  3948 net.cpp:408] bn4_0 -> bn4_0
I0312 12:42:16.592265  3948 net.cpp:150] Setting up bn4_0
I0312 12:42:16.592265  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.592265  3948 net.cpp:165] Memory required for data: 245716400
I0312 12:42:16.592265  3948 layer_factory.cpp:58] Creating layer scale4_0
I0312 12:42:16.592265  3948 net.cpp:100] Creating Layer scale4_0
I0312 12:42:16.592265  3948 net.cpp:434] scale4_0 <- bn4_0
I0312 12:42:16.592265  3948 net.cpp:408] scale4_0 -> scale4_0
I0312 12:42:16.592265  3948 layer_factory.cpp:58] Creating layer scale4_0
I0312 12:42:16.592265  3948 net.cpp:150] Setting up scale4_0
I0312 12:42:16.592265  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.592265  3948 net.cpp:165] Memory required for data: 246535600
I0312 12:42:16.592265  3948 layer_factory.cpp:58] Creating layer relu4_0
I0312 12:42:16.592265  3948 net.cpp:100] Creating Layer relu4_0
I0312 12:42:16.592265  3948 net.cpp:434] relu4_0 <- scale4_0
I0312 12:42:16.592265  3948 net.cpp:408] relu4_0 -> relu4_0
I0312 12:42:16.592265  3948 net.cpp:150] Setting up relu4_0
I0312 12:42:16.592265  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.592265  3948 net.cpp:165] Memory required for data: 247354800
I0312 12:42:16.592265  3948 layer_factory.cpp:58] Creating layer cccp4
I0312 12:42:16.592265  3948 net.cpp:100] Creating Layer cccp4
I0312 12:42:16.592265  3948 net.cpp:434] cccp4 <- relu4_0
I0312 12:42:16.592265  3948 net.cpp:408] cccp4 -> cccp4
I0312 12:42:16.602260  3948 net.cpp:150] Setting up cccp4
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 248993200
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer relu_cccp4
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer relu_cccp4
I0312 12:42:16.602260  3948 net.cpp:434] relu_cccp4 <- cccp4
I0312 12:42:16.602260  3948 net.cpp:395] relu_cccp4 -> cccp4 (in-place)
I0312 12:42:16.602260  3948 net.cpp:150] Setting up relu_cccp4
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 250631600
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer cccp5
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer cccp5
I0312 12:42:16.602260  3948 net.cpp:434] cccp5 <- cccp4
I0312 12:42:16.602260  3948 net.cpp:408] cccp5 -> cccp5
I0312 12:42:16.602260  3948 net.cpp:150] Setting up cccp5
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251041200
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer relu_cccp5
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer relu_cccp5
I0312 12:42:16.602260  3948 net.cpp:434] relu_cccp5 <- cccp5
I0312 12:42:16.602260  3948 net.cpp:395] relu_cccp5 -> cccp5 (in-place)
I0312 12:42:16.602260  3948 net.cpp:150] Setting up relu_cccp5
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251450800
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer poolcp5
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer poolcp5
I0312 12:42:16.602260  3948 net.cpp:434] poolcp5 <- cccp5
I0312 12:42:16.602260  3948 net.cpp:408] poolcp5 -> poolcp5
I0312 12:42:16.602260  3948 net.cpp:150] Setting up poolcp5
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251553200
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer cccp6
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer cccp6
I0312 12:42:16.602260  3948 net.cpp:434] cccp6 <- poolcp5
I0312 12:42:16.602260  3948 net.cpp:408] cccp6 -> cccp6
I0312 12:42:16.602260  3948 net.cpp:150] Setting up cccp6
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251655600
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer relu_cccp6
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer relu_cccp6
I0312 12:42:16.602260  3948 net.cpp:434] relu_cccp6 <- cccp6
I0312 12:42:16.602260  3948 net.cpp:395] relu_cccp6 -> cccp6 (in-place)
I0312 12:42:16.602260  3948 net.cpp:150] Setting up relu_cccp6
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251758000
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer poolcp6
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer poolcp6
I0312 12:42:16.602260  3948 net.cpp:434] poolcp6 <- cccp6
I0312 12:42:16.602260  3948 net.cpp:408] poolcp6 -> poolcp6
I0312 12:42:16.602260  3948 net.cpp:150] Setting up poolcp6
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 64 1 1 (6400)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251783600
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer ip1
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer ip1
I0312 12:42:16.602260  3948 net.cpp:434] ip1 <- poolcp6
I0312 12:42:16.602260  3948 net.cpp:408] ip1 -> ip1
I0312 12:42:16.602260  3948 net.cpp:150] Setting up ip1
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251787600
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer ip1_ip1_0_split
I0312 12:42:16.602260  3948 net.cpp:434] ip1_ip1_0_split <- ip1
I0312 12:42:16.602260  3948 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0312 12:42:16.602260  3948 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0312 12:42:16.602260  3948 net.cpp:150] Setting up ip1_ip1_0_split
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251795600
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer accuracy_training
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer accuracy_training
I0312 12:42:16.602260  3948 net.cpp:434] accuracy_training <- ip1_ip1_0_split_0
I0312 12:42:16.602260  3948 net.cpp:434] accuracy_training <- label_mnist_1_split_0
I0312 12:42:16.602260  3948 net.cpp:408] accuracy_training -> accuracy_training
I0312 12:42:16.602260  3948 net.cpp:150] Setting up accuracy_training
I0312 12:42:16.602260  3948 net.cpp:157] Top shape: (1)
I0312 12:42:16.602260  3948 net.cpp:165] Memory required for data: 251795604
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer loss
I0312 12:42:16.602260  3948 net.cpp:100] Creating Layer loss
I0312 12:42:16.602260  3948 net.cpp:434] loss <- ip1_ip1_0_split_1
I0312 12:42:16.602260  3948 net.cpp:434] loss <- label_mnist_1_split_1
I0312 12:42:16.602260  3948 net.cpp:408] loss -> loss
I0312 12:42:16.602260  3948 layer_factory.cpp:58] Creating layer loss
I0312 12:42:16.612246  3948 net.cpp:150] Setting up loss
I0312 12:42:16.612246  3948 net.cpp:157] Top shape: (1)
I0312 12:42:16.612246  3948 net.cpp:160]     with loss weight 1
I0312 12:42:16.612246  3948 net.cpp:165] Memory required for data: 251795608
I0312 12:42:16.612246  3948 net.cpp:226] loss needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:228] accuracy_training does not need backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] ip1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] poolcp6 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu_cccp6 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] cccp6 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] poolcp5 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu_cccp5 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] cccp5 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu_cccp4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] cccp4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu4_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale4_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn4_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv4_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] pool4_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu4_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale4_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn4_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv4_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu4_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale4_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn4_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv4_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] pool4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv4 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu3 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale3 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn3 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv3 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu2_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale2_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn2_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv2_2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] pool2_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu2_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale2_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn2_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv2_1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv2 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu1_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale1_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn1_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv1_0 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] relu1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] scale1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] bn1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:226] conv1 needs backward computation.
I0312 12:42:16.612246  3948 net.cpp:228] label_mnist_1_split does not need backward computation.
I0312 12:42:16.612246  3948 net.cpp:228] mnist does not need backward computation.
I0312 12:42:16.612246  3948 net.cpp:270] This network produces output accuracy_training
I0312 12:42:16.612246  3948 net.cpp:270] This network produces output loss
I0312 12:42:16.612246  3948 net.cpp:283] Network initialization done.
I0312 12:42:16.612246  3948 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I0312 12:42:16.612246  3948 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 12:42:16.612246  3948 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I0312 12:42:16.612246  3948 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I0312 12:42:16.612246  3948 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0312 12:42:16.612246  3948 layer_factory.cpp:58] Creating layer mnist
I0312 12:42:16.612246  3948 net.cpp:100] Creating Layer mnist
I0312 12:42:16.612246  3948 net.cpp:408] mnist -> data
I0312 12:42:16.612246  3948 net.cpp:408] mnist -> label
I0312 12:42:16.612246  9460 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:16.622267  9460 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_test_lmdb_norm2
I0312 12:42:16.622267  3948 data_layer.cpp:41] output data size: 100,1,28,28
I0312 12:42:16.622267  3948 net.cpp:150] Setting up mnist
I0312 12:42:16.622267  3948 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0312 12:42:16.622267  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:16.622267  3948 net.cpp:165] Memory required for data: 314000
I0312 12:42:16.622267  3948 layer_factory.cpp:58] Creating layer label_mnist_1_split
I0312 12:42:16.622267  3948 net.cpp:100] Creating Layer label_mnist_1_split
I0312 12:42:16.622267  3948 net.cpp:434] label_mnist_1_split <- label
I0312 12:42:16.622267  3948 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0312 12:42:16.622267  3948 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0312 12:42:16.622267  3948 net.cpp:150] Setting up label_mnist_1_split
I0312 12:42:16.622267  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:16.622267  3948 net.cpp:157] Top shape: 100 (100)
I0312 12:42:16.622267  3948 net.cpp:165] Memory required for data: 314800
I0312 12:42:16.622267  3948 layer_factory.cpp:58] Creating layer conv1
I0312 12:42:16.622267  3948 net.cpp:100] Creating Layer conv1
I0312 12:42:16.622267  3948 net.cpp:434] conv1 <- data
I0312 12:42:16.622267  3948 net.cpp:408] conv1 -> conv1
I0312 12:42:16.632247  4952 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 12:42:16.642244  3948 net.cpp:150] Setting up conv1
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 20385200
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer bn1
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer bn1
I0312 12:42:16.642244  3948 net.cpp:434] bn1 <- conv1
I0312 12:42:16.642244  3948 net.cpp:408] bn1 -> bn1
I0312 12:42:16.642244  3948 net.cpp:150] Setting up bn1
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 40455600
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer scale1
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer scale1
I0312 12:42:16.642244  3948 net.cpp:434] scale1 <- bn1
I0312 12:42:16.642244  3948 net.cpp:408] scale1 -> scale1
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer scale1
I0312 12:42:16.642244  3948 net.cpp:150] Setting up scale1
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 60526000
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer relu1
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer relu1
I0312 12:42:16.642244  3948 net.cpp:434] relu1 <- scale1
I0312 12:42:16.642244  3948 net.cpp:408] relu1 -> relu1
I0312 12:42:16.642244  3948 net.cpp:150] Setting up relu1
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 80596400
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer conv1_0
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer conv1_0
I0312 12:42:16.642244  3948 net.cpp:434] conv1_0 <- relu1
I0312 12:42:16.642244  3948 net.cpp:408] conv1_0 -> conv1_0
I0312 12:42:16.642244  3948 net.cpp:150] Setting up conv1_0
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 90631600
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer bn1_0
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer bn1_0
I0312 12:42:16.642244  3948 net.cpp:434] bn1_0 <- conv1_0
I0312 12:42:16.642244  3948 net.cpp:408] bn1_0 -> bn1_0
I0312 12:42:16.642244  3948 net.cpp:150] Setting up bn1_0
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 100666800
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer scale1_0
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer scale1_0
I0312 12:42:16.642244  3948 net.cpp:434] scale1_0 <- bn1_0
I0312 12:42:16.642244  3948 net.cpp:408] scale1_0 -> scale1_0
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer scale1_0
I0312 12:42:16.642244  3948 net.cpp:150] Setting up scale1_0
I0312 12:42:16.642244  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.642244  3948 net.cpp:165] Memory required for data: 110702000
I0312 12:42:16.642244  3948 layer_factory.cpp:58] Creating layer relu1_0
I0312 12:42:16.642244  3948 net.cpp:100] Creating Layer relu1_0
I0312 12:42:16.642244  3948 net.cpp:434] relu1_0 <- scale1_0
I0312 12:42:16.642244  3948 net.cpp:408] relu1_0 -> relu1_0
I0312 12:42:16.652263  3948 net.cpp:150] Setting up relu1_0
I0312 12:42:16.652263  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.652263  3948 net.cpp:165] Memory required for data: 120737200
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer conv2
I0312 12:42:16.652263  3948 net.cpp:100] Creating Layer conv2
I0312 12:42:16.652263  3948 net.cpp:434] conv2 <- relu1_0
I0312 12:42:16.652263  3948 net.cpp:408] conv2 -> conv2
I0312 12:42:16.652263  3948 net.cpp:150] Setting up conv2
I0312 12:42:16.652263  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.652263  3948 net.cpp:165] Memory required for data: 130772400
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer bn2
I0312 12:42:16.652263  3948 net.cpp:100] Creating Layer bn2
I0312 12:42:16.652263  3948 net.cpp:434] bn2 <- conv2
I0312 12:42:16.652263  3948 net.cpp:408] bn2 -> bn2
I0312 12:42:16.652263  3948 net.cpp:150] Setting up bn2
I0312 12:42:16.652263  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.652263  3948 net.cpp:165] Memory required for data: 140807600
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer scale2
I0312 12:42:16.652263  3948 net.cpp:100] Creating Layer scale2
I0312 12:42:16.652263  3948 net.cpp:434] scale2 <- bn2
I0312 12:42:16.652263  3948 net.cpp:408] scale2 -> scale2
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer scale2
I0312 12:42:16.652263  3948 net.cpp:150] Setting up scale2
I0312 12:42:16.652263  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.652263  3948 net.cpp:165] Memory required for data: 150842800
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer relu2
I0312 12:42:16.652263  3948 net.cpp:100] Creating Layer relu2
I0312 12:42:16.652263  3948 net.cpp:434] relu2 <- scale2
I0312 12:42:16.652263  3948 net.cpp:408] relu2 -> relu2
I0312 12:42:16.652263  3948 net.cpp:150] Setting up relu2
I0312 12:42:16.652263  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.652263  3948 net.cpp:165] Memory required for data: 160878000
I0312 12:42:16.652263  3948 layer_factory.cpp:58] Creating layer conv2_1
I0312 12:42:16.652263  3948 net.cpp:100] Creating Layer conv2_1
I0312 12:42:16.652263  3948 net.cpp:434] conv2_1 <- relu2
I0312 12:42:16.652263  3948 net.cpp:408] conv2_1 -> conv2_1
I0312 12:42:16.662245  3948 net.cpp:150] Setting up conv2_1
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 170913200
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer bn2_1
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer bn2_1
I0312 12:42:16.662245  3948 net.cpp:434] bn2_1 <- conv2_1
I0312 12:42:16.662245  3948 net.cpp:408] bn2_1 -> bn2_1
I0312 12:42:16.662245  3948 net.cpp:150] Setting up bn2_1
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 180948400
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale2_1
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer scale2_1
I0312 12:42:16.662245  3948 net.cpp:434] scale2_1 <- bn2_1
I0312 12:42:16.662245  3948 net.cpp:408] scale2_1 -> scale2_1
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale2_1
I0312 12:42:16.662245  3948 net.cpp:150] Setting up scale2_1
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 190983600
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer relu2_1
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer relu2_1
I0312 12:42:16.662245  3948 net.cpp:434] relu2_1 <- scale2_1
I0312 12:42:16.662245  3948 net.cpp:408] relu2_1 -> relu2_1
I0312 12:42:16.662245  3948 net.cpp:150] Setting up relu2_1
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 201018800
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer pool2_1
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer pool2_1
I0312 12:42:16.662245  3948 net.cpp:434] pool2_1 <- relu2_1
I0312 12:42:16.662245  3948 net.cpp:408] pool2_1 -> pool2_1
I0312 12:42:16.662245  3948 net.cpp:150] Setting up pool2_1
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 203527600
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer conv2_2
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer conv2_2
I0312 12:42:16.662245  3948 net.cpp:434] conv2_2 <- pool2_1
I0312 12:42:16.662245  3948 net.cpp:408] conv2_2 -> conv2_2
I0312 12:42:16.662245  3948 net.cpp:150] Setting up conv2_2
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 206036400
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer bn2_2
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer bn2_2
I0312 12:42:16.662245  3948 net.cpp:434] bn2_2 <- conv2_2
I0312 12:42:16.662245  3948 net.cpp:408] bn2_2 -> bn2_2
I0312 12:42:16.662245  3948 net.cpp:150] Setting up bn2_2
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 208545200
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale2_2
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer scale2_2
I0312 12:42:16.662245  3948 net.cpp:434] scale2_2 <- bn2_2
I0312 12:42:16.662245  3948 net.cpp:408] scale2_2 -> scale2_2
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale2_2
I0312 12:42:16.662245  3948 net.cpp:150] Setting up scale2_2
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 211054000
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer relu2_2
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer relu2_2
I0312 12:42:16.662245  3948 net.cpp:434] relu2_2 <- scale2_2
I0312 12:42:16.662245  3948 net.cpp:408] relu2_2 -> relu2_2
I0312 12:42:16.662245  3948 net.cpp:150] Setting up relu2_2
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 213562800
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer conv3
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer conv3
I0312 12:42:16.662245  3948 net.cpp:434] conv3 <- relu2_2
I0312 12:42:16.662245  3948 net.cpp:408] conv3 -> conv3
I0312 12:42:16.662245  3948 net.cpp:150] Setting up conv3
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 216071600
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer bn3
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer bn3
I0312 12:42:16.662245  3948 net.cpp:434] bn3 <- conv3
I0312 12:42:16.662245  3948 net.cpp:408] bn3 -> bn3
I0312 12:42:16.662245  3948 net.cpp:150] Setting up bn3
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 218580400
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale3
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer scale3
I0312 12:42:16.662245  3948 net.cpp:434] scale3 <- bn3
I0312 12:42:16.662245  3948 net.cpp:408] scale3 -> scale3
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer scale3
I0312 12:42:16.662245  3948 net.cpp:150] Setting up scale3
I0312 12:42:16.662245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.662245  3948 net.cpp:165] Memory required for data: 221089200
I0312 12:42:16.662245  3948 layer_factory.cpp:58] Creating layer relu3
I0312 12:42:16.662245  3948 net.cpp:100] Creating Layer relu3
I0312 12:42:16.662245  3948 net.cpp:434] relu3 <- scale3
I0312 12:42:16.662245  3948 net.cpp:408] relu3 -> relu3
I0312 12:42:16.672245  3948 net.cpp:150] Setting up relu3
I0312 12:42:16.672245  3948 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 12:42:16.672245  3948 net.cpp:165] Memory required for data: 223598000
I0312 12:42:16.672245  3948 layer_factory.cpp:58] Creating layer conv4
I0312 12:42:16.672245  3948 net.cpp:100] Creating Layer conv4
I0312 12:42:16.672245  3948 net.cpp:434] conv4 <- relu3
I0312 12:42:16.672245  3948 net.cpp:408] conv4 -> conv4
I0312 12:42:16.677752  3948 net.cpp:150] Setting up conv4
I0312 12:42:16.677752  3948 net.cpp:157] Top shape: 100 64 14 14 (1254400)
I0312 12:42:16.677752  3948 net.cpp:165] Memory required for data: 228615600
I0312 12:42:16.677752  3948 layer_factory.cpp:58] Creating layer pool4
I0312 12:42:16.677752  3948 net.cpp:100] Creating Layer pool4
I0312 12:42:16.677752  3948 net.cpp:434] pool4 <- conv4
I0312 12:42:16.677752  3948 net.cpp:408] pool4 -> pool4
I0312 12:42:16.677752  3948 net.cpp:150] Setting up pool4
I0312 12:42:16.677752  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.677752  3948 net.cpp:165] Memory required for data: 229870000
I0312 12:42:16.677752  3948 layer_factory.cpp:58] Creating layer bn4
I0312 12:42:16.677752  3948 net.cpp:100] Creating Layer bn4
I0312 12:42:16.677752  3948 net.cpp:434] bn4 <- pool4
I0312 12:42:16.677752  3948 net.cpp:408] bn4 -> bn4
I0312 12:42:16.678251  3948 net.cpp:150] Setting up bn4
I0312 12:42:16.678251  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.678251  3948 net.cpp:165] Memory required for data: 231124400
I0312 12:42:16.678251  3948 layer_factory.cpp:58] Creating layer scale4
I0312 12:42:16.678251  3948 net.cpp:100] Creating Layer scale4
I0312 12:42:16.678251  3948 net.cpp:434] scale4 <- bn4
I0312 12:42:16.678251  3948 net.cpp:408] scale4 -> scale4
I0312 12:42:16.678251  3948 layer_factory.cpp:58] Creating layer scale4
I0312 12:42:16.678251  3948 net.cpp:150] Setting up scale4
I0312 12:42:16.678251  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.678251  3948 net.cpp:165] Memory required for data: 232378800
I0312 12:42:16.678251  3948 layer_factory.cpp:58] Creating layer relu4
I0312 12:42:16.678251  3948 net.cpp:100] Creating Layer relu4
I0312 12:42:16.678251  3948 net.cpp:434] relu4 <- scale4
I0312 12:42:16.678251  3948 net.cpp:408] relu4 -> relu4
I0312 12:42:16.681751  3948 net.cpp:150] Setting up relu4
I0312 12:42:16.681751  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.681751  3948 net.cpp:165] Memory required for data: 233633200
I0312 12:42:16.681751  3948 layer_factory.cpp:58] Creating layer conv4_1
I0312 12:42:16.681751  3948 net.cpp:100] Creating Layer conv4_1
I0312 12:42:16.681751  3948 net.cpp:434] conv4_1 <- relu4
I0312 12:42:16.681751  3948 net.cpp:408] conv4_1 -> conv4_1
I0312 12:42:16.681751  3948 net.cpp:150] Setting up conv4_1
I0312 12:42:16.681751  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.681751  3948 net.cpp:165] Memory required for data: 234887600
I0312 12:42:16.681751  3948 layer_factory.cpp:58] Creating layer bn4_1
I0312 12:42:16.681751  3948 net.cpp:100] Creating Layer bn4_1
I0312 12:42:16.681751  3948 net.cpp:434] bn4_1 <- conv4_1
I0312 12:42:16.681751  3948 net.cpp:408] bn4_1 -> bn4_1
I0312 12:42:16.681751  3948 net.cpp:150] Setting up bn4_1
I0312 12:42:16.681751  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.681751  3948 net.cpp:165] Memory required for data: 236142000
I0312 12:42:16.681751  3948 layer_factory.cpp:58] Creating layer scale4_1
I0312 12:42:16.681751  3948 net.cpp:100] Creating Layer scale4_1
I0312 12:42:16.681751  3948 net.cpp:434] scale4_1 <- bn4_1
I0312 12:42:16.681751  3948 net.cpp:408] scale4_1 -> scale4_1
I0312 12:42:16.681751  3948 layer_factory.cpp:58] Creating layer scale4_1
I0312 12:42:16.681751  3948 net.cpp:150] Setting up scale4_1
I0312 12:42:16.681751  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.681751  3948 net.cpp:165] Memory required for data: 237396400
I0312 12:42:16.681751  3948 layer_factory.cpp:58] Creating layer relu4_1
I0312 12:42:16.681751  3948 net.cpp:100] Creating Layer relu4_1
I0312 12:42:16.681751  3948 net.cpp:434] relu4_1 <- scale4_1
I0312 12:42:16.681751  3948 net.cpp:408] relu4_1 -> relu4_1
I0312 12:42:16.691840  3948 net.cpp:150] Setting up relu4_1
I0312 12:42:16.691840  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.691840  3948 net.cpp:165] Memory required for data: 238650800
I0312 12:42:16.691840  3948 layer_factory.cpp:58] Creating layer conv4_2
I0312 12:42:16.691840  3948 net.cpp:100] Creating Layer conv4_2
I0312 12:42:16.691840  3948 net.cpp:434] conv4_2 <- relu4_1
I0312 12:42:16.691840  3948 net.cpp:408] conv4_2 -> conv4_2
I0312 12:42:16.701846  3948 net.cpp:150] Setting up conv4_2
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 239905200
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer bn4_2
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer bn4_2
I0312 12:42:16.701846  3948 net.cpp:434] bn4_2 <- conv4_2
I0312 12:42:16.701846  3948 net.cpp:408] bn4_2 -> bn4_2
I0312 12:42:16.701846  3948 net.cpp:150] Setting up bn4_2
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 241159600
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer scale4_2
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer scale4_2
I0312 12:42:16.701846  3948 net.cpp:434] scale4_2 <- bn4_2
I0312 12:42:16.701846  3948 net.cpp:408] scale4_2 -> scale4_2
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer scale4_2
I0312 12:42:16.701846  3948 net.cpp:150] Setting up scale4_2
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 242414000
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer relu4_2
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer relu4_2
I0312 12:42:16.701846  3948 net.cpp:434] relu4_2 <- scale4_2
I0312 12:42:16.701846  3948 net.cpp:408] relu4_2 -> relu4_2
I0312 12:42:16.701846  3948 net.cpp:150] Setting up relu4_2
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 243668400
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer pool4_2
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer pool4_2
I0312 12:42:16.701846  3948 net.cpp:434] pool4_2 <- relu4_2
I0312 12:42:16.701846  3948 net.cpp:408] pool4_2 -> pool4_2
I0312 12:42:16.701846  3948 net.cpp:150] Setting up pool4_2
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 244078000
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer conv4_0
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer conv4_0
I0312 12:42:16.701846  3948 net.cpp:434] conv4_0 <- pool4_2
I0312 12:42:16.701846  3948 net.cpp:408] conv4_0 -> conv4_0
I0312 12:42:16.701846  3948 net.cpp:150] Setting up conv4_0
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 244897200
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer bn4_0
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer bn4_0
I0312 12:42:16.701846  3948 net.cpp:434] bn4_0 <- conv4_0
I0312 12:42:16.701846  3948 net.cpp:408] bn4_0 -> bn4_0
I0312 12:42:16.701846  3948 net.cpp:150] Setting up bn4_0
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 245716400
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer scale4_0
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer scale4_0
I0312 12:42:16.701846  3948 net.cpp:434] scale4_0 <- bn4_0
I0312 12:42:16.701846  3948 net.cpp:408] scale4_0 -> scale4_0
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer scale4_0
I0312 12:42:16.701846  3948 net.cpp:150] Setting up scale4_0
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 246535600
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer relu4_0
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer relu4_0
I0312 12:42:16.701846  3948 net.cpp:434] relu4_0 <- scale4_0
I0312 12:42:16.701846  3948 net.cpp:408] relu4_0 -> relu4_0
I0312 12:42:16.701846  3948 net.cpp:150] Setting up relu4_0
I0312 12:42:16.701846  3948 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 12:42:16.701846  3948 net.cpp:165] Memory required for data: 247354800
I0312 12:42:16.701846  3948 layer_factory.cpp:58] Creating layer cccp4
I0312 12:42:16.701846  3948 net.cpp:100] Creating Layer cccp4
I0312 12:42:16.701846  3948 net.cpp:434] cccp4 <- relu4_0
I0312 12:42:16.701846  3948 net.cpp:408] cccp4 -> cccp4
I0312 12:42:16.711833  3948 net.cpp:150] Setting up cccp4
I0312 12:42:16.711833  3948 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 12:42:16.711833  3948 net.cpp:165] Memory required for data: 248993200
I0312 12:42:16.711833  3948 layer_factory.cpp:58] Creating layer relu_cccp4
I0312 12:42:16.711833  3948 net.cpp:100] Creating Layer relu_cccp4
I0312 12:42:16.711833  3948 net.cpp:434] relu_cccp4 <- cccp4
I0312 12:42:16.711833  3948 net.cpp:395] relu_cccp4 -> cccp4 (in-place)
I0312 12:42:16.711833  3948 net.cpp:150] Setting up relu_cccp4
I0312 12:42:16.711833  3948 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 12:42:16.711833  3948 net.cpp:165] Memory required for data: 250631600
I0312 12:42:16.711833  3948 layer_factory.cpp:58] Creating layer cccp5
I0312 12:42:16.711833  3948 net.cpp:100] Creating Layer cccp5
I0312 12:42:16.711833  3948 net.cpp:434] cccp5 <- cccp4
I0312 12:42:16.711833  3948 net.cpp:408] cccp5 -> cccp5
I0312 12:42:16.711833  3948 net.cpp:150] Setting up cccp5
I0312 12:42:16.711833  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.711833  3948 net.cpp:165] Memory required for data: 251041200
I0312 12:42:16.711833  3948 layer_factory.cpp:58] Creating layer relu_cccp5
I0312 12:42:16.711833  3948 net.cpp:100] Creating Layer relu_cccp5
I0312 12:42:16.711833  3948 net.cpp:434] relu_cccp5 <- cccp5
I0312 12:42:16.711833  3948 net.cpp:395] relu_cccp5 -> cccp5 (in-place)
I0312 12:42:16.711833  3948 net.cpp:150] Setting up relu_cccp5
I0312 12:42:16.711833  3948 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 12:42:16.711833  3948 net.cpp:165] Memory required for data: 251450800
I0312 12:42:16.711833  3948 layer_factory.cpp:58] Creating layer poolcp5
I0312 12:42:16.711833  3948 net.cpp:100] Creating Layer poolcp5
I0312 12:42:16.711833  3948 net.cpp:434] poolcp5 <- cccp5
I0312 12:42:16.711833  3948 net.cpp:408] poolcp5 -> poolcp5
I0312 12:42:16.711833  3948 net.cpp:150] Setting up poolcp5
I0312 12:42:16.711833  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.711833  3948 net.cpp:165] Memory required for data: 251553200
I0312 12:42:16.711833  3948 layer_factory.cpp:58] Creating layer cccp6
I0312 12:42:16.711833  3948 net.cpp:100] Creating Layer cccp6
I0312 12:42:16.711833  3948 net.cpp:434] cccp6 <- poolcp5
I0312 12:42:16.711833  3948 net.cpp:408] cccp6 -> cccp6
I0312 12:42:16.721846  3948 net.cpp:150] Setting up cccp6
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251655600
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer relu_cccp6
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer relu_cccp6
I0312 12:42:16.721846  3948 net.cpp:434] relu_cccp6 <- cccp6
I0312 12:42:16.721846  3948 net.cpp:395] relu_cccp6 -> cccp6 (in-place)
I0312 12:42:16.721846  3948 net.cpp:150] Setting up relu_cccp6
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251758000
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer poolcp6
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer poolcp6
I0312 12:42:16.721846  3948 net.cpp:434] poolcp6 <- cccp6
I0312 12:42:16.721846  3948 net.cpp:408] poolcp6 -> poolcp6
I0312 12:42:16.721846  3948 net.cpp:150] Setting up poolcp6
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 64 1 1 (6400)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251783600
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer ip1
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer ip1
I0312 12:42:16.721846  3948 net.cpp:434] ip1 <- poolcp6
I0312 12:42:16.721846  3948 net.cpp:408] ip1 -> ip1
I0312 12:42:16.721846  3948 net.cpp:150] Setting up ip1
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251787600
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer ip1_ip1_0_split
I0312 12:42:16.721846  3948 net.cpp:434] ip1_ip1_0_split <- ip1
I0312 12:42:16.721846  3948 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0312 12:42:16.721846  3948 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0312 12:42:16.721846  3948 net.cpp:150] Setting up ip1_ip1_0_split
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: 100 10 (1000)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251795600
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer accuracy
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer accuracy
I0312 12:42:16.721846  3948 net.cpp:434] accuracy <- ip1_ip1_0_split_0
I0312 12:42:16.721846  3948 net.cpp:434] accuracy <- label_mnist_1_split_0
I0312 12:42:16.721846  3948 net.cpp:408] accuracy -> accuracy
I0312 12:42:16.721846  3948 net.cpp:150] Setting up accuracy
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: (1)
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251795604
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer loss
I0312 12:42:16.721846  3948 net.cpp:100] Creating Layer loss
I0312 12:42:16.721846  3948 net.cpp:434] loss <- ip1_ip1_0_split_1
I0312 12:42:16.721846  3948 net.cpp:434] loss <- label_mnist_1_split_1
I0312 12:42:16.721846  3948 net.cpp:408] loss -> loss
I0312 12:42:16.721846  3948 layer_factory.cpp:58] Creating layer loss
I0312 12:42:16.721846  3948 net.cpp:150] Setting up loss
I0312 12:42:16.721846  3948 net.cpp:157] Top shape: (1)
I0312 12:42:16.721846  3948 net.cpp:160]     with loss weight 1
I0312 12:42:16.721846  3948 net.cpp:165] Memory required for data: 251795608
I0312 12:42:16.721846  3948 net.cpp:226] loss needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:228] accuracy does not need backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] ip1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] poolcp6 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu_cccp6 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] cccp6 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] poolcp5 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu_cccp5 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] cccp5 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu_cccp4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] cccp4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu4_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale4_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn4_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv4_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] pool4_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu4_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale4_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn4_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv4_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu4_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale4_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn4_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv4_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] pool4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv4 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu3 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale3 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn3 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv3 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu2_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale2_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn2_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv2_2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] pool2_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu2_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale2_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn2_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv2_1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv2 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu1_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale1_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn1_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv1_0 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] relu1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] scale1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] bn1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:226] conv1 needs backward computation.
I0312 12:42:16.721846  3948 net.cpp:228] label_mnist_1_split does not need backward computation.
I0312 12:42:16.721846  3948 net.cpp:228] mnist does not need backward computation.
I0312 12:42:16.721846  3948 net.cpp:270] This network produces output accuracy
I0312 12:42:16.721846  3948 net.cpp:270] This network produces output loss
I0312 12:42:16.721846  3948 net.cpp:283] Network initialization done.
I0312 12:42:16.721846  3948 solver.cpp:60] Solver scaffolding done.
I0312 12:42:16.721846  3948 caffe.cpp:155] Finetuning from examples/mnist/snaps/lenet_iter_16800.caffemodel
I0312 12:42:16.721846  3948 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/snaps/lenet_iter_16800.caffemodel
I0312 12:42:16.721846  3948 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 12:42:16.721846  3948 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/snaps/lenet_iter_16800.caffemodel
I0312 12:42:16.721846  3948 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 12:42:16.731853  3948 net.cpp:761] Ignoring source layer accuracy_training
I0312 12:42:16.731853  3948 caffe.cpp:252] Starting Optimization
I0312 12:42:16.731853  3948 solver.cpp:279] Solving LeNet
I0312 12:42:16.731853  3948 solver.cpp:280] Learning Rate Policy: poly
I0312 12:42:16.731853  3948 solver.cpp:337] Iteration 0, Testing net (#0)
I0312 12:42:16.751832  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:42:19.177997  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 12:42:19.178498  3948 solver.cpp:404]     Test net output #1: loss = 0.0137049 (* 1 = 0.0137049 loss)
I0312 12:42:19.391507  3948 solver.cpp:228] Iteration 0, loss = 0.00279194
I0312 12:42:19.391507  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:42:19.391507  3948 solver.cpp:244]     Train net output #1: loss = 0.00279194 (* 1 = 0.00279194 loss)
I0312 12:42:19.391507  3948 sgd_solver.cpp:106] Iteration 0, lr = 0.0198366
I0312 12:42:29.296658  3948 solver.cpp:228] Iteration 100, loss = 0.00853298
I0312 12:42:29.296658  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:42:29.296658  3948 solver.cpp:244]     Train net output #1: loss = 0.00853298 (* 1 = 0.00853298 loss)
I0312 12:42:29.296658  3948 sgd_solver.cpp:106] Iteration 100, lr = 0.0198341
I0312 12:42:39.379842  3948 solver.cpp:228] Iteration 200, loss = 0.00694872
I0312 12:42:39.379842  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:42:39.379842  3948 solver.cpp:244]     Train net output #1: loss = 0.00694872 (* 1 = 0.00694872 loss)
I0312 12:42:39.379842  3948 sgd_solver.cpp:106] Iteration 200, lr = 0.0198316
I0312 12:42:49.885220  3948 solver.cpp:228] Iteration 300, loss = 0.0379559
I0312 12:42:49.885220  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 12:42:49.885220  3948 solver.cpp:244]     Train net output #1: loss = 0.0379559 (* 1 = 0.0379559 loss)
I0312 12:42:49.885220  3948 sgd_solver.cpp:106] Iteration 300, lr = 0.0198291
I0312 12:43:00.162446  3948 solver.cpp:228] Iteration 400, loss = 0.00170922
I0312 12:43:00.162446  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:43:00.162446  3948 solver.cpp:244]     Train net output #1: loss = 0.00170921 (* 1 = 0.00170921 loss)
I0312 12:43:00.162446  3948 sgd_solver.cpp:106] Iteration 400, lr = 0.0198266
I0312 12:43:10.331979  3948 solver.cpp:228] Iteration 500, loss = 0.00546183
I0312 12:43:10.331979  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:43:10.331979  3948 solver.cpp:244]     Train net output #1: loss = 0.00546183 (* 1 = 0.00546183 loss)
I0312 12:43:10.331979  3948 sgd_solver.cpp:106] Iteration 500, lr = 0.0198242
I0312 12:43:20.747690  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_600.caffemodel
I0312 12:43:20.777690  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_600.solverstate
I0312 12:43:20.777690  3948 solver.cpp:337] Iteration 600, Testing net (#0)
I0312 12:43:20.777690  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:43:23.140300  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9895
I0312 12:43:23.140300  3948 solver.cpp:404]     Test net output #1: loss = 0.0337374 (* 1 = 0.0337374 loss)
I0312 12:43:23.165287  3948 solver.cpp:228] Iteration 600, loss = 0.032362
I0312 12:43:23.165287  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 12:43:23.165287  3948 solver.cpp:244]     Train net output #1: loss = 0.032362 (* 1 = 0.032362 loss)
I0312 12:43:23.165287  3948 sgd_solver.cpp:106] Iteration 600, lr = 0.0198217
I0312 12:43:33.253736  3948 solver.cpp:228] Iteration 700, loss = 0.0280451
I0312 12:43:33.253736  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 12:43:33.253736  3948 solver.cpp:244]     Train net output #1: loss = 0.0280451 (* 1 = 0.0280451 loss)
I0312 12:43:33.253736  3948 sgd_solver.cpp:106] Iteration 700, lr = 0.0198192
I0312 12:43:43.406354  3948 solver.cpp:228] Iteration 800, loss = 0.0326925
I0312 12:43:43.406354  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 12:43:43.406354  3948 solver.cpp:244]     Train net output #1: loss = 0.0326926 (* 1 = 0.0326926 loss)
I0312 12:43:43.406354  3948 sgd_solver.cpp:106] Iteration 800, lr = 0.0198167
I0312 12:43:53.539824  3948 solver.cpp:228] Iteration 900, loss = 0.024861
I0312 12:43:53.539824  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:43:53.539824  3948 solver.cpp:244]     Train net output #1: loss = 0.024861 (* 1 = 0.024861 loss)
I0312 12:43:53.539824  3948 sgd_solver.cpp:106] Iteration 900, lr = 0.0198142
I0312 12:44:03.715219  3948 solver.cpp:228] Iteration 1000, loss = 0.0218634
I0312 12:44:03.715219  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:44:03.715219  3948 solver.cpp:244]     Train net output #1: loss = 0.0218635 (* 1 = 0.0218635 loss)
I0312 12:44:03.715719  3948 sgd_solver.cpp:106] Iteration 1000, lr = 0.0198118
I0312 12:44:14.069485  3948 solver.cpp:228] Iteration 1100, loss = 0.00684706
I0312 12:44:14.069485  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:44:14.069485  3948 solver.cpp:244]     Train net output #1: loss = 0.0068471 (* 1 = 0.0068471 loss)
I0312 12:44:14.069485  3948 sgd_solver.cpp:106] Iteration 1100, lr = 0.0198093
I0312 12:44:24.269744  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1200.caffemodel
I0312 12:44:24.281244  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1200.solverstate
I0312 12:44:24.284245  3948 solver.cpp:337] Iteration 1200, Testing net (#0)
I0312 12:44:24.284245  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:44:26.618027  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9878
I0312 12:44:26.618528  3948 solver.cpp:404]     Test net output #1: loss = 0.0359597 (* 1 = 0.0359597 loss)
I0312 12:44:26.645081  3948 solver.cpp:228] Iteration 1200, loss = 0.0145701
I0312 12:44:26.645081  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:44:26.645081  3948 solver.cpp:244]     Train net output #1: loss = 0.0145702 (* 1 = 0.0145702 loss)
I0312 12:44:26.645081  3948 sgd_solver.cpp:106] Iteration 1200, lr = 0.0198068
I0312 12:44:36.820025  3948 solver.cpp:228] Iteration 1300, loss = 0.00367295
I0312 12:44:36.820025  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:44:36.820025  3948 solver.cpp:244]     Train net output #1: loss = 0.003673 (* 1 = 0.003673 loss)
I0312 12:44:36.820025  3948 sgd_solver.cpp:106] Iteration 1300, lr = 0.0198043
I0312 12:44:47.068586  3948 solver.cpp:228] Iteration 1400, loss = 0.0213225
I0312 12:44:47.068586  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 12:44:47.068586  3948 solver.cpp:244]     Train net output #1: loss = 0.0213225 (* 1 = 0.0213225 loss)
I0312 12:44:47.068586  3948 sgd_solver.cpp:106] Iteration 1400, lr = 0.0198018
I0312 12:44:57.512998  3948 solver.cpp:228] Iteration 1500, loss = 0.00979116
I0312 12:44:57.512998  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:44:57.512998  3948 solver.cpp:244]     Train net output #1: loss = 0.00979122 (* 1 = 0.00979122 loss)
I0312 12:44:57.512998  3948 sgd_solver.cpp:106] Iteration 1500, lr = 0.0197994
I0312 12:45:07.836825  3948 solver.cpp:228] Iteration 1600, loss = 0.00590623
I0312 12:45:07.836825  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:07.836825  3948 solver.cpp:244]     Train net output #1: loss = 0.00590628 (* 1 = 0.00590628 loss)
I0312 12:45:07.836825  3948 sgd_solver.cpp:106] Iteration 1600, lr = 0.0197969
I0312 12:45:18.016111  3948 solver.cpp:228] Iteration 1700, loss = 0.0025625
I0312 12:45:18.016111  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:18.016111  3948 solver.cpp:244]     Train net output #1: loss = 0.00256256 (* 1 = 0.00256256 loss)
I0312 12:45:18.016111  3948 sgd_solver.cpp:106] Iteration 1700, lr = 0.0197944
I0312 12:45:26.321775  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1800.caffemodel
I0312 12:45:26.343263  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1800.solverstate
I0312 12:45:26.346263  3948 solver.cpp:337] Iteration 1800, Testing net (#0)
I0312 12:45:26.346263  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:45:27.945674  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9936
I0312 12:45:27.945674  3948 solver.cpp:404]     Test net output #1: loss = 0.0214208 (* 1 = 0.0214208 loss)
I0312 12:45:27.982677  3948 solver.cpp:228] Iteration 1800, loss = 0.00274503
I0312 12:45:27.982677  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:27.982677  3948 solver.cpp:244]     Train net output #1: loss = 0.00274507 (* 1 = 0.00274507 loss)
I0312 12:45:27.982677  3948 sgd_solver.cpp:106] Iteration 1800, lr = 0.0197919
I0312 12:45:34.884915  3948 solver.cpp:228] Iteration 1900, loss = 0.00198261
I0312 12:45:34.884915  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:34.884915  3948 solver.cpp:244]     Train net output #1: loss = 0.00198265 (* 1 = 0.00198265 loss)
I0312 12:45:34.884915  3948 sgd_solver.cpp:106] Iteration 1900, lr = 0.0197894
I0312 12:45:41.737787  3948 solver.cpp:228] Iteration 2000, loss = 0.00305608
I0312 12:45:41.737787  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:41.737787  3948 solver.cpp:244]     Train net output #1: loss = 0.00305613 (* 1 = 0.00305613 loss)
I0312 12:45:41.737787  3948 sgd_solver.cpp:106] Iteration 2000, lr = 0.019787
I0312 12:45:51.441105  3948 solver.cpp:228] Iteration 2100, loss = 0.00450198
I0312 12:45:51.441105  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:45:51.441105  3948 solver.cpp:244]     Train net output #1: loss = 0.00450203 (* 1 = 0.00450203 loss)
I0312 12:45:51.441105  3948 sgd_solver.cpp:106] Iteration 2100, lr = 0.0197845
I0312 12:46:01.625416  3948 solver.cpp:228] Iteration 2200, loss = 0.00227315
I0312 12:46:01.625416  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:46:01.625416  3948 solver.cpp:244]     Train net output #1: loss = 0.00227319 (* 1 = 0.00227319 loss)
I0312 12:46:01.625416  3948 sgd_solver.cpp:106] Iteration 2200, lr = 0.019782
I0312 12:46:11.715529  3948 solver.cpp:228] Iteration 2300, loss = 0.000967888
I0312 12:46:11.715529  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:46:11.715529  3948 solver.cpp:244]     Train net output #1: loss = 0.000967933 (* 1 = 0.000967933 loss)
I0312 12:46:11.715529  3948 sgd_solver.cpp:106] Iteration 2300, lr = 0.0197795
I0312 12:46:21.929672  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_2400.caffemodel
I0312 12:46:21.959674  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_2400.solverstate
I0312 12:46:21.959674  3948 solver.cpp:337] Iteration 2400, Testing net (#0)
I0312 12:46:21.959674  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:46:24.273958  3948 solver.cpp:404]     Test net output #0: accuracy = 0.994
I0312 12:46:24.274447  3948 solver.cpp:404]     Test net output #1: loss = 0.0196797 (* 1 = 0.0196797 loss)
I0312 12:46:24.320484  3948 solver.cpp:228] Iteration 2400, loss = 0.00129668
I0312 12:46:24.320484  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:46:24.320484  3948 solver.cpp:244]     Train net output #1: loss = 0.00129672 (* 1 = 0.00129672 loss)
I0312 12:46:24.320484  3948 sgd_solver.cpp:106] Iteration 2400, lr = 0.0197771
I0312 12:46:34.603164  3948 solver.cpp:228] Iteration 2500, loss = 0.0125102
I0312 12:46:34.603164  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:46:34.603164  3948 solver.cpp:244]     Train net output #1: loss = 0.0125102 (* 1 = 0.0125102 loss)
I0312 12:46:34.603164  3948 sgd_solver.cpp:106] Iteration 2500, lr = 0.0197746
I0312 12:46:44.666460  3948 solver.cpp:228] Iteration 2600, loss = 0.0164286
I0312 12:46:44.666460  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:46:44.666460  3948 solver.cpp:244]     Train net output #1: loss = 0.0164287 (* 1 = 0.0164287 loss)
I0312 12:46:44.666460  3948 sgd_solver.cpp:106] Iteration 2600, lr = 0.0197721
I0312 12:46:54.775167  3948 solver.cpp:228] Iteration 2700, loss = 0.00366273
I0312 12:46:54.775167  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:46:54.775167  3948 solver.cpp:244]     Train net output #1: loss = 0.00366278 (* 1 = 0.00366278 loss)
I0312 12:46:54.775167  3948 sgd_solver.cpp:106] Iteration 2700, lr = 0.0197696
I0312 12:47:04.959636  3948 solver.cpp:228] Iteration 2800, loss = 0.00168978
I0312 12:47:04.959636  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:47:04.959636  3948 solver.cpp:244]     Train net output #1: loss = 0.00168982 (* 1 = 0.00168982 loss)
I0312 12:47:04.959636  3948 sgd_solver.cpp:106] Iteration 2800, lr = 0.0197671
I0312 12:47:15.033288  3948 solver.cpp:228] Iteration 2900, loss = 0.00063199
I0312 12:47:15.033288  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:47:15.033288  3948 solver.cpp:244]     Train net output #1: loss = 0.000632031 (* 1 = 0.000632031 loss)
I0312 12:47:15.033288  3948 sgd_solver.cpp:106] Iteration 2900, lr = 0.0197647
I0312 12:47:25.166133  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_3000.caffemodel
I0312 12:47:25.177162  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_3000.solverstate
I0312 12:47:25.180163  3948 solver.cpp:337] Iteration 3000, Testing net (#0)
I0312 12:47:25.180163  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:47:27.447258  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9945
I0312 12:47:27.447258  3948 solver.cpp:404]     Test net output #1: loss = 0.0194039 (* 1 = 0.0194039 loss)
I0312 12:47:27.477332  3948 solver.cpp:228] Iteration 3000, loss = 0.0127928
I0312 12:47:27.477332  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:47:27.477332  3948 solver.cpp:244]     Train net output #1: loss = 0.0127928 (* 1 = 0.0127928 loss)
I0312 12:47:27.477332  3948 sgd_solver.cpp:106] Iteration 3000, lr = 0.0197622
I0312 12:47:37.579324  3948 solver.cpp:228] Iteration 3100, loss = 0.00105987
I0312 12:47:37.579324  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:47:37.579324  3948 solver.cpp:244]     Train net output #1: loss = 0.00105991 (* 1 = 0.00105991 loss)
I0312 12:47:37.579324  3948 sgd_solver.cpp:106] Iteration 3100, lr = 0.0197597
I0312 12:47:47.812108  3948 solver.cpp:228] Iteration 3200, loss = 0.00327866
I0312 12:47:47.812108  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:47:47.812108  3948 solver.cpp:244]     Train net output #1: loss = 0.0032787 (* 1 = 0.0032787 loss)
I0312 12:47:47.812108  3948 sgd_solver.cpp:106] Iteration 3200, lr = 0.0197572
I0312 12:47:58.333556  3948 solver.cpp:228] Iteration 3300, loss = 0.00543463
I0312 12:47:58.333556  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:47:58.333556  3948 solver.cpp:244]     Train net output #1: loss = 0.00543468 (* 1 = 0.00543468 loss)
I0312 12:47:58.333556  3948 sgd_solver.cpp:106] Iteration 3300, lr = 0.0197547
I0312 12:48:08.771185  3948 solver.cpp:228] Iteration 3400, loss = 0.000249627
I0312 12:48:08.771185  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:48:08.771185  3948 solver.cpp:244]     Train net output #1: loss = 0.000249672 (* 1 = 0.000249672 loss)
I0312 12:48:08.771185  3948 sgd_solver.cpp:106] Iteration 3400, lr = 0.0197523
I0312 12:48:18.907783  3948 solver.cpp:228] Iteration 3500, loss = 0.000142269
I0312 12:48:18.907783  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:48:18.907783  3948 solver.cpp:244]     Train net output #1: loss = 0.000142315 (* 1 = 0.000142315 loss)
I0312 12:48:18.907783  3948 sgd_solver.cpp:106] Iteration 3500, lr = 0.0197498
I0312 12:48:29.129865  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_3600.caffemodel
I0312 12:48:29.155864  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_3600.solverstate
I0312 12:48:29.158864  3948 solver.cpp:337] Iteration 3600, Testing net (#0)
I0312 12:48:29.158864  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:48:31.456023  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9952
I0312 12:48:31.456023  3948 solver.cpp:404]     Test net output #1: loss = 0.0180032 (* 1 = 0.0180032 loss)
I0312 12:48:31.508023  3948 solver.cpp:228] Iteration 3600, loss = 0.000278942
I0312 12:48:31.508522  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:48:31.508522  3948 solver.cpp:244]     Train net output #1: loss = 0.000278988 (* 1 = 0.000278988 loss)
I0312 12:48:31.508522  3948 sgd_solver.cpp:106] Iteration 3600, lr = 0.0197473
I0312 12:48:41.860658  3948 solver.cpp:228] Iteration 3700, loss = 0.00190732
I0312 12:48:41.860658  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:48:41.860658  3948 solver.cpp:244]     Train net output #1: loss = 0.00190737 (* 1 = 0.00190737 loss)
I0312 12:48:41.860658  3948 sgd_solver.cpp:106] Iteration 3700, lr = 0.0197448
I0312 12:48:52.180646  3948 solver.cpp:228] Iteration 3800, loss = 0.000513461
I0312 12:48:52.190640  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:48:52.190640  3948 solver.cpp:244]     Train net output #1: loss = 0.000513506 (* 1 = 0.000513506 loss)
I0312 12:48:52.190640  3948 sgd_solver.cpp:106] Iteration 3800, lr = 0.0197423
I0312 12:49:02.338366  3948 solver.cpp:228] Iteration 3900, loss = 0.000134837
I0312 12:49:02.338366  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:02.338366  3948 solver.cpp:244]     Train net output #1: loss = 0.000134881 (* 1 = 0.000134881 loss)
I0312 12:49:02.338366  3948 sgd_solver.cpp:106] Iteration 3900, lr = 0.0197399
I0312 12:49:12.672629  3948 solver.cpp:228] Iteration 4000, loss = 0.000434164
I0312 12:49:12.672629  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:12.672629  3948 solver.cpp:244]     Train net output #1: loss = 0.00043421 (* 1 = 0.00043421 loss)
I0312 12:49:12.672629  3948 sgd_solver.cpp:106] Iteration 4000, lr = 0.0197374
I0312 12:49:22.901522  3948 solver.cpp:228] Iteration 4100, loss = 7.39449e-005
I0312 12:49:22.901522  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:22.901522  3948 solver.cpp:244]     Train net output #1: loss = 7.3989e-005 (* 1 = 7.3989e-005 loss)
I0312 12:49:22.901522  3948 sgd_solver.cpp:106] Iteration 4100, lr = 0.0197349
I0312 12:49:33.299991  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_4200.caffemodel
I0312 12:49:33.309993  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_4200.solverstate
I0312 12:49:33.309993  3948 solver.cpp:337] Iteration 4200, Testing net (#0)
I0312 12:49:33.309993  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:49:35.589243  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9946
I0312 12:49:35.589243  3948 solver.cpp:404]     Test net output #1: loss = 0.0193319 (* 1 = 0.0193319 loss)
I0312 12:49:35.621764  3948 solver.cpp:228] Iteration 4200, loss = 0.000732221
I0312 12:49:35.621764  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:35.621764  3948 solver.cpp:244]     Train net output #1: loss = 0.000732264 (* 1 = 0.000732264 loss)
I0312 12:49:35.621764  3948 sgd_solver.cpp:106] Iteration 4200, lr = 0.0197324
I0312 12:49:45.857856  3948 solver.cpp:228] Iteration 4300, loss = 0.00331879
I0312 12:49:45.857856  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:45.857856  3948 solver.cpp:244]     Train net output #1: loss = 0.00331883 (* 1 = 0.00331883 loss)
I0312 12:49:45.857856  3948 sgd_solver.cpp:106] Iteration 4300, lr = 0.0197299
I0312 12:49:56.342092  3948 solver.cpp:228] Iteration 4400, loss = 0.00122771
I0312 12:49:56.342092  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:49:56.342092  3948 solver.cpp:244]     Train net output #1: loss = 0.00122776 (* 1 = 0.00122776 loss)
I0312 12:49:56.342092  3948 sgd_solver.cpp:106] Iteration 4400, lr = 0.0197275
I0312 12:50:06.584893  3948 solver.cpp:228] Iteration 4500, loss = 0.000606146
I0312 12:50:06.584893  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:06.584893  3948 solver.cpp:244]     Train net output #1: loss = 0.00060619 (* 1 = 0.00060619 loss)
I0312 12:50:06.584893  3948 sgd_solver.cpp:106] Iteration 4500, lr = 0.019725
I0312 12:50:16.743592  3948 solver.cpp:228] Iteration 4600, loss = 0.000126078
I0312 12:50:16.743592  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:16.743592  3948 solver.cpp:244]     Train net output #1: loss = 0.000126122 (* 1 = 0.000126122 loss)
I0312 12:50:16.743592  3948 sgd_solver.cpp:106] Iteration 4600, lr = 0.0197225
I0312 12:50:27.033097  3948 solver.cpp:228] Iteration 4700, loss = 0.00275977
I0312 12:50:27.033097  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:27.033097  3948 solver.cpp:244]     Train net output #1: loss = 0.00275982 (* 1 = 0.00275982 loss)
I0312 12:50:27.033097  3948 sgd_solver.cpp:106] Iteration 4700, lr = 0.01972
I0312 12:50:37.308583  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_4800.caffemodel
I0312 12:50:37.355661  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_4800.solverstate
I0312 12:50:37.361662  3948 solver.cpp:337] Iteration 4800, Testing net (#0)
I0312 12:50:37.361662  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:50:39.786662  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9955
I0312 12:50:39.786662  3948 solver.cpp:404]     Test net output #1: loss = 0.0174931 (* 1 = 0.0174931 loss)
I0312 12:50:39.820161  3948 solver.cpp:228] Iteration 4800, loss = 0.00110455
I0312 12:50:39.820161  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:39.820161  3948 solver.cpp:244]     Train net output #1: loss = 0.00110459 (* 1 = 0.00110459 loss)
I0312 12:50:39.820161  3948 sgd_solver.cpp:106] Iteration 4800, lr = 0.0197175
I0312 12:50:50.183706  3948 solver.cpp:228] Iteration 4900, loss = 0.00109643
I0312 12:50:50.183706  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:50.183706  3948 solver.cpp:244]     Train net output #1: loss = 0.00109648 (* 1 = 0.00109648 loss)
I0312 12:50:50.183706  3948 sgd_solver.cpp:106] Iteration 4900, lr = 0.0197151
I0312 12:50:58.511250  3948 solver.cpp:228] Iteration 5000, loss = 0.00112555
I0312 12:50:58.511250  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:50:58.511250  3948 solver.cpp:244]     Train net output #1: loss = 0.0011256 (* 1 = 0.0011256 loss)
I0312 12:50:58.511250  3948 sgd_solver.cpp:106] Iteration 5000, lr = 0.0197126
I0312 12:51:05.534783  3948 solver.cpp:228] Iteration 5100, loss = 0.00100301
I0312 12:51:05.534783  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:51:05.534783  3948 solver.cpp:244]     Train net output #1: loss = 0.00100306 (* 1 = 0.00100306 loss)
I0312 12:51:05.534783  3948 sgd_solver.cpp:106] Iteration 5100, lr = 0.0197101
I0312 12:51:12.479360  3948 solver.cpp:228] Iteration 5200, loss = 3.58272e-005
I0312 12:51:12.479360  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:51:12.479360  3948 solver.cpp:244]     Train net output #1: loss = 3.58774e-005 (* 1 = 3.58774e-005 loss)
I0312 12:51:12.479861  3948 sgd_solver.cpp:106] Iteration 5200, lr = 0.0197076
I0312 12:51:22.032290  3948 solver.cpp:228] Iteration 5300, loss = 0.00977787
I0312 12:51:22.032290  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:51:22.032290  3948 solver.cpp:244]     Train net output #1: loss = 0.00977791 (* 1 = 0.00977791 loss)
I0312 12:51:22.032290  3948 sgd_solver.cpp:106] Iteration 5300, lr = 0.0197051
I0312 12:51:32.424579  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_5400.caffemodel
I0312 12:51:32.450083  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5400.solverstate
I0312 12:51:32.453583  3948 solver.cpp:337] Iteration 5400, Testing net (#0)
I0312 12:51:32.453583  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:51:34.857789  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9949
I0312 12:51:34.857789  3948 solver.cpp:404]     Test net output #1: loss = 0.017863 (* 1 = 0.017863 loss)
I0312 12:51:34.897912  3948 solver.cpp:228] Iteration 5400, loss = 0.000835039
I0312 12:51:34.897912  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:51:34.897912  3948 solver.cpp:244]     Train net output #1: loss = 0.000835088 (* 1 = 0.000835088 loss)
I0312 12:51:34.897912  3948 sgd_solver.cpp:106] Iteration 5400, lr = 0.0197027
I0312 12:51:45.055953  3948 solver.cpp:228] Iteration 5500, loss = 0.0033621
I0312 12:51:45.055953  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:51:45.055953  3948 solver.cpp:244]     Train net output #1: loss = 0.00336215 (* 1 = 0.00336215 loss)
I0312 12:51:45.055953  3948 sgd_solver.cpp:106] Iteration 5500, lr = 0.0197002
I0312 12:51:55.193377  3948 solver.cpp:228] Iteration 5600, loss = 0.00101019
I0312 12:51:55.193377  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:51:55.193377  3948 solver.cpp:244]     Train net output #1: loss = 0.00101024 (* 1 = 0.00101024 loss)
I0312 12:51:55.193377  3948 sgd_solver.cpp:106] Iteration 5600, lr = 0.0196977
I0312 12:52:05.292446  3948 solver.cpp:228] Iteration 5700, loss = 0.000409762
I0312 12:52:05.292446  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:52:05.292446  3948 solver.cpp:244]     Train net output #1: loss = 0.000409809 (* 1 = 0.000409809 loss)
I0312 12:52:05.292446  3948 sgd_solver.cpp:106] Iteration 5700, lr = 0.0196952
I0312 12:52:15.427811  3948 solver.cpp:228] Iteration 5800, loss = 0.015877
I0312 12:52:15.427811  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:52:15.427811  3948 solver.cpp:244]     Train net output #1: loss = 0.015877 (* 1 = 0.015877 loss)
I0312 12:52:15.427811  3948 sgd_solver.cpp:106] Iteration 5800, lr = 0.0196927
I0312 12:52:25.711522  3948 solver.cpp:228] Iteration 5900, loss = 0.000239189
I0312 12:52:25.711522  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:52:25.711522  3948 solver.cpp:244]     Train net output #1: loss = 0.000239235 (* 1 = 0.000239235 loss)
I0312 12:52:25.711522  3948 sgd_solver.cpp:106] Iteration 5900, lr = 0.0196903
I0312 12:52:35.845274  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_6000.caffemodel
I0312 12:52:35.865275  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_6000.solverstate
I0312 12:52:35.875280  3948 solver.cpp:337] Iteration 6000, Testing net (#0)
I0312 12:52:35.875280  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:52:38.176072  3948 solver.cpp:404]     Test net output #0: accuracy = 0.993
I0312 12:52:38.176072  3948 solver.cpp:404]     Test net output #1: loss = 0.0252391 (* 1 = 0.0252391 loss)
I0312 12:52:38.216079  3948 solver.cpp:228] Iteration 6000, loss = 0.010761
I0312 12:52:38.216079  3948 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 12:52:38.216079  3948 solver.cpp:244]     Train net output #1: loss = 0.010761 (* 1 = 0.010761 loss)
I0312 12:52:38.216079  3948 sgd_solver.cpp:106] Iteration 6000, lr = 0.0196878
I0312 12:52:48.484561  3948 solver.cpp:228] Iteration 6100, loss = 0.00039883
I0312 12:52:48.484561  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:52:48.484561  3948 solver.cpp:244]     Train net output #1: loss = 0.000398884 (* 1 = 0.000398884 loss)
I0312 12:52:48.484561  3948 sgd_solver.cpp:106] Iteration 6100, lr = 0.0196853
I0312 12:52:58.664733  3948 solver.cpp:228] Iteration 6200, loss = 0.00183771
I0312 12:52:58.664733  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:52:58.664733  3948 solver.cpp:244]     Train net output #1: loss = 0.00183777 (* 1 = 0.00183777 loss)
I0312 12:52:58.664733  3948 sgd_solver.cpp:106] Iteration 6200, lr = 0.0196828
I0312 12:53:09.026262  3948 solver.cpp:228] Iteration 6300, loss = 0.00242039
I0312 12:53:09.026262  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:53:09.026262  3948 solver.cpp:244]     Train net output #1: loss = 0.00242044 (* 1 = 0.00242044 loss)
I0312 12:53:09.026262  3948 sgd_solver.cpp:106] Iteration 6300, lr = 0.0196803
I0312 12:53:19.417934  3948 solver.cpp:228] Iteration 6400, loss = 8.46304e-005
I0312 12:53:19.417934  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:53:19.417934  3948 solver.cpp:244]     Train net output #1: loss = 8.46797e-005 (* 1 = 8.46797e-005 loss)
I0312 12:53:19.417934  3948 sgd_solver.cpp:106] Iteration 6400, lr = 0.0196779
I0312 12:53:29.927247  3948 solver.cpp:228] Iteration 6500, loss = 0.000210947
I0312 12:53:29.927247  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:53:29.927247  3948 solver.cpp:244]     Train net output #1: loss = 0.000210994 (* 1 = 0.000210994 loss)
I0312 12:53:29.927247  3948 sgd_solver.cpp:106] Iteration 6500, lr = 0.0196754
I0312 12:53:40.133857  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_6600.caffemodel
I0312 12:53:40.153842  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_6600.solverstate
I0312 12:53:40.153842  3948 solver.cpp:337] Iteration 6600, Testing net (#0)
I0312 12:53:40.153842  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:53:42.424710  3948 solver.cpp:404]     Test net output #0: accuracy = 0.995
I0312 12:53:42.424710  3948 solver.cpp:404]     Test net output #1: loss = 0.0191261 (* 1 = 0.0191261 loss)
I0312 12:53:42.454708  3948 solver.cpp:228] Iteration 6600, loss = 0.000685111
I0312 12:53:42.454708  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:53:42.454708  3948 solver.cpp:244]     Train net output #1: loss = 0.000685161 (* 1 = 0.000685161 loss)
I0312 12:53:42.454708  3948 sgd_solver.cpp:106] Iteration 6600, lr = 0.0196729
I0312 12:53:53.036538  3948 solver.cpp:228] Iteration 6700, loss = 0.000784335
I0312 12:53:53.036538  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:53:53.036538  3948 solver.cpp:244]     Train net output #1: loss = 0.000784385 (* 1 = 0.000784385 loss)
I0312 12:53:53.036538  3948 sgd_solver.cpp:106] Iteration 6700, lr = 0.0196704
I0312 12:54:03.227838  3948 solver.cpp:228] Iteration 6800, loss = 0.000502037
I0312 12:54:03.227838  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:03.227838  3948 solver.cpp:244]     Train net output #1: loss = 0.000502086 (* 1 = 0.000502086 loss)
I0312 12:54:03.227838  3948 sgd_solver.cpp:106] Iteration 6800, lr = 0.0196679
I0312 12:54:13.404752  3948 solver.cpp:228] Iteration 6900, loss = 0.00127007
I0312 12:54:13.404752  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:13.404752  3948 solver.cpp:244]     Train net output #1: loss = 0.00127013 (* 1 = 0.00127013 loss)
I0312 12:54:13.404752  3948 sgd_solver.cpp:106] Iteration 6900, lr = 0.0196655
I0312 12:54:23.610319  3948 solver.cpp:228] Iteration 7000, loss = 0.000123885
I0312 12:54:23.610319  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:23.610319  3948 solver.cpp:244]     Train net output #1: loss = 0.000123943 (* 1 = 0.000123943 loss)
I0312 12:54:23.610319  3948 sgd_solver.cpp:106] Iteration 7000, lr = 0.019663
I0312 12:54:33.768162  3948 solver.cpp:228] Iteration 7100, loss = 0.000558748
I0312 12:54:33.768661  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:33.768661  3948 solver.cpp:244]     Train net output #1: loss = 0.000558806 (* 1 = 0.000558806 loss)
I0312 12:54:33.768661  3948 sgd_solver.cpp:106] Iteration 7100, lr = 0.0196605
I0312 12:54:43.860755  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_7200.caffemodel
I0312 12:54:43.875756  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_7200.solverstate
I0312 12:54:43.878756  3948 solver.cpp:337] Iteration 7200, Testing net (#0)
I0312 12:54:43.878756  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:54:46.183966  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9937
I0312 12:54:46.183966  3948 solver.cpp:404]     Test net output #1: loss = 0.0223682 (* 1 = 0.0223682 loss)
I0312 12:54:46.253835  3948 solver.cpp:228] Iteration 7200, loss = 0.000269104
I0312 12:54:46.253835  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:46.253835  3948 solver.cpp:244]     Train net output #1: loss = 0.000269163 (* 1 = 0.000269163 loss)
I0312 12:54:46.253835  3948 sgd_solver.cpp:106] Iteration 7200, lr = 0.019658
I0312 12:54:56.637637  3948 solver.cpp:228] Iteration 7300, loss = 0.000577165
I0312 12:54:56.637637  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:54:56.637637  3948 solver.cpp:244]     Train net output #1: loss = 0.000577228 (* 1 = 0.000577228 loss)
I0312 12:54:56.637637  3948 sgd_solver.cpp:106] Iteration 7300, lr = 0.0196556
I0312 12:55:06.749218  3948 solver.cpp:228] Iteration 7400, loss = 0.000831632
I0312 12:55:06.749218  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:55:06.749218  3948 solver.cpp:244]     Train net output #1: loss = 0.000831693 (* 1 = 0.000831693 loss)
I0312 12:55:06.749218  3948 sgd_solver.cpp:106] Iteration 7400, lr = 0.0196531
I0312 12:55:17.092725  3948 solver.cpp:228] Iteration 7500, loss = 0.00172127
I0312 12:55:17.092725  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:55:17.092725  3948 solver.cpp:244]     Train net output #1: loss = 0.00172133 (* 1 = 0.00172133 loss)
I0312 12:55:17.092725  3948 sgd_solver.cpp:106] Iteration 7500, lr = 0.0196506
I0312 12:55:27.210698  3948 solver.cpp:228] Iteration 7600, loss = 4.99327e-005
I0312 12:55:27.210698  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:55:27.210698  3948 solver.cpp:244]     Train net output #1: loss = 4.99936e-005 (* 1 = 4.99936e-005 loss)
I0312 12:55:27.210698  3948 sgd_solver.cpp:106] Iteration 7600, lr = 0.0196481
I0312 12:55:37.325284  3948 solver.cpp:228] Iteration 7700, loss = 0.000286776
I0312 12:55:37.325783  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:55:37.325783  3948 solver.cpp:244]     Train net output #1: loss = 0.000286834 (* 1 = 0.000286834 loss)
I0312 12:55:37.325783  3948 sgd_solver.cpp:106] Iteration 7700, lr = 0.0196456
I0312 12:55:47.460064  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_7800.caffemodel
I0312 12:55:47.488065  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_7800.solverstate
I0312 12:55:47.491084  3948 solver.cpp:337] Iteration 7800, Testing net (#0)
I0312 12:55:47.491084  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:55:49.837457  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9957
I0312 12:55:49.837457  3948 solver.cpp:404]     Test net output #1: loss = 0.0155767 (* 1 = 0.0155767 loss)
I0312 12:55:49.877460  3948 solver.cpp:228] Iteration 7800, loss = 0.000112611
I0312 12:55:49.877460  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:55:49.877460  3948 solver.cpp:244]     Train net output #1: loss = 0.000112668 (* 1 = 0.000112668 loss)
I0312 12:55:49.877460  3948 sgd_solver.cpp:106] Iteration 7800, lr = 0.0196432
I0312 12:56:00.011615  3948 solver.cpp:228] Iteration 7900, loss = 0.00185376
I0312 12:56:00.011615  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:00.011615  3948 solver.cpp:244]     Train net output #1: loss = 0.00185382 (* 1 = 0.00185382 loss)
I0312 12:56:00.011615  3948 sgd_solver.cpp:106] Iteration 7900, lr = 0.0196407
I0312 12:56:10.302570  3948 solver.cpp:228] Iteration 8000, loss = 0.000583853
I0312 12:56:10.302570  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:10.302570  3948 solver.cpp:244]     Train net output #1: loss = 0.000583912 (* 1 = 0.000583912 loss)
I0312 12:56:10.302570  3948 sgd_solver.cpp:106] Iteration 8000, lr = 0.0196382
I0312 12:56:20.510411  3948 solver.cpp:228] Iteration 8100, loss = 8.58967e-005
I0312 12:56:20.510411  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:20.510411  3948 solver.cpp:244]     Train net output #1: loss = 8.59554e-005 (* 1 = 8.59554e-005 loss)
I0312 12:56:20.510411  3948 sgd_solver.cpp:106] Iteration 8100, lr = 0.0196357
I0312 12:56:28.820188  3948 solver.cpp:228] Iteration 8200, loss = 1.50309e-005
I0312 12:56:28.820188  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:28.820188  3948 solver.cpp:244]     Train net output #1: loss = 1.50901e-005 (* 1 = 1.50901e-005 loss)
I0312 12:56:28.820188  3948 sgd_solver.cpp:106] Iteration 8200, lr = 0.0196332
I0312 12:56:35.751940  3948 solver.cpp:228] Iteration 8300, loss = 6.993e-005
I0312 12:56:35.751940  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:35.751940  3948 solver.cpp:244]     Train net output #1: loss = 6.99914e-005 (* 1 = 6.99914e-005 loss)
I0312 12:56:35.751940  3948 sgd_solver.cpp:106] Iteration 8300, lr = 0.0196308
I0312 12:56:42.634461  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_8400.caffemodel
I0312 12:56:42.654471  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_8400.solverstate
I0312 12:56:42.654471  3948 solver.cpp:337] Iteration 8400, Testing net (#0)
I0312 12:56:42.654471  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:56:44.275152  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9957
I0312 12:56:44.275152  3948 solver.cpp:404]     Test net output #1: loss = 0.0152337 (* 1 = 0.0152337 loss)
I0312 12:56:44.305138  3948 solver.cpp:228] Iteration 8400, loss = 6.57449e-005
I0312 12:56:44.305138  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:44.305138  3948 solver.cpp:244]     Train net output #1: loss = 6.58064e-005 (* 1 = 6.58064e-005 loss)
I0312 12:56:44.305138  3948 sgd_solver.cpp:106] Iteration 8400, lr = 0.0196283
I0312 12:56:54.426076  3948 solver.cpp:228] Iteration 8500, loss = 6.95619e-005
I0312 12:56:54.426076  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:56:54.426076  3948 solver.cpp:244]     Train net output #1: loss = 6.9625e-005 (* 1 = 6.9625e-005 loss)
I0312 12:56:54.426076  3948 sgd_solver.cpp:106] Iteration 8500, lr = 0.0196258
I0312 12:57:04.680552  3948 solver.cpp:228] Iteration 8600, loss = 0.000370153
I0312 12:57:04.680552  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:04.680552  3948 solver.cpp:244]     Train net output #1: loss = 0.000370216 (* 1 = 0.000370216 loss)
I0312 12:57:04.680552  3948 sgd_solver.cpp:106] Iteration 8600, lr = 0.0196233
I0312 12:57:14.833571  3948 solver.cpp:228] Iteration 8700, loss = 0.000235748
I0312 12:57:14.833571  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:14.833571  3948 solver.cpp:244]     Train net output #1: loss = 0.00023581 (* 1 = 0.00023581 loss)
I0312 12:57:14.833571  3948 sgd_solver.cpp:106] Iteration 8700, lr = 0.0196208
I0312 12:57:24.976181  3948 solver.cpp:228] Iteration 8800, loss = 1.04168e-005
I0312 12:57:24.976181  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:24.976181  3948 solver.cpp:244]     Train net output #1: loss = 1.04797e-005 (* 1 = 1.04797e-005 loss)
I0312 12:57:24.976181  3948 sgd_solver.cpp:106] Iteration 8800, lr = 0.0196184
I0312 12:57:35.129243  3948 solver.cpp:228] Iteration 8900, loss = 5.98694e-005
I0312 12:57:35.129243  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:35.129243  3948 solver.cpp:244]     Train net output #1: loss = 5.99324e-005 (* 1 = 5.99324e-005 loss)
I0312 12:57:35.129243  3948 sgd_solver.cpp:106] Iteration 8900, lr = 0.0196159
I0312 12:57:45.250830  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_9000.caffemodel
I0312 12:57:45.260900  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_9000.solverstate
I0312 12:57:45.260900  3948 solver.cpp:337] Iteration 9000, Testing net (#0)
I0312 12:57:45.260900  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:57:47.521858  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9961
I0312 12:57:47.521858  3948 solver.cpp:404]     Test net output #1: loss = 0.0138539 (* 1 = 0.0138539 loss)
I0312 12:57:47.581864  3948 solver.cpp:228] Iteration 9000, loss = 0.000133974
I0312 12:57:47.581864  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:47.581864  3948 solver.cpp:244]     Train net output #1: loss = 0.000134037 (* 1 = 0.000134037 loss)
I0312 12:57:47.581864  3948 sgd_solver.cpp:106] Iteration 9000, lr = 0.0196134
I0312 12:57:57.736095  3948 solver.cpp:228] Iteration 9100, loss = 0.00054515
I0312 12:57:57.736095  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:57:57.736095  3948 solver.cpp:244]     Train net output #1: loss = 0.000545213 (* 1 = 0.000545213 loss)
I0312 12:57:57.736095  3948 sgd_solver.cpp:106] Iteration 9100, lr = 0.0196109
I0312 12:58:08.014266  3948 solver.cpp:228] Iteration 9200, loss = 0.000205938
I0312 12:58:08.014266  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:58:08.014266  3948 solver.cpp:244]     Train net output #1: loss = 0.000206001 (* 1 = 0.000206001 loss)
I0312 12:58:08.014266  3948 sgd_solver.cpp:106] Iteration 9200, lr = 0.0196084
I0312 12:58:18.198967  3948 solver.cpp:228] Iteration 9300, loss = 0.000111385
I0312 12:58:18.198967  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:58:18.198967  3948 solver.cpp:244]     Train net output #1: loss = 0.000111448 (* 1 = 0.000111448 loss)
I0312 12:58:18.198967  3948 sgd_solver.cpp:106] Iteration 9300, lr = 0.019606
I0312 12:58:28.726102  3948 solver.cpp:228] Iteration 9400, loss = 1.00827e-005
I0312 12:58:28.726102  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:58:28.726102  3948 solver.cpp:244]     Train net output #1: loss = 1.01456e-005 (* 1 = 1.01456e-005 loss)
I0312 12:58:28.726102  3948 sgd_solver.cpp:106] Iteration 9400, lr = 0.0196035
I0312 12:58:39.159039  3948 solver.cpp:228] Iteration 9500, loss = 0.000418937
I0312 12:58:39.159538  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:58:39.159538  3948 solver.cpp:244]     Train net output #1: loss = 0.000419001 (* 1 = 0.000419001 loss)
I0312 12:58:39.159538  3948 sgd_solver.cpp:106] Iteration 9500, lr = 0.019601
I0312 12:58:49.791123  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_9600.caffemodel
I0312 12:58:49.805125  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_9600.solverstate
I0312 12:58:49.808626  3948 solver.cpp:337] Iteration 9600, Testing net (#0)
I0312 12:58:49.808626  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:58:52.087625  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9961
I0312 12:58:52.088125  3948 solver.cpp:404]     Test net output #1: loss = 0.0137827 (* 1 = 0.0137827 loss)
I0312 12:58:52.119127  3948 solver.cpp:228] Iteration 9600, loss = 0.000372488
I0312 12:58:52.119127  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:58:52.119127  3948 solver.cpp:244]     Train net output #1: loss = 0.000372552 (* 1 = 0.000372552 loss)
I0312 12:58:52.119127  3948 sgd_solver.cpp:106] Iteration 9600, lr = 0.0195985
I0312 12:59:02.775406  3948 solver.cpp:228] Iteration 9700, loss = 0.0048707
I0312 12:59:02.775406  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:02.775406  3948 solver.cpp:244]     Train net output #1: loss = 0.00487076 (* 1 = 0.00487076 loss)
I0312 12:59:02.775406  3948 sgd_solver.cpp:106] Iteration 9700, lr = 0.019596
I0312 12:59:13.546385  3948 solver.cpp:228] Iteration 9800, loss = 0.000142433
I0312 12:59:13.546385  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:13.546385  3948 solver.cpp:244]     Train net output #1: loss = 0.000142496 (* 1 = 0.000142496 loss)
I0312 12:59:13.546385  3948 sgd_solver.cpp:106] Iteration 9800, lr = 0.0195936
I0312 12:59:24.236384  3948 solver.cpp:228] Iteration 9900, loss = 0.000103695
I0312 12:59:24.236886  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:24.236886  3948 solver.cpp:244]     Train net output #1: loss = 0.000103758 (* 1 = 0.000103758 loss)
I0312 12:59:24.236886  3948 sgd_solver.cpp:106] Iteration 9900, lr = 0.0195911
I0312 12:59:34.845918  3948 solver.cpp:228] Iteration 10000, loss = 1.23803e-005
I0312 12:59:34.845918  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:34.845918  3948 solver.cpp:244]     Train net output #1: loss = 1.24434e-005 (* 1 = 1.24434e-005 loss)
I0312 12:59:34.845918  3948 sgd_solver.cpp:106] Iteration 10000, lr = 0.0195886
I0312 12:59:45.448920  3948 solver.cpp:228] Iteration 10100, loss = 4.60432e-005
I0312 12:59:45.448920  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:45.448920  3948 solver.cpp:244]     Train net output #1: loss = 4.61062e-005 (* 1 = 4.61062e-005 loss)
I0312 12:59:45.448920  3948 sgd_solver.cpp:106] Iteration 10100, lr = 0.0195861
I0312 12:59:56.170919  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_10200.caffemodel
I0312 12:59:56.197419  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10200.solverstate
I0312 12:59:56.201419  3948 solver.cpp:337] Iteration 10200, Testing net (#0)
I0312 12:59:56.201419  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 12:59:58.458420  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9965
I0312 12:59:58.458420  3948 solver.cpp:404]     Test net output #1: loss = 0.0136733 (* 1 = 0.0136733 loss)
I0312 12:59:58.487421  3948 solver.cpp:228] Iteration 10200, loss = 4.9991e-005
I0312 12:59:58.487421  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 12:59:58.487421  3948 solver.cpp:244]     Train net output #1: loss = 5.0054e-005 (* 1 = 5.0054e-005 loss)
I0312 12:59:58.487421  3948 sgd_solver.cpp:106] Iteration 10200, lr = 0.0195836
I0312 13:00:09.095422  3948 solver.cpp:228] Iteration 10300, loss = 2.1438e-005
I0312 13:00:09.095422  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:00:09.095422  3948 solver.cpp:244]     Train net output #1: loss = 2.15011e-005 (* 1 = 2.15011e-005 loss)
I0312 13:00:09.095422  3948 sgd_solver.cpp:106] Iteration 10300, lr = 0.0195812
I0312 13:00:19.662920  3948 solver.cpp:228] Iteration 10400, loss = 0.000129726
I0312 13:00:19.662920  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:00:19.662920  3948 solver.cpp:244]     Train net output #1: loss = 0.000129789 (* 1 = 0.000129789 loss)
I0312 13:00:19.662920  3948 sgd_solver.cpp:106] Iteration 10400, lr = 0.0195787
I0312 13:00:30.322443  3948 solver.cpp:228] Iteration 10500, loss = 8.5395e-005
I0312 13:00:30.322443  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:00:30.322942  3948 solver.cpp:244]     Train net output #1: loss = 8.5458e-005 (* 1 = 8.5458e-005 loss)
I0312 13:00:30.322942  3948 sgd_solver.cpp:106] Iteration 10500, lr = 0.0195762
I0312 13:00:40.989920  3948 solver.cpp:228] Iteration 10600, loss = 1.34871e-005
I0312 13:00:40.989920  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:00:40.990420  3948 solver.cpp:244]     Train net output #1: loss = 1.35502e-005 (* 1 = 1.35502e-005 loss)
I0312 13:00:40.990420  3948 sgd_solver.cpp:106] Iteration 10600, lr = 0.0195737
I0312 13:00:51.724336  3948 solver.cpp:228] Iteration 10700, loss = 4.54405e-005
I0312 13:00:51.724336  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:00:51.724336  3948 solver.cpp:244]     Train net output #1: loss = 4.55036e-005 (* 1 = 4.55036e-005 loss)
I0312 13:00:51.724336  3948 sgd_solver.cpp:106] Iteration 10700, lr = 0.0195712
I0312 13:01:02.243227  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_10800.caffemodel
I0312 13:01:02.258725  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10800.solverstate
I0312 13:01:02.261726  3948 solver.cpp:337] Iteration 10800, Testing net (#0)
I0312 13:01:02.262226  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 13:01:04.487229  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 13:01:04.487229  3948 solver.cpp:404]     Test net output #1: loss = 0.0135991 (* 1 = 0.0135991 loss)
I0312 13:01:04.520725  3948 solver.cpp:228] Iteration 10800, loss = 4.57623e-005
I0312 13:01:04.520725  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:04.520725  3948 solver.cpp:244]     Train net output #1: loss = 4.58254e-005 (* 1 = 4.58254e-005 loss)
I0312 13:01:04.520725  3948 sgd_solver.cpp:106] Iteration 10800, lr = 0.0195688
I0312 13:01:14.624791  3948 solver.cpp:228] Iteration 10900, loss = 2.06294e-005
I0312 13:01:14.624791  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:14.624791  3948 solver.cpp:244]     Train net output #1: loss = 2.06925e-005 (* 1 = 2.06925e-005 loss)
I0312 13:01:14.624791  3948 sgd_solver.cpp:106] Iteration 10900, lr = 0.0195663
I0312 13:01:24.587893  3948 solver.cpp:228] Iteration 11000, loss = 0.000123093
I0312 13:01:24.587893  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:24.587893  3948 solver.cpp:244]     Train net output #1: loss = 0.000123156 (* 1 = 0.000123156 loss)
I0312 13:01:24.587893  3948 sgd_solver.cpp:106] Iteration 11000, lr = 0.0195638
I0312 13:01:35.044909  3948 solver.cpp:228] Iteration 11100, loss = 7.80759e-005
I0312 13:01:35.044909  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:35.045409  3948 solver.cpp:244]     Train net output #1: loss = 7.81389e-005 (* 1 = 7.81389e-005 loss)
I0312 13:01:35.045409  3948 sgd_solver.cpp:106] Iteration 11100, lr = 0.0195613
I0312 13:01:45.616921  3948 solver.cpp:228] Iteration 11200, loss = 1.39748e-005
I0312 13:01:45.616921  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:45.616921  3948 solver.cpp:244]     Train net output #1: loss = 1.40379e-005 (* 1 = 1.40379e-005 loss)
I0312 13:01:45.616921  3948 sgd_solver.cpp:106] Iteration 11200, lr = 0.0195588
I0312 13:01:56.246929  3948 solver.cpp:228] Iteration 11300, loss = 4.33914e-005
I0312 13:01:56.246929  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:01:56.246929  3948 solver.cpp:244]     Train net output #1: loss = 4.34545e-005 (* 1 = 4.34545e-005 loss)
I0312 13:01:56.246929  3948 sgd_solver.cpp:106] Iteration 11300, lr = 0.0195564
I0312 13:02:05.593945  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_11400.caffemodel
I0312 13:02:05.610966  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_11400.solverstate
I0312 13:02:05.614449  3948 solver.cpp:337] Iteration 11400, Testing net (#0)
I0312 13:02:05.614966  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 13:02:07.211716  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 13:02:07.211716  3948 solver.cpp:404]     Test net output #1: loss = 0.0135344 (* 1 = 0.0135344 loss)
I0312 13:02:07.239215  3948 solver.cpp:228] Iteration 11400, loss = 4.07353e-005
I0312 13:02:07.239215  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:07.239215  3948 solver.cpp:244]     Train net output #1: loss = 4.07983e-005 (* 1 = 4.07983e-005 loss)
I0312 13:02:07.239215  3948 sgd_solver.cpp:106] Iteration 11400, lr = 0.0195539
I0312 13:02:14.321215  3948 solver.cpp:228] Iteration 11500, loss = 2.09246e-005
I0312 13:02:14.321215  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:14.321215  3948 solver.cpp:244]     Train net output #1: loss = 2.09877e-005 (* 1 = 2.09877e-005 loss)
I0312 13:02:14.321215  3948 sgd_solver.cpp:106] Iteration 11500, lr = 0.0195514
I0312 13:02:21.163216  3948 solver.cpp:228] Iteration 11600, loss = 0.000117832
I0312 13:02:21.163216  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:21.163216  3948 solver.cpp:244]     Train net output #1: loss = 0.000117895 (* 1 = 0.000117895 loss)
I0312 13:02:21.163216  3948 sgd_solver.cpp:106] Iteration 11600, lr = 0.0195489
I0312 13:02:30.306993  3948 solver.cpp:228] Iteration 11700, loss = 7.46799e-005
I0312 13:02:30.306993  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:30.306993  3948 solver.cpp:244]     Train net output #1: loss = 7.4743e-005 (* 1 = 7.4743e-005 loss)
I0312 13:02:30.306993  3948 sgd_solver.cpp:106] Iteration 11700, lr = 0.0195464
I0312 13:02:40.256273  3948 solver.cpp:228] Iteration 11800, loss = 1.43953e-005
I0312 13:02:40.256273  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:40.256273  3948 solver.cpp:244]     Train net output #1: loss = 1.44584e-005 (* 1 = 1.44584e-005 loss)
I0312 13:02:40.256273  3948 sgd_solver.cpp:106] Iteration 11800, lr = 0.019544
I0312 13:02:50.187273  3948 solver.cpp:228] Iteration 11900, loss = 4.22932e-005
I0312 13:02:50.187273  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:02:50.187273  3948 solver.cpp:244]     Train net output #1: loss = 4.23562e-005 (* 1 = 4.23562e-005 loss)
I0312 13:02:50.187273  3948 sgd_solver.cpp:106] Iteration 11900, lr = 0.0195415
I0312 13:03:00.092773  3948 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_12000.caffemodel
I0312 13:03:00.103772  3948 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_12000.solverstate
I0312 13:03:00.106773  3948 solver.cpp:337] Iteration 12000, Testing net (#0)
I0312 13:03:00.106773  3948 net.cpp:693] Ignoring source layer accuracy_training
I0312 13:03:02.338773  3948 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 13:03:02.338773  3948 solver.cpp:404]     Test net output #1: loss = 0.0134623 (* 1 = 0.0134623 loss)
I0312 13:03:02.361289  3948 solver.cpp:228] Iteration 12000, loss = 3.9326e-005
I0312 13:03:02.361289  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:03:02.361289  3948 solver.cpp:244]     Train net output #1: loss = 3.9389e-005 (* 1 = 3.9389e-005 loss)
I0312 13:03:02.361289  3948 sgd_solver.cpp:106] Iteration 12000, lr = 0.019539
I0312 13:03:12.354274  3948 solver.cpp:228] Iteration 12100, loss = 2.13874e-005
I0312 13:03:12.354274  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:03:12.354274  3948 solver.cpp:244]     Train net output #1: loss = 2.14504e-005 (* 1 = 2.14504e-005 loss)
I0312 13:03:12.354274  3948 sgd_solver.cpp:106] Iteration 12100, lr = 0.0195365
I0312 13:03:22.306274  3948 solver.cpp:228] Iteration 12200, loss = 0.000113081
I0312 13:03:22.306274  3948 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 13:03:22.306274  3948 solver.cpp:244]     Train net output