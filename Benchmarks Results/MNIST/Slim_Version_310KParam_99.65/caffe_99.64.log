
G:\Caffe\examples\mnist>REM going to the caffe root 

G:\Caffe\examples\mnist>CD ../../ 

G:\Caffe>SET TOOLS=Build/x64/Release 

G:\Caffe>"Build/x64/Release/caffe.exe" train --solver=examples/mnist/lenet_solver.prototxt 
I0312 10:32:26.896250  7012 caffe.cpp:218] Using GPUs 0
I0312 10:32:27.195403  7012 caffe.cpp:223] GPU 0: GeForce GTX 980
I0312 10:32:27.457959  7012 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:27.457959  7012 solver.cpp:48] Initializing solver from parameters: 
test_iter: 200
test_interval: 600
base_lr: 0.1
display: 100
max_iter: 400000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.005
snapshot: 600
snapshot_prefix: "examples/mnist/snaps/lenet"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
delta: 0.001
stepvalue: 5000
stepvalue: 12500
stepvalue: 22000
stepvalue: 29600
stepvalue: 32000
stepvalue: 37000
type: "AdaDelta"
I0312 10:32:27.457959  7012 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0312 10:32:27.493460  7012 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I0312 10:32:27.493460  7012 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn1_0
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_1
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn2_2
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn3
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_1
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_2
I0312 10:32:27.493460  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer bn4_0
I0312 10:32:27.493960  7012 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0312 10:32:27.493960  7012 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  include {
    phase: TRAIN
  }
  batch_norm_param {
    use_global_stats: false
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy_training"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy_training"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0312 10:32:27.494460  7012 layer_factory.cpp:58] Creating layer mnist
I0312 10:32:27.494460  7012 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:27.494954  7012 net.cpp:100] Creating Layer mnist
I0312 10:32:27.494954  7012 net.cpp:408] mnist -> data
I0312 10:32:27.494954  7012 net.cpp:408] mnist -> label
I0312 10:32:27.494954 12212 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:27.573173 12212 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_train_lmdb_norm2
I0312 10:32:27.642264  7012 data_layer.cpp:41] output data size: 100,1,28,28
I0312 10:32:27.642264  7012 net.cpp:150] Setting up mnist
I0312 10:32:27.642264  7012 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0312 10:32:27.642264  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:27.642264  7012 net.cpp:165] Memory required for data: 314000
I0312 10:32:27.642264  7012 layer_factory.cpp:58] Creating layer label_mnist_1_split
I0312 10:32:27.642264  7012 net.cpp:100] Creating Layer label_mnist_1_split
I0312 10:32:27.642264  7012 net.cpp:434] label_mnist_1_split <- label
I0312 10:32:27.642264  7012 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0312 10:32:27.642264  7012 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0312 10:32:27.642264  7012 net.cpp:150] Setting up label_mnist_1_split
I0312 10:32:27.642264  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:27.642264  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:27.642264  7012 net.cpp:165] Memory required for data: 314800
I0312 10:32:27.642264  7012 layer_factory.cpp:58] Creating layer conv1
I0312 10:32:27.642264  7012 net.cpp:100] Creating Layer conv1
I0312 10:32:27.642264  7012 net.cpp:434] conv1 <- data
I0312 10:32:27.642264  7012 net.cpp:408] conv1 -> conv1
I0312 10:32:27.642264 12216 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:27.883996  7012 net.cpp:150] Setting up conv1
I0312 10:32:27.883996  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:27.883996  7012 net.cpp:165] Memory required for data: 20385200
I0312 10:32:27.883996  7012 layer_factory.cpp:58] Creating layer bn1
I0312 10:32:27.883996  7012 net.cpp:100] Creating Layer bn1
I0312 10:32:27.883996  7012 net.cpp:434] bn1 <- conv1
I0312 10:32:27.883996  7012 net.cpp:408] bn1 -> bn1
I0312 10:32:27.883996  7012 net.cpp:150] Setting up bn1
I0312 10:32:27.883996  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:27.883996  7012 net.cpp:165] Memory required for data: 40455600
I0312 10:32:27.883996  7012 layer_factory.cpp:58] Creating layer scale1
I0312 10:32:27.883996  7012 net.cpp:100] Creating Layer scale1
I0312 10:32:27.883996  7012 net.cpp:434] scale1 <- bn1
I0312 10:32:27.883996  7012 net.cpp:408] scale1 -> scale1
I0312 10:32:27.884503  7012 layer_factory.cpp:58] Creating layer scale1
I0312 10:32:27.884503  7012 net.cpp:150] Setting up scale1
I0312 10:32:27.884503  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:27.884503  7012 net.cpp:165] Memory required for data: 60526000
I0312 10:32:27.884503  7012 layer_factory.cpp:58] Creating layer relu1
I0312 10:32:27.884503  7012 net.cpp:100] Creating Layer relu1
I0312 10:32:27.884503  7012 net.cpp:434] relu1 <- scale1
I0312 10:32:27.884503  7012 net.cpp:408] relu1 -> relu1
I0312 10:32:27.884994  7012 net.cpp:150] Setting up relu1
I0312 10:32:27.884994  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:27.884994  7012 net.cpp:165] Memory required for data: 80596400
I0312 10:32:27.884994  7012 layer_factory.cpp:58] Creating layer conv1_0
I0312 10:32:27.884994  7012 net.cpp:100] Creating Layer conv1_0
I0312 10:32:27.884994  7012 net.cpp:434] conv1_0 <- relu1
I0312 10:32:27.884994  7012 net.cpp:408] conv1_0 -> conv1_0
I0312 10:32:27.886503  7012 net.cpp:150] Setting up conv1_0
I0312 10:32:27.886503  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.886503  7012 net.cpp:165] Memory required for data: 90631600
I0312 10:32:27.886503  7012 layer_factory.cpp:58] Creating layer bn1_0
I0312 10:32:27.886503  7012 net.cpp:100] Creating Layer bn1_0
I0312 10:32:27.886503  7012 net.cpp:434] bn1_0 <- conv1_0
I0312 10:32:27.886503  7012 net.cpp:408] bn1_0 -> bn1_0
I0312 10:32:27.887002  7012 net.cpp:150] Setting up bn1_0
I0312 10:32:27.887002  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.887002  7012 net.cpp:165] Memory required for data: 100666800
I0312 10:32:27.887002  7012 layer_factory.cpp:58] Creating layer scale1_0
I0312 10:32:27.887002  7012 net.cpp:100] Creating Layer scale1_0
I0312 10:32:27.887002  7012 net.cpp:434] scale1_0 <- bn1_0
I0312 10:32:27.887002  7012 net.cpp:408] scale1_0 -> scale1_0
I0312 10:32:27.887002  7012 layer_factory.cpp:58] Creating layer scale1_0
I0312 10:32:27.887002  7012 net.cpp:150] Setting up scale1_0
I0312 10:32:27.887002  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.887002  7012 net.cpp:165] Memory required for data: 110702000
I0312 10:32:27.887002  7012 layer_factory.cpp:58] Creating layer relu1_0
I0312 10:32:27.887002  7012 net.cpp:100] Creating Layer relu1_0
I0312 10:32:27.887002  7012 net.cpp:434] relu1_0 <- scale1_0
I0312 10:32:27.887002  7012 net.cpp:408] relu1_0 -> relu1_0
I0312 10:32:27.887501  7012 net.cpp:150] Setting up relu1_0
I0312 10:32:27.887501  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.887501  7012 net.cpp:165] Memory required for data: 120737200
I0312 10:32:27.887501  7012 layer_factory.cpp:58] Creating layer conv2
I0312 10:32:27.887501  7012 net.cpp:100] Creating Layer conv2
I0312 10:32:27.887501  7012 net.cpp:434] conv2 <- relu1_0
I0312 10:32:27.887501  7012 net.cpp:408] conv2 -> conv2
I0312 10:32:27.889495  7012 net.cpp:150] Setting up conv2
I0312 10:32:27.889495  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.889495  7012 net.cpp:165] Memory required for data: 130772400
I0312 10:32:27.889495  7012 layer_factory.cpp:58] Creating layer bn2
I0312 10:32:27.889495  7012 net.cpp:100] Creating Layer bn2
I0312 10:32:27.889495  7012 net.cpp:434] bn2 <- conv2
I0312 10:32:27.889495  7012 net.cpp:408] bn2 -> bn2
I0312 10:32:27.890003  7012 net.cpp:150] Setting up bn2
I0312 10:32:27.890003  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.890003  7012 net.cpp:165] Memory required for data: 140807600
I0312 10:32:27.890003  7012 layer_factory.cpp:58] Creating layer scale2
I0312 10:32:27.890003  7012 net.cpp:100] Creating Layer scale2
I0312 10:32:27.890003  7012 net.cpp:434] scale2 <- bn2
I0312 10:32:27.890003  7012 net.cpp:408] scale2 -> scale2
I0312 10:32:27.890003  7012 layer_factory.cpp:58] Creating layer scale2
I0312 10:32:27.890003  7012 net.cpp:150] Setting up scale2
I0312 10:32:27.890003  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.890003  7012 net.cpp:165] Memory required for data: 150842800
I0312 10:32:27.890003  7012 layer_factory.cpp:58] Creating layer relu2
I0312 10:32:27.890003  7012 net.cpp:100] Creating Layer relu2
I0312 10:32:27.890003  7012 net.cpp:434] relu2 <- scale2
I0312 10:32:27.890003  7012 net.cpp:408] relu2 -> relu2
I0312 10:32:27.890003  7012 net.cpp:150] Setting up relu2
I0312 10:32:27.890003  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.890003  7012 net.cpp:165] Memory required for data: 160878000
I0312 10:32:27.890003  7012 layer_factory.cpp:58] Creating layer conv2_1
I0312 10:32:27.890003  7012 net.cpp:100] Creating Layer conv2_1
I0312 10:32:27.890502  7012 net.cpp:434] conv2_1 <- relu2
I0312 10:32:27.890502  7012 net.cpp:408] conv2_1 -> conv2_1
I0312 10:32:27.891504  7012 net.cpp:150] Setting up conv2_1
I0312 10:32:27.891504  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.891504  7012 net.cpp:165] Memory required for data: 170913200
I0312 10:32:27.891504  7012 layer_factory.cpp:58] Creating layer bn2_1
I0312 10:32:27.891504  7012 net.cpp:100] Creating Layer bn2_1
I0312 10:32:27.891504  7012 net.cpp:434] bn2_1 <- conv2_1
I0312 10:32:27.891504  7012 net.cpp:408] bn2_1 -> bn2_1
I0312 10:32:27.891995  7012 net.cpp:150] Setting up bn2_1
I0312 10:32:27.891995  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.891995  7012 net.cpp:165] Memory required for data: 180948400
I0312 10:32:27.891995  7012 layer_factory.cpp:58] Creating layer scale2_1
I0312 10:32:27.891995  7012 net.cpp:100] Creating Layer scale2_1
I0312 10:32:27.891995  7012 net.cpp:434] scale2_1 <- bn2_1
I0312 10:32:27.891995  7012 net.cpp:408] scale2_1 -> scale2_1
I0312 10:32:27.891995  7012 layer_factory.cpp:58] Creating layer scale2_1
I0312 10:32:27.891995  7012 net.cpp:150] Setting up scale2_1
I0312 10:32:27.891995  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.891995  7012 net.cpp:165] Memory required for data: 190983600
I0312 10:32:27.891995  7012 layer_factory.cpp:58] Creating layer relu2_1
I0312 10:32:27.891995  7012 net.cpp:100] Creating Layer relu2_1
I0312 10:32:27.891995  7012 net.cpp:434] relu2_1 <- scale2_1
I0312 10:32:27.891995  7012 net.cpp:408] relu2_1 -> relu2_1
I0312 10:32:27.892495  7012 net.cpp:150] Setting up relu2_1
I0312 10:32:27.892495  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:27.892495  7012 net.cpp:165] Memory required for data: 201018800
I0312 10:32:27.892495  7012 layer_factory.cpp:58] Creating layer pool2_1
I0312 10:32:27.892495  7012 net.cpp:100] Creating Layer pool2_1
I0312 10:32:27.892495  7012 net.cpp:434] pool2_1 <- relu2_1
I0312 10:32:27.892495  7012 net.cpp:408] pool2_1 -> pool2_1
I0312 10:32:27.892495  7012 net.cpp:150] Setting up pool2_1
I0312 10:32:27.892495  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.892495  7012 net.cpp:165] Memory required for data: 203527600
I0312 10:32:27.892495  7012 layer_factory.cpp:58] Creating layer conv2_2
I0312 10:32:27.892495  7012 net.cpp:100] Creating Layer conv2_2
I0312 10:32:27.892495  7012 net.cpp:434] conv2_2 <- pool2_1
I0312 10:32:27.892495  7012 net.cpp:408] conv2_2 -> conv2_2
I0312 10:32:27.893996  7012 net.cpp:150] Setting up conv2_2
I0312 10:32:27.893996  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.893996  7012 net.cpp:165] Memory required for data: 206036400
I0312 10:32:27.893996  7012 layer_factory.cpp:58] Creating layer bn2_2
I0312 10:32:27.893996  7012 net.cpp:100] Creating Layer bn2_2
I0312 10:32:27.893996  7012 net.cpp:434] bn2_2 <- conv2_2
I0312 10:32:27.893996  7012 net.cpp:408] bn2_2 -> bn2_2
I0312 10:32:27.894503  7012 net.cpp:150] Setting up bn2_2
I0312 10:32:27.894503  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.894503  7012 net.cpp:165] Memory required for data: 208545200
I0312 10:32:27.894503  7012 layer_factory.cpp:58] Creating layer scale2_2
I0312 10:32:27.894503  7012 net.cpp:100] Creating Layer scale2_2
I0312 10:32:27.894503  7012 net.cpp:434] scale2_2 <- bn2_2
I0312 10:32:27.894503  7012 net.cpp:408] scale2_2 -> scale2_2
I0312 10:32:27.894503  7012 layer_factory.cpp:58] Creating layer scale2_2
I0312 10:32:27.894503  7012 net.cpp:150] Setting up scale2_2
I0312 10:32:27.894503  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.894503  7012 net.cpp:165] Memory required for data: 211054000
I0312 10:32:27.894503  7012 layer_factory.cpp:58] Creating layer relu2_2
I0312 10:32:27.894503  7012 net.cpp:100] Creating Layer relu2_2
I0312 10:32:27.894503  7012 net.cpp:434] relu2_2 <- scale2_2
I0312 10:32:27.894503  7012 net.cpp:408] relu2_2 -> relu2_2
I0312 10:32:27.894503  7012 net.cpp:150] Setting up relu2_2
I0312 10:32:27.894503  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.894992  7012 net.cpp:165] Memory required for data: 213562800
I0312 10:32:27.894992  7012 layer_factory.cpp:58] Creating layer conv3
I0312 10:32:27.894992  7012 net.cpp:100] Creating Layer conv3
I0312 10:32:27.894992  7012 net.cpp:434] conv3 <- relu2_2
I0312 10:32:27.894992  7012 net.cpp:408] conv3 -> conv3
I0312 10:32:27.896004  7012 net.cpp:150] Setting up conv3
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 216071600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer bn3
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer bn3
I0312 10:32:27.896004  7012 net.cpp:434] bn3 <- conv3
I0312 10:32:27.896004  7012 net.cpp:408] bn3 -> bn3
I0312 10:32:27.896004  7012 net.cpp:150] Setting up bn3
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 218580400
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale3
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer scale3
I0312 10:32:27.896004  7012 net.cpp:434] scale3 <- bn3
I0312 10:32:27.896004  7012 net.cpp:408] scale3 -> scale3
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale3
I0312 10:32:27.896004  7012 net.cpp:150] Setting up scale3
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 221089200
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu3
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu3
I0312 10:32:27.896004  7012 net.cpp:434] relu3 <- scale3
I0312 10:32:27.896004  7012 net.cpp:408] relu3 -> relu3
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu3
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 223598000
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer conv4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer conv4
I0312 10:32:27.896004  7012 net.cpp:434] conv4 <- relu3
I0312 10:32:27.896004  7012 net.cpp:408] conv4 -> conv4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up conv4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 14 14 (1254400)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 228615600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer pool4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer pool4
I0312 10:32:27.896004  7012 net.cpp:434] pool4 <- conv4
I0312 10:32:27.896004  7012 net.cpp:408] pool4 -> pool4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up pool4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 229870000
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer bn4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer bn4
I0312 10:32:27.896004  7012 net.cpp:434] bn4 <- pool4
I0312 10:32:27.896004  7012 net.cpp:408] bn4 -> bn4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up bn4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 231124400
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer scale4
I0312 10:32:27.896004  7012 net.cpp:434] scale4 <- bn4
I0312 10:32:27.896004  7012 net.cpp:408] scale4 -> scale4
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up scale4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 232378800
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu4
I0312 10:32:27.896004  7012 net.cpp:434] relu4 <- scale4
I0312 10:32:27.896004  7012 net.cpp:408] relu4 -> relu4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 233633200
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer conv4_1
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer conv4_1
I0312 10:32:27.896004  7012 net.cpp:434] conv4_1 <- relu4
I0312 10:32:27.896004  7012 net.cpp:408] conv4_1 -> conv4_1
I0312 10:32:27.896004  7012 net.cpp:150] Setting up conv4_1
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 234887600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer bn4_1
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer bn4_1
I0312 10:32:27.896004  7012 net.cpp:434] bn4_1 <- conv4_1
I0312 10:32:27.896004  7012 net.cpp:408] bn4_1 -> bn4_1
I0312 10:32:27.896004  7012 net.cpp:150] Setting up bn4_1
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 236142000
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_1
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer scale4_1
I0312 10:32:27.896004  7012 net.cpp:434] scale4_1 <- bn4_1
I0312 10:32:27.896004  7012 net.cpp:408] scale4_1 -> scale4_1
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_1
I0312 10:32:27.896004  7012 net.cpp:150] Setting up scale4_1
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 237396400
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu4_1
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu4_1
I0312 10:32:27.896004  7012 net.cpp:434] relu4_1 <- scale4_1
I0312 10:32:27.896004  7012 net.cpp:408] relu4_1 -> relu4_1
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu4_1
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 238650800
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer conv4_2
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer conv4_2
I0312 10:32:27.896004  7012 net.cpp:434] conv4_2 <- relu4_1
I0312 10:32:27.896004  7012 net.cpp:408] conv4_2 -> conv4_2
I0312 10:32:27.896004  7012 net.cpp:150] Setting up conv4_2
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 239905200
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer bn4_2
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer bn4_2
I0312 10:32:27.896004  7012 net.cpp:434] bn4_2 <- conv4_2
I0312 10:32:27.896004  7012 net.cpp:408] bn4_2 -> bn4_2
I0312 10:32:27.896004  7012 net.cpp:150] Setting up bn4_2
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 241159600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_2
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer scale4_2
I0312 10:32:27.896004  7012 net.cpp:434] scale4_2 <- bn4_2
I0312 10:32:27.896004  7012 net.cpp:408] scale4_2 -> scale4_2
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_2
I0312 10:32:27.896004  7012 net.cpp:150] Setting up scale4_2
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 242414000
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu4_2
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu4_2
I0312 10:32:27.896004  7012 net.cpp:434] relu4_2 <- scale4_2
I0312 10:32:27.896004  7012 net.cpp:408] relu4_2 -> relu4_2
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu4_2
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 243668400
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer pool4_2
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer pool4_2
I0312 10:32:27.896004  7012 net.cpp:434] pool4_2 <- relu4_2
I0312 10:32:27.896004  7012 net.cpp:408] pool4_2 -> pool4_2
I0312 10:32:27.896004  7012 net.cpp:150] Setting up pool4_2
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 244078000
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer conv4_0
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer conv4_0
I0312 10:32:27.896004  7012 net.cpp:434] conv4_0 <- pool4_2
I0312 10:32:27.896004  7012 net.cpp:408] conv4_0 -> conv4_0
I0312 10:32:27.896004  7012 net.cpp:150] Setting up conv4_0
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 244897200
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer bn4_0
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer bn4_0
I0312 10:32:27.896004  7012 net.cpp:434] bn4_0 <- conv4_0
I0312 10:32:27.896004  7012 net.cpp:408] bn4_0 -> bn4_0
I0312 10:32:27.896004  7012 net.cpp:150] Setting up bn4_0
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 245716400
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_0
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer scale4_0
I0312 10:32:27.896004  7012 net.cpp:434] scale4_0 <- bn4_0
I0312 10:32:27.896004  7012 net.cpp:408] scale4_0 -> scale4_0
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer scale4_0
I0312 10:32:27.896004  7012 net.cpp:150] Setting up scale4_0
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 246535600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu4_0
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu4_0
I0312 10:32:27.896004  7012 net.cpp:434] relu4_0 <- scale4_0
I0312 10:32:27.896004  7012 net.cpp:408] relu4_0 -> relu4_0
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu4_0
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 247354800
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer cccp4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer cccp4
I0312 10:32:27.896004  7012 net.cpp:434] cccp4 <- relu4_0
I0312 10:32:27.896004  7012 net.cpp:408] cccp4 -> cccp4
I0312 10:32:27.896004  7012 net.cpp:150] Setting up cccp4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 248993200
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer relu_cccp4
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer relu_cccp4
I0312 10:32:27.896004  7012 net.cpp:434] relu_cccp4 <- cccp4
I0312 10:32:27.896004  7012 net.cpp:395] relu_cccp4 -> cccp4 (in-place)
I0312 10:32:27.896004  7012 net.cpp:150] Setting up relu_cccp4
I0312 10:32:27.896004  7012 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 10:32:27.896004  7012 net.cpp:165] Memory required for data: 250631600
I0312 10:32:27.896004  7012 layer_factory.cpp:58] Creating layer cccp5
I0312 10:32:27.896004  7012 net.cpp:100] Creating Layer cccp5
I0312 10:32:27.896004  7012 net.cpp:434] cccp5 <- cccp4
I0312 10:32:27.896004  7012 net.cpp:408] cccp5 -> cccp5
I0312 10:32:27.911635  7012 net.cpp:150] Setting up cccp5
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251041200
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer relu_cccp5
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer relu_cccp5
I0312 10:32:27.911635  7012 net.cpp:434] relu_cccp5 <- cccp5
I0312 10:32:27.911635  7012 net.cpp:395] relu_cccp5 -> cccp5 (in-place)
I0312 10:32:27.911635  7012 net.cpp:150] Setting up relu_cccp5
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251450800
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer poolcp5
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer poolcp5
I0312 10:32:27.911635  7012 net.cpp:434] poolcp5 <- cccp5
I0312 10:32:27.911635  7012 net.cpp:408] poolcp5 -> poolcp5
I0312 10:32:27.911635  7012 net.cpp:150] Setting up poolcp5
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251553200
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer cccp6
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer cccp6
I0312 10:32:27.911635  7012 net.cpp:434] cccp6 <- poolcp5
I0312 10:32:27.911635  7012 net.cpp:408] cccp6 -> cccp6
I0312 10:32:27.911635  7012 net.cpp:150] Setting up cccp6
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251655600
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer relu_cccp6
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer relu_cccp6
I0312 10:32:27.911635  7012 net.cpp:434] relu_cccp6 <- cccp6
I0312 10:32:27.911635  7012 net.cpp:395] relu_cccp6 -> cccp6 (in-place)
I0312 10:32:27.911635  7012 net.cpp:150] Setting up relu_cccp6
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251758000
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer poolcp6
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer poolcp6
I0312 10:32:27.911635  7012 net.cpp:434] poolcp6 <- cccp6
I0312 10:32:27.911635  7012 net.cpp:408] poolcp6 -> poolcp6
I0312 10:32:27.911635  7012 net.cpp:150] Setting up poolcp6
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 64 1 1 (6400)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251783600
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer ip1
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer ip1
I0312 10:32:27.911635  7012 net.cpp:434] ip1 <- poolcp6
I0312 10:32:27.911635  7012 net.cpp:408] ip1 -> ip1
I0312 10:32:27.911635  7012 net.cpp:150] Setting up ip1
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251787600
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer ip1_ip1_0_split
I0312 10:32:27.911635  7012 net.cpp:434] ip1_ip1_0_split <- ip1
I0312 10:32:27.911635  7012 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0312 10:32:27.911635  7012 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0312 10:32:27.911635  7012 net.cpp:150] Setting up ip1_ip1_0_split
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251795600
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer accuracy_training
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer accuracy_training
I0312 10:32:27.911635  7012 net.cpp:434] accuracy_training <- ip1_ip1_0_split_0
I0312 10:32:27.911635  7012 net.cpp:434] accuracy_training <- label_mnist_1_split_0
I0312 10:32:27.911635  7012 net.cpp:408] accuracy_training -> accuracy_training
I0312 10:32:27.911635  7012 net.cpp:150] Setting up accuracy_training
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: (1)
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251795604
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer loss
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer loss
I0312 10:32:27.911635  7012 net.cpp:434] loss <- ip1_ip1_0_split_1
I0312 10:32:27.911635  7012 net.cpp:434] loss <- label_mnist_1_split_1
I0312 10:32:27.911635  7012 net.cpp:408] loss -> loss
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer loss
I0312 10:32:27.911635  7012 net.cpp:150] Setting up loss
I0312 10:32:27.911635  7012 net.cpp:157] Top shape: (1)
I0312 10:32:27.911635  7012 net.cpp:160]     with loss weight 1
I0312 10:32:27.911635  7012 net.cpp:165] Memory required for data: 251795608
I0312 10:32:27.911635  7012 net.cpp:226] loss needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:228] accuracy_training does not need backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] ip1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] poolcp6 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu_cccp6 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] cccp6 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] poolcp5 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu_cccp5 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] cccp5 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu_cccp4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] cccp4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu4_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale4_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn4_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv4_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] pool4_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu4_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale4_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn4_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv4_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu4_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale4_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn4_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv4_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] pool4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv4 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu3 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale3 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn3 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv3 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu2_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale2_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn2_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv2_2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] pool2_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu2_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale2_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn2_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv2_1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv2 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu1_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale1_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn1_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv1_0 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] relu1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] scale1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] bn1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:226] conv1 needs backward computation.
I0312 10:32:27.911635  7012 net.cpp:228] label_mnist_1_split does not need backward computation.
I0312 10:32:27.911635  7012 net.cpp:228] mnist does not need backward computation.
I0312 10:32:27.911635  7012 net.cpp:270] This network produces output accuracy_training
I0312 10:32:27.911635  7012 net.cpp:270] This network produces output loss
I0312 10:32:27.911635  7012 net.cpp:283] Network initialization done.
I0312 10:32:27.911635  7012 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: examples/mnist/lenet_train_test.prototxt
I0312 10:32:27.911635  7012 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0312 10:32:27.911635  7012 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test.prototxt
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn1_0
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_1
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn2_2
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn3
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_1
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_2
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer bn4_0
I0312 10:32:27.911635  7012 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer accuracy_training
I0312 10:32:27.911635  7012 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb_norm2"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv1_0"
  type: "Convolution"
  bottom: "relu1"
  top: "conv1_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1_0"
  type: "BatchNorm"
  bottom: "conv1_0"
  top: "bn1_0"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale1_0"
  type: "Scale"
  bottom: "bn1_0"
  top: "scale1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1_0"
  type: "ReLU"
  bottom: "scale1_0"
  top: "relu1_0"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1_0"
  top: "conv2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "relu2"
  top: "conv2_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_1"
  type: "BatchNorm"
  bottom: "conv2_1"
  top: "bn2_1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_1"
  type: "Scale"
  bottom: "bn2_1"
  top: "scale2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "scale2_1"
  top: "relu2_1"
}
layer {
  name: "pool2_1"
  type: "Pooling"
  bottom: "relu2_1"
  top: "pool2_1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "pool2_1"
  top: "conv2_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
  }
}
layer {
  name: "bn2_2"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "bn2_2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale2_2"
  type: "Scale"
  bottom: "bn2_2"
  top: "scale2_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "scale2_2"
  top: "relu2_2"
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "relu2_2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "pool4"
  top: "bn4"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "relu4"
  top: "conv4_1"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_1"
  type: "BatchNorm"
  bottom: "conv4_1"
  top: "bn4_1"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_1"
  type: "Scale"
  bottom: "bn4_1"
  top: "scale4_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "scale4_1"
  top: "relu4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "relu4_1"
  top: "conv4_2"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_2"
  type: "BatchNorm"
  bottom: "conv4_2"
  top: "bn4_2"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_2"
  type: "Scale"
  bottom: "bn4_2"
  top: "scale4_2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "scale4_2"
  top: "relu4_2"
}
layer {
  name: "pool4_2"
  type: "Pooling"
  bottom: "relu4_2"
  top: "pool4_2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_0"
  type: "Convolution"
  bottom: "pool4_2"
  top: "conv4_0"
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn4_0"
  type: "BatchNorm"
  bottom: "conv4_0"
  top: "bn4_0"
  include {
    phase: TEST
  }
  batch_norm_param {
    use_global_stats: true
    moving_average_fraction: 0.95
  }
}
layer {
  name: "scale4_0"
  type: "Scale"
  bottom: "bn4_0"
  top: "scale4_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_0"
  type: "ReLU"
  bottom: "scale4_0"
  top: "relu4_0"
}
layer {
  name: "cccp4"
  type: "Convolution"
  bottom: "relu4_0"
  top: "cccp4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp4"
  type: "ReLU"
  bottom: "cccp4"
  top: "cccp4"
}
layer {
  name: "cccp5"
  type: "Convolution"
  bottom: "cccp4"
  top: "cccp5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 1
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp5"
  type: "ReLU"
  bottom: "cccp5"
  top: "cccp5"
}
layer {
  name: "poolcp5"
  type: "Pooling"
  bottom: "cccp5"
  top: "poolcp5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "cccp6"
  type: "Convolution"
  bottom: "poolcp5"
  top: "cccp6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    group: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu_cccp6"
  type: "ReLU"
  bottom: "cccp6"
  top: "cccp6"
}
layer {
  name: "poolcp6"
  type: "Pooling"
  bottom: "cccp6"
  top: "poolcp6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "poolcp6"
  top: "ip1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I0312 10:32:27.911635  7012 layer_factory.cpp:58] Creating layer mnist
I0312 10:32:27.911635  7012 net.cpp:100] Creating Layer mnist
I0312 10:32:27.911635  7012 net.cpp:408] mnist -> data
I0312 10:32:27.911635  7012 net.cpp:408] mnist -> label
I0312 10:32:27.911635 12208 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:27.995848 12208 db_lmdb.cpp:40] Opened lmdb examples/mnist/mnist_test_lmdb_norm2
I0312 10:32:28.011519  7012 data_layer.cpp:41] output data size: 100,1,28,28
I0312 10:32:28.011519  7012 net.cpp:150] Setting up mnist
I0312 10:32:28.011519  7012 net.cpp:157] Top shape: 100 1 28 28 (78400)
I0312 10:32:28.011519  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:28.011519  7012 net.cpp:165] Memory required for data: 314000
I0312 10:32:28.011519  7012 layer_factory.cpp:58] Creating layer label_mnist_1_split
I0312 10:32:28.011519  7012 net.cpp:100] Creating Layer label_mnist_1_split
I0312 10:32:28.011519  7012 net.cpp:434] label_mnist_1_split <- label
I0312 10:32:28.011519  7012 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0312 10:32:28.011519  7012 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0312 10:32:28.011519  7012 net.cpp:150] Setting up label_mnist_1_split
I0312 10:32:28.011519  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:28.011519  7012 net.cpp:157] Top shape: 100 (100)
I0312 10:32:28.011519  7012 net.cpp:165] Memory required for data: 314800
I0312 10:32:28.011519  7012 layer_factory.cpp:58] Creating layer conv1
I0312 10:32:28.011519  7012 net.cpp:100] Creating Layer conv1
I0312 10:32:28.011519  7012 net.cpp:434] conv1 <- data
I0312 10:32:28.011519  7012 net.cpp:408] conv1 -> conv1
I0312 10:32:28.011519 11600 common.cpp:36] System entropy source not available, using fallback algorithm to generate seed instead.
I0312 10:32:28.011519  7012 net.cpp:150] Setting up conv1
I0312 10:32:28.011519  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:28.011519  7012 net.cpp:165] Memory required for data: 20385200
I0312 10:32:28.011519  7012 layer_factory.cpp:58] Creating layer bn1
I0312 10:32:28.011519  7012 net.cpp:100] Creating Layer bn1
I0312 10:32:28.011519  7012 net.cpp:434] bn1 <- conv1
I0312 10:32:28.011519  7012 net.cpp:408] bn1 -> bn1
I0312 10:32:28.027115  7012 net.cpp:150] Setting up bn1
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 40455600
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer scale1
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer scale1
I0312 10:32:28.027115  7012 net.cpp:434] scale1 <- bn1
I0312 10:32:28.027115  7012 net.cpp:408] scale1 -> scale1
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer scale1
I0312 10:32:28.027115  7012 net.cpp:150] Setting up scale1
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 60526000
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer relu1
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer relu1
I0312 10:32:28.027115  7012 net.cpp:434] relu1 <- scale1
I0312 10:32:28.027115  7012 net.cpp:408] relu1 -> relu1
I0312 10:32:28.027115  7012 net.cpp:150] Setting up relu1
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 64 28 28 (5017600)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 80596400
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer conv1_0
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer conv1_0
I0312 10:32:28.027115  7012 net.cpp:434] conv1_0 <- relu1
I0312 10:32:28.027115  7012 net.cpp:408] conv1_0 -> conv1_0
I0312 10:32:28.027115  7012 net.cpp:150] Setting up conv1_0
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 90631600
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer bn1_0
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer bn1_0
I0312 10:32:28.027115  7012 net.cpp:434] bn1_0 <- conv1_0
I0312 10:32:28.027115  7012 net.cpp:408] bn1_0 -> bn1_0
I0312 10:32:28.027115  7012 net.cpp:150] Setting up bn1_0
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 100666800
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer scale1_0
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer scale1_0
I0312 10:32:28.027115  7012 net.cpp:434] scale1_0 <- bn1_0
I0312 10:32:28.027115  7012 net.cpp:408] scale1_0 -> scale1_0
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer scale1_0
I0312 10:32:28.027115  7012 net.cpp:150] Setting up scale1_0
I0312 10:32:28.027115  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.027115  7012 net.cpp:165] Memory required for data: 110702000
I0312 10:32:28.027115  7012 layer_factory.cpp:58] Creating layer relu1_0
I0312 10:32:28.027115  7012 net.cpp:100] Creating Layer relu1_0
I0312 10:32:28.027115  7012 net.cpp:434] relu1_0 <- scale1_0
I0312 10:32:28.027115  7012 net.cpp:408] relu1_0 -> relu1_0
I0312 10:32:28.042737  7012 net.cpp:150] Setting up relu1_0
I0312 10:32:28.042737  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.042737  7012 net.cpp:165] Memory required for data: 120737200
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer conv2
I0312 10:32:28.042737  7012 net.cpp:100] Creating Layer conv2
I0312 10:32:28.042737  7012 net.cpp:434] conv2 <- relu1_0
I0312 10:32:28.042737  7012 net.cpp:408] conv2 -> conv2
I0312 10:32:28.042737  7012 net.cpp:150] Setting up conv2
I0312 10:32:28.042737  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.042737  7012 net.cpp:165] Memory required for data: 130772400
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer bn2
I0312 10:32:28.042737  7012 net.cpp:100] Creating Layer bn2
I0312 10:32:28.042737  7012 net.cpp:434] bn2 <- conv2
I0312 10:32:28.042737  7012 net.cpp:408] bn2 -> bn2
I0312 10:32:28.042737  7012 net.cpp:150] Setting up bn2
I0312 10:32:28.042737  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.042737  7012 net.cpp:165] Memory required for data: 140807600
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer scale2
I0312 10:32:28.042737  7012 net.cpp:100] Creating Layer scale2
I0312 10:32:28.042737  7012 net.cpp:434] scale2 <- bn2
I0312 10:32:28.042737  7012 net.cpp:408] scale2 -> scale2
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer scale2
I0312 10:32:28.042737  7012 net.cpp:150] Setting up scale2
I0312 10:32:28.042737  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.042737  7012 net.cpp:165] Memory required for data: 150842800
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer relu2
I0312 10:32:28.042737  7012 net.cpp:100] Creating Layer relu2
I0312 10:32:28.042737  7012 net.cpp:434] relu2 <- scale2
I0312 10:32:28.042737  7012 net.cpp:408] relu2 -> relu2
I0312 10:32:28.042737  7012 net.cpp:150] Setting up relu2
I0312 10:32:28.042737  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.042737  7012 net.cpp:165] Memory required for data: 160878000
I0312 10:32:28.042737  7012 layer_factory.cpp:58] Creating layer conv2_1
I0312 10:32:28.042737  7012 net.cpp:100] Creating Layer conv2_1
I0312 10:32:28.042737  7012 net.cpp:434] conv2_1 <- relu2
I0312 10:32:28.042737  7012 net.cpp:408] conv2_1 -> conv2_1
I0312 10:32:28.058384  7012 net.cpp:150] Setting up conv2_1
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 170913200
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer bn2_1
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer bn2_1
I0312 10:32:28.058384  7012 net.cpp:434] bn2_1 <- conv2_1
I0312 10:32:28.058384  7012 net.cpp:408] bn2_1 -> bn2_1
I0312 10:32:28.058384  7012 net.cpp:150] Setting up bn2_1
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 180948400
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale2_1
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer scale2_1
I0312 10:32:28.058384  7012 net.cpp:434] scale2_1 <- bn2_1
I0312 10:32:28.058384  7012 net.cpp:408] scale2_1 -> scale2_1
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale2_1
I0312 10:32:28.058384  7012 net.cpp:150] Setting up scale2_1
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 190983600
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer relu2_1
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer relu2_1
I0312 10:32:28.058384  7012 net.cpp:434] relu2_1 <- scale2_1
I0312 10:32:28.058384  7012 net.cpp:408] relu2_1 -> relu2_1
I0312 10:32:28.058384  7012 net.cpp:150] Setting up relu2_1
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 28 28 (2508800)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 201018800
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer pool2_1
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer pool2_1
I0312 10:32:28.058384  7012 net.cpp:434] pool2_1 <- relu2_1
I0312 10:32:28.058384  7012 net.cpp:408] pool2_1 -> pool2_1
I0312 10:32:28.058384  7012 net.cpp:150] Setting up pool2_1
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 203527600
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer conv2_2
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer conv2_2
I0312 10:32:28.058384  7012 net.cpp:434] conv2_2 <- pool2_1
I0312 10:32:28.058384  7012 net.cpp:408] conv2_2 -> conv2_2
I0312 10:32:28.058384  7012 net.cpp:150] Setting up conv2_2
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 206036400
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer bn2_2
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer bn2_2
I0312 10:32:28.058384  7012 net.cpp:434] bn2_2 <- conv2_2
I0312 10:32:28.058384  7012 net.cpp:408] bn2_2 -> bn2_2
I0312 10:32:28.058384  7012 net.cpp:150] Setting up bn2_2
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 208545200
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale2_2
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer scale2_2
I0312 10:32:28.058384  7012 net.cpp:434] scale2_2 <- bn2_2
I0312 10:32:28.058384  7012 net.cpp:408] scale2_2 -> scale2_2
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale2_2
I0312 10:32:28.058384  7012 net.cpp:150] Setting up scale2_2
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 211054000
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer relu2_2
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer relu2_2
I0312 10:32:28.058384  7012 net.cpp:434] relu2_2 <- scale2_2
I0312 10:32:28.058384  7012 net.cpp:408] relu2_2 -> relu2_2
I0312 10:32:28.058384  7012 net.cpp:150] Setting up relu2_2
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 213562800
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer conv3
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer conv3
I0312 10:32:28.058384  7012 net.cpp:434] conv3 <- relu2_2
I0312 10:32:28.058384  7012 net.cpp:408] conv3 -> conv3
I0312 10:32:28.058384  7012 net.cpp:150] Setting up conv3
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 216071600
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer bn3
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer bn3
I0312 10:32:28.058384  7012 net.cpp:434] bn3 <- conv3
I0312 10:32:28.058384  7012 net.cpp:408] bn3 -> bn3
I0312 10:32:28.058384  7012 net.cpp:150] Setting up bn3
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 218580400
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale3
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer scale3
I0312 10:32:28.058384  7012 net.cpp:434] scale3 <- bn3
I0312 10:32:28.058384  7012 net.cpp:408] scale3 -> scale3
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer scale3
I0312 10:32:28.058384  7012 net.cpp:150] Setting up scale3
I0312 10:32:28.058384  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.058384  7012 net.cpp:165] Memory required for data: 221089200
I0312 10:32:28.058384  7012 layer_factory.cpp:58] Creating layer relu3
I0312 10:32:28.058384  7012 net.cpp:100] Creating Layer relu3
I0312 10:32:28.058384  7012 net.cpp:434] relu3 <- scale3
I0312 10:32:28.058384  7012 net.cpp:408] relu3 -> relu3
I0312 10:32:28.073978  7012 net.cpp:150] Setting up relu3
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 32 14 14 (627200)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 223598000
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer conv4
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer conv4
I0312 10:32:28.073978  7012 net.cpp:434] conv4 <- relu3
I0312 10:32:28.073978  7012 net.cpp:408] conv4 -> conv4
I0312 10:32:28.073978  7012 net.cpp:150] Setting up conv4
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 14 14 (1254400)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 228615600
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer pool4
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer pool4
I0312 10:32:28.073978  7012 net.cpp:434] pool4 <- conv4
I0312 10:32:28.073978  7012 net.cpp:408] pool4 -> pool4
I0312 10:32:28.073978  7012 net.cpp:150] Setting up pool4
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 229870000
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer bn4
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer bn4
I0312 10:32:28.073978  7012 net.cpp:434] bn4 <- pool4
I0312 10:32:28.073978  7012 net.cpp:408] bn4 -> bn4
I0312 10:32:28.073978  7012 net.cpp:150] Setting up bn4
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 231124400
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer scale4
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer scale4
I0312 10:32:28.073978  7012 net.cpp:434] scale4 <- bn4
I0312 10:32:28.073978  7012 net.cpp:408] scale4 -> scale4
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer scale4
I0312 10:32:28.073978  7012 net.cpp:150] Setting up scale4
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 232378800
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer relu4
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer relu4
I0312 10:32:28.073978  7012 net.cpp:434] relu4 <- scale4
I0312 10:32:28.073978  7012 net.cpp:408] relu4 -> relu4
I0312 10:32:28.073978  7012 net.cpp:150] Setting up relu4
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 233633200
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer conv4_1
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer conv4_1
I0312 10:32:28.073978  7012 net.cpp:434] conv4_1 <- relu4
I0312 10:32:28.073978  7012 net.cpp:408] conv4_1 -> conv4_1
I0312 10:32:28.073978  7012 net.cpp:150] Setting up conv4_1
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 234887600
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer bn4_1
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer bn4_1
I0312 10:32:28.073978  7012 net.cpp:434] bn4_1 <- conv4_1
I0312 10:32:28.073978  7012 net.cpp:408] bn4_1 -> bn4_1
I0312 10:32:28.073978  7012 net.cpp:150] Setting up bn4_1
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 236142000
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer scale4_1
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer scale4_1
I0312 10:32:28.073978  7012 net.cpp:434] scale4_1 <- bn4_1
I0312 10:32:28.073978  7012 net.cpp:408] scale4_1 -> scale4_1
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer scale4_1
I0312 10:32:28.073978  7012 net.cpp:150] Setting up scale4_1
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 237396400
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer relu4_1
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer relu4_1
I0312 10:32:28.073978  7012 net.cpp:434] relu4_1 <- scale4_1
I0312 10:32:28.073978  7012 net.cpp:408] relu4_1 -> relu4_1
I0312 10:32:28.073978  7012 net.cpp:150] Setting up relu4_1
I0312 10:32:28.073978  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.073978  7012 net.cpp:165] Memory required for data: 238650800
I0312 10:32:28.073978  7012 layer_factory.cpp:58] Creating layer conv4_2
I0312 10:32:28.073978  7012 net.cpp:100] Creating Layer conv4_2
I0312 10:32:28.073978  7012 net.cpp:434] conv4_2 <- relu4_1
I0312 10:32:28.073978  7012 net.cpp:408] conv4_2 -> conv4_2
I0312 10:32:28.090646  7012 net.cpp:150] Setting up conv4_2
I0312 10:32:28.090646  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.090646  7012 net.cpp:165] Memory required for data: 239905200
I0312 10:32:28.090646  7012 layer_factory.cpp:58] Creating layer bn4_2
I0312 10:32:28.090646  7012 net.cpp:100] Creating Layer bn4_2
I0312 10:32:28.090646  7012 net.cpp:434] bn4_2 <- conv4_2
I0312 10:32:28.090646  7012 net.cpp:408] bn4_2 -> bn4_2
I0312 10:32:28.091145  7012 net.cpp:150] Setting up bn4_2
I0312 10:32:28.091145  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.091145  7012 net.cpp:165] Memory required for data: 241159600
I0312 10:32:28.091145  7012 layer_factory.cpp:58] Creating layer scale4_2
I0312 10:32:28.091145  7012 net.cpp:100] Creating Layer scale4_2
I0312 10:32:28.091145  7012 net.cpp:434] scale4_2 <- bn4_2
I0312 10:32:28.091145  7012 net.cpp:408] scale4_2 -> scale4_2
I0312 10:32:28.091145  7012 layer_factory.cpp:58] Creating layer scale4_2
I0312 10:32:28.091645  7012 net.cpp:150] Setting up scale4_2
I0312 10:32:28.091645  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.091645  7012 net.cpp:165] Memory required for data: 242414000
I0312 10:32:28.091645  7012 layer_factory.cpp:58] Creating layer relu4_2
I0312 10:32:28.091645  7012 net.cpp:100] Creating Layer relu4_2
I0312 10:32:28.091645  7012 net.cpp:434] relu4_2 <- scale4_2
I0312 10:32:28.091645  7012 net.cpp:408] relu4_2 -> relu4_2
I0312 10:32:28.092134  7012 net.cpp:150] Setting up relu4_2
I0312 10:32:28.092134  7012 net.cpp:157] Top shape: 100 64 7 7 (313600)
I0312 10:32:28.092134  7012 net.cpp:165] Memory required for data: 243668400
I0312 10:32:28.092134  7012 layer_factory.cpp:58] Creating layer pool4_2
I0312 10:32:28.092134  7012 net.cpp:100] Creating Layer pool4_2
I0312 10:32:28.092134  7012 net.cpp:434] pool4_2 <- relu4_2
I0312 10:32:28.092134  7012 net.cpp:408] pool4_2 -> pool4_2
I0312 10:32:28.092134  7012 net.cpp:150] Setting up pool4_2
I0312 10:32:28.092134  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:28.092134  7012 net.cpp:165] Memory required for data: 244078000
I0312 10:32:28.092134  7012 layer_factory.cpp:58] Creating layer conv4_0
I0312 10:32:28.092134  7012 net.cpp:100] Creating Layer conv4_0
I0312 10:32:28.092134  7012 net.cpp:434] conv4_0 <- pool4_2
I0312 10:32:28.092134  7012 net.cpp:408] conv4_0 -> conv4_0
I0312 10:32:28.095644  7012 net.cpp:150] Setting up conv4_0
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 244897200
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer bn4_0
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer bn4_0
I0312 10:32:28.095644  7012 net.cpp:434] bn4_0 <- conv4_0
I0312 10:32:28.095644  7012 net.cpp:408] bn4_0 -> bn4_0
I0312 10:32:28.095644  7012 net.cpp:150] Setting up bn4_0
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 245716400
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer scale4_0
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer scale4_0
I0312 10:32:28.095644  7012 net.cpp:434] scale4_0 <- bn4_0
I0312 10:32:28.095644  7012 net.cpp:408] scale4_0 -> scale4_0
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer scale4_0
I0312 10:32:28.095644  7012 net.cpp:150] Setting up scale4_0
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 246535600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer relu4_0
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer relu4_0
I0312 10:32:28.095644  7012 net.cpp:434] relu4_0 <- scale4_0
I0312 10:32:28.095644  7012 net.cpp:408] relu4_0 -> relu4_0
I0312 10:32:28.095644  7012 net.cpp:150] Setting up relu4_0
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 128 4 4 (204800)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 247354800
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer cccp4
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer cccp4
I0312 10:32:28.095644  7012 net.cpp:434] cccp4 <- relu4_0
I0312 10:32:28.095644  7012 net.cpp:408] cccp4 -> cccp4
I0312 10:32:28.095644  7012 net.cpp:150] Setting up cccp4
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 248993200
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer relu_cccp4
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer relu_cccp4
I0312 10:32:28.095644  7012 net.cpp:434] relu_cccp4 <- cccp4
I0312 10:32:28.095644  7012 net.cpp:395] relu_cccp4 -> cccp4 (in-place)
I0312 10:32:28.095644  7012 net.cpp:150] Setting up relu_cccp4
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 256 4 4 (409600)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 250631600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer cccp5
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer cccp5
I0312 10:32:28.095644  7012 net.cpp:434] cccp5 <- cccp4
I0312 10:32:28.095644  7012 net.cpp:408] cccp5 -> cccp5
I0312 10:32:28.095644  7012 net.cpp:150] Setting up cccp5
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251041200
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer relu_cccp5
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer relu_cccp5
I0312 10:32:28.095644  7012 net.cpp:434] relu_cccp5 <- cccp5
I0312 10:32:28.095644  7012 net.cpp:395] relu_cccp5 -> cccp5 (in-place)
I0312 10:32:28.095644  7012 net.cpp:150] Setting up relu_cccp5
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 4 4 (102400)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251450800
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer poolcp5
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer poolcp5
I0312 10:32:28.095644  7012 net.cpp:434] poolcp5 <- cccp5
I0312 10:32:28.095644  7012 net.cpp:408] poolcp5 -> poolcp5
I0312 10:32:28.095644  7012 net.cpp:150] Setting up poolcp5
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251553200
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer cccp6
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer cccp6
I0312 10:32:28.095644  7012 net.cpp:434] cccp6 <- poolcp5
I0312 10:32:28.095644  7012 net.cpp:408] cccp6 -> cccp6
I0312 10:32:28.095644  7012 net.cpp:150] Setting up cccp6
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251655600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer relu_cccp6
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer relu_cccp6
I0312 10:32:28.095644  7012 net.cpp:434] relu_cccp6 <- cccp6
I0312 10:32:28.095644  7012 net.cpp:395] relu_cccp6 -> cccp6 (in-place)
I0312 10:32:28.095644  7012 net.cpp:150] Setting up relu_cccp6
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 2 2 (25600)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251758000
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer poolcp6
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer poolcp6
I0312 10:32:28.095644  7012 net.cpp:434] poolcp6 <- cccp6
I0312 10:32:28.095644  7012 net.cpp:408] poolcp6 -> poolcp6
I0312 10:32:28.095644  7012 net.cpp:150] Setting up poolcp6
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 64 1 1 (6400)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251783600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer ip1
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer ip1
I0312 10:32:28.095644  7012 net.cpp:434] ip1 <- poolcp6
I0312 10:32:28.095644  7012 net.cpp:408] ip1 -> ip1
I0312 10:32:28.095644  7012 net.cpp:150] Setting up ip1
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251787600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer ip1_ip1_0_split
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer ip1_ip1_0_split
I0312 10:32:28.095644  7012 net.cpp:434] ip1_ip1_0_split <- ip1
I0312 10:32:28.095644  7012 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_0
I0312 10:32:28.095644  7012 net.cpp:408] ip1_ip1_0_split -> ip1_ip1_0_split_1
I0312 10:32:28.095644  7012 net.cpp:150] Setting up ip1_ip1_0_split
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: 100 10 (1000)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251795600
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer accuracy
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer accuracy
I0312 10:32:28.095644  7012 net.cpp:434] accuracy <- ip1_ip1_0_split_0
I0312 10:32:28.095644  7012 net.cpp:434] accuracy <- label_mnist_1_split_0
I0312 10:32:28.095644  7012 net.cpp:408] accuracy -> accuracy
I0312 10:32:28.095644  7012 net.cpp:150] Setting up accuracy
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: (1)
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251795604
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer loss
I0312 10:32:28.095644  7012 net.cpp:100] Creating Layer loss
I0312 10:32:28.095644  7012 net.cpp:434] loss <- ip1_ip1_0_split_1
I0312 10:32:28.095644  7012 net.cpp:434] loss <- label_mnist_1_split_1
I0312 10:32:28.095644  7012 net.cpp:408] loss -> loss
I0312 10:32:28.095644  7012 layer_factory.cpp:58] Creating layer loss
I0312 10:32:28.095644  7012 net.cpp:150] Setting up loss
I0312 10:32:28.095644  7012 net.cpp:157] Top shape: (1)
I0312 10:32:28.095644  7012 net.cpp:160]     with loss weight 1
I0312 10:32:28.095644  7012 net.cpp:165] Memory required for data: 251795608
I0312 10:32:28.095644  7012 net.cpp:226] loss needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:228] accuracy does not need backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] ip1_ip1_0_split needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] ip1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] poolcp6 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu_cccp6 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] cccp6 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] poolcp5 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu_cccp5 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] cccp5 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu_cccp4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] cccp4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu4_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale4_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn4_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv4_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] pool4_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu4_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale4_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn4_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv4_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu4_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale4_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn4_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv4_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] pool4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv4 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu3 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale3 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn3 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv3 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu2_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale2_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn2_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv2_2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] pool2_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu2_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale2_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn2_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv2_1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv2 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu1_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale1_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn1_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv1_0 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] relu1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] scale1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] bn1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:226] conv1 needs backward computation.
I0312 10:32:28.095644  7012 net.cpp:228] label_mnist_1_split does not need backward computation.
I0312 10:32:28.095644  7012 net.cpp:228] mnist does not need backward computation.
I0312 10:32:28.095644  7012 net.cpp:270] This network produces output accuracy
I0312 10:32:28.095644  7012 net.cpp:270] This network produces output loss
I0312 10:32:28.095644  7012 net.cpp:283] Network initialization done.
I0312 10:32:28.095644  7012 solver.cpp:60] Solver scaffolding done.
I0312 10:32:28.111497  7012 caffe.cpp:252] Starting Optimization
I0312 10:32:28.111497  7012 solver.cpp:279] Solving LeNet
I0312 10:32:28.111497  7012 solver.cpp:280] Learning Rate Policy: multistep
I0312 10:32:28.111497  7012 solver.cpp:337] Iteration 0, Testing net (#0)
I0312 10:32:28.111497  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:32:28.843857 11600 blocking_queue.cpp:50] Waiting for data
I0312 10:32:28.891741  7012 blocking_queue.cpp:50] Data layer prefetch queue empty
I0312 10:32:30.447105  7012 solver.cpp:404]     Test net output #0: accuracy = 0.0974
I0312 10:32:30.447105  7012 solver.cpp:404]     Test net output #1: loss = 78.83 (* 1 = 78.83 loss)
I0312 10:32:30.549523  7012 solver.cpp:228] Iteration 0, loss = 2.43434
I0312 10:32:30.549523  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.09
I0312 10:32:30.549523  7012 solver.cpp:244]     Train net output #1: loss = 2.43434 (* 1 = 2.43434 loss)
I0312 10:32:30.549523  7012 sgd_solver.cpp:106] Iteration 0, lr = 0.1
I0312 10:32:35.251230  7012 solver.cpp:228] Iteration 100, loss = 0.232641
I0312 10:32:35.251230  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.91
I0312 10:32:35.251230  7012 solver.cpp:244]     Train net output #1: loss = 0.232641 (* 1 = 0.232641 loss)
I0312 10:32:35.251230  7012 sgd_solver.cpp:106] Iteration 100, lr = 0.1
I0312 10:32:40.003923  7012 solver.cpp:228] Iteration 200, loss = 0.116911
I0312 10:32:40.003923  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.95
I0312 10:32:40.003923  7012 solver.cpp:244]     Train net output #1: loss = 0.116911 (* 1 = 0.116911 loss)
I0312 10:32:40.003923  7012 sgd_solver.cpp:106] Iteration 200, lr = 0.1
I0312 10:32:44.769507  7012 solver.cpp:228] Iteration 300, loss = 0.104282
I0312 10:32:44.769507  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.97
I0312 10:32:44.769507  7012 solver.cpp:244]     Train net output #1: loss = 0.104282 (* 1 = 0.104282 loss)
I0312 10:32:44.769507  7012 sgd_solver.cpp:106] Iteration 300, lr = 0.1
I0312 10:32:49.526317  7012 solver.cpp:228] Iteration 400, loss = 0.04023
I0312 10:32:49.526317  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:32:49.526317  7012 solver.cpp:244]     Train net output #1: loss = 0.0402299 (* 1 = 0.0402299 loss)
I0312 10:32:49.526317  7012 sgd_solver.cpp:106] Iteration 400, lr = 0.1
I0312 10:32:54.358340  7012 solver.cpp:228] Iteration 500, loss = 0.0423485
I0312 10:32:54.358340  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:32:54.358340  7012 solver.cpp:244]     Train net output #1: loss = 0.0423485 (* 1 = 0.0423485 loss)
I0312 10:32:54.358340  7012 sgd_solver.cpp:106] Iteration 500, lr = 0.1
I0312 10:32:59.170792  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_600.caffemodel
I0312 10:32:59.186792  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_600.solverstate
I0312 10:32:59.190793  7012 solver.cpp:337] Iteration 600, Testing net (#0)
I0312 10:32:59.190793  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:33:01.384855  7012 solver.cpp:404]     Test net output #0: accuracy = 0.988501
I0312 10:33:01.384855  7012 solver.cpp:404]     Test net output #1: loss = 0.0494354 (* 1 = 0.0494354 loss)
I0312 10:33:01.403844  7012 solver.cpp:228] Iteration 600, loss = 0.0352746
I0312 10:33:01.403844  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:33:01.403844  7012 solver.cpp:244]     Train net output #1: loss = 0.0352746 (* 1 = 0.0352746 loss)
I0312 10:33:01.403844  7012 sgd_solver.cpp:106] Iteration 600, lr = 0.1
I0312 10:33:06.200166  7012 solver.cpp:228] Iteration 700, loss = 0.0603139
I0312 10:33:06.200166  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:33:06.200166  7012 solver.cpp:244]     Train net output #1: loss = 0.0603139 (* 1 = 0.0603139 loss)
I0312 10:33:06.200166  7012 sgd_solver.cpp:106] Iteration 700, lr = 0.1
I0312 10:33:10.983858  7012 solver.cpp:228] Iteration 800, loss = 0.0407411
I0312 10:33:10.983858  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.97
I0312 10:33:10.983858  7012 solver.cpp:244]     Train net output #1: loss = 0.0407411 (* 1 = 0.0407411 loss)
I0312 10:33:10.983858  7012 sgd_solver.cpp:106] Iteration 800, lr = 0.1
I0312 10:33:15.779811  7012 solver.cpp:228] Iteration 900, loss = 0.0430615
I0312 10:33:15.779811  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:33:15.779811  7012 solver.cpp:244]     Train net output #1: loss = 0.0430614 (* 1 = 0.0430614 loss)
I0312 10:33:15.779811  7012 sgd_solver.cpp:106] Iteration 900, lr = 0.1
I0312 10:33:20.554194  7012 solver.cpp:228] Iteration 1000, loss = 0.0180652
I0312 10:33:20.554194  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:33:20.554194  7012 solver.cpp:244]     Train net output #1: loss = 0.0180652 (* 1 = 0.0180652 loss)
I0312 10:33:20.554194  7012 sgd_solver.cpp:106] Iteration 1000, lr = 0.1
I0312 10:33:25.309171  7012 solver.cpp:228] Iteration 1100, loss = 0.0206398
I0312 10:33:25.309171  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:33:25.309171  7012 solver.cpp:244]     Train net output #1: loss = 0.0206398 (* 1 = 0.0206398 loss)
I0312 10:33:25.309171  7012 sgd_solver.cpp:106] Iteration 1100, lr = 0.1
I0312 10:33:30.040815  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_1200.caffemodel
I0312 10:33:30.052312  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_1200.solverstate
I0312 10:33:30.056310  7012 solver.cpp:337] Iteration 1200, Testing net (#0)
I0312 10:33:30.056310  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:33:32.243628  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9906
I0312 10:33:32.244616  7012 solver.cpp:404]     Test net output #1: loss = 0.0395023 (* 1 = 0.0395023 loss)
I0312 10:33:32.262615  7012 solver.cpp:228] Iteration 1200, loss = 0.028163
I0312 10:33:32.262615  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:33:32.262615  7012 solver.cpp:244]     Train net output #1: loss = 0.028163 (* 1 = 0.028163 loss)
I0312 10:33:32.262615  7012 sgd_solver.cpp:106] Iteration 1200, lr = 0.1
I0312 10:33:37.006944  7012 solver.cpp:228] Iteration 1300, loss = 0.0732072
I0312 10:33:37.006944  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.97
I0312 10:33:37.006944  7012 solver.cpp:244]     Train net output #1: loss = 0.0732071 (* 1 = 0.0732071 loss)
I0312 10:33:37.006944  7012 sgd_solver.cpp:106] Iteration 1300, lr = 0.1
I0312 10:33:41.791851  7012 solver.cpp:228] Iteration 1400, loss = 0.0212524
I0312 10:33:41.791851  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:33:41.791851  7012 solver.cpp:244]     Train net output #1: loss = 0.0212523 (* 1 = 0.0212523 loss)
I0312 10:33:41.791851  7012 sgd_solver.cpp:106] Iteration 1400, lr = 0.1
I0312 10:33:46.453811  7012 solver.cpp:228] Iteration 1500, loss = 0.0267902
I0312 10:33:46.453811  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:33:46.453811  7012 solver.cpp:244]     Train net output #1: loss = 0.0267901 (* 1 = 0.0267901 loss)
I0312 10:33:46.453811  7012 sgd_solver.cpp:106] Iteration 1500, lr = 0.1
I0312 10:33:51.152484  7012 solver.cpp:228] Iteration 1600, loss = 0.00701965
I0312 10:33:51.152484  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:33:51.152484  7012 solver.cpp:244]     Train net output #1: loss = 0.00701955 (* 1 = 0.00701955 loss)
I0312 10:33:51.152484  7012 sgd_solver.cpp:106] Iteration 1600, lr = 0.1
I0312 10:33:55.841753  7012 solver.cpp:228] Iteration 1700, loss = 0.0220256
I0312 10:33:55.841753  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:33:55.841753  7012 solver.cpp:244]     Train net output #1: loss = 0.0220255 (* 1 = 0.0220255 loss)
I0312 10:33:55.841753  7012 sgd_solver.cpp:106] Iteration 1700, lr = 0.1
I0312 10:34:00.573006  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_1800.caffemodel
I0312 10:34:00.584506  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_1800.solverstate
I0312 10:34:00.589007  7012 solver.cpp:337] Iteration 1800, Testing net (#0)
I0312 10:34:00.589007  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:34:02.746579  7012 solver.cpp:404]     Test net output #0: accuracy = 0.502
I0312 10:34:02.746579  7012 solver.cpp:404]     Test net output #1: loss = 2.24888 (* 1 = 2.24888 loss)
I0312 10:34:02.764580  7012 solver.cpp:228] Iteration 1800, loss = 0.11459
I0312 10:34:02.764580  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.95
I0312 10:34:02.764580  7012 solver.cpp:244]     Train net output #1: loss = 0.11459 (* 1 = 0.11459 loss)
I0312 10:34:02.764580  7012 sgd_solver.cpp:106] Iteration 1800, lr = 0.1
I0312 10:34:07.402205  7012 solver.cpp:228] Iteration 1900, loss = 0.13335
I0312 10:34:07.402205  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.95
I0312 10:34:07.402205  7012 solver.cpp:244]     Train net output #1: loss = 0.13335 (* 1 = 0.13335 loss)
I0312 10:34:07.402205  7012 sgd_solver.cpp:106] Iteration 1900, lr = 0.1
I0312 10:34:12.037191  7012 solver.cpp:228] Iteration 2000, loss = 0.0319187
I0312 10:34:12.037191  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:34:12.037191  7012 solver.cpp:244]     Train net output #1: loss = 0.0319186 (* 1 = 0.0319186 loss)
I0312 10:34:12.037191  7012 sgd_solver.cpp:106] Iteration 2000, lr = 0.1
I0312 10:34:16.668763  7012 solver.cpp:228] Iteration 2100, loss = 0.0350003
I0312 10:34:16.668763  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:34:16.668763  7012 solver.cpp:244]     Train net output #1: loss = 0.0350002 (* 1 = 0.0350002 loss)
I0312 10:34:16.668763  7012 sgd_solver.cpp:106] Iteration 2100, lr = 0.1
I0312 10:34:21.305470  7012 solver.cpp:228] Iteration 2200, loss = 0.0160599
I0312 10:34:21.305470  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:34:21.305470  7012 solver.cpp:244]     Train net output #1: loss = 0.0160598 (* 1 = 0.0160598 loss)
I0312 10:34:21.305470  7012 sgd_solver.cpp:106] Iteration 2200, lr = 0.1
I0312 10:34:25.938006  7012 solver.cpp:228] Iteration 2300, loss = 0.0172974
I0312 10:34:25.938006  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:34:25.938006  7012 solver.cpp:244]     Train net output #1: loss = 0.0172973 (* 1 = 0.0172973 loss)
I0312 10:34:25.938006  7012 sgd_solver.cpp:106] Iteration 2300, lr = 0.1
I0312 10:34:30.536895  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_2400.caffemodel
I0312 10:34:30.547446  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_2400.solverstate
I0312 10:34:30.551446  7012 solver.cpp:337] Iteration 2400, Testing net (#0)
I0312 10:34:30.551446  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:34:32.707463  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9901
I0312 10:34:32.708462  7012 solver.cpp:404]     Test net output #1: loss = 0.0422552 (* 1 = 0.0422552 loss)
I0312 10:34:32.725448  7012 solver.cpp:228] Iteration 2400, loss = 0.0313051
I0312 10:34:32.725448  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:34:32.725448  7012 solver.cpp:244]     Train net output #1: loss = 0.0313049 (* 1 = 0.0313049 loss)
I0312 10:34:32.725448  7012 sgd_solver.cpp:106] Iteration 2400, lr = 0.1
I0312 10:34:37.342691  7012 solver.cpp:228] Iteration 2500, loss = 0.0676285
I0312 10:34:37.342691  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:34:37.342691  7012 solver.cpp:244]     Train net output #1: loss = 0.0676284 (* 1 = 0.0676284 loss)
I0312 10:34:37.342691  7012 sgd_solver.cpp:106] Iteration 2500, lr = 0.1
I0312 10:34:41.963192  7012 solver.cpp:228] Iteration 2600, loss = 0.0147986
I0312 10:34:41.963192  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:34:41.963192  7012 solver.cpp:244]     Train net output #1: loss = 0.0147985 (* 1 = 0.0147985 loss)
I0312 10:34:41.963192  7012 sgd_solver.cpp:106] Iteration 2600, lr = 0.1
I0312 10:34:46.584005  7012 solver.cpp:228] Iteration 2700, loss = 0.0269251
I0312 10:34:46.584005  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:34:46.584005  7012 solver.cpp:244]     Train net output #1: loss = 0.026925 (* 1 = 0.026925 loss)
I0312 10:34:46.584005  7012 sgd_solver.cpp:106] Iteration 2700, lr = 0.1
I0312 10:34:51.200821  7012 solver.cpp:228] Iteration 2800, loss = 0.00722292
I0312 10:34:51.200821  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:34:51.200821  7012 solver.cpp:244]     Train net output #1: loss = 0.00722281 (* 1 = 0.00722281 loss)
I0312 10:34:51.200821  7012 sgd_solver.cpp:106] Iteration 2800, lr = 0.1
I0312 10:34:55.815563  7012 solver.cpp:228] Iteration 2900, loss = 0.07073
I0312 10:34:55.815563  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:34:55.815563  7012 solver.cpp:244]     Train net output #1: loss = 0.0707298 (* 1 = 0.0707298 loss)
I0312 10:34:55.815563  7012 sgd_solver.cpp:106] Iteration 2900, lr = 0.1
I0312 10:35:00.419930  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_3000.caffemodel
I0312 10:35:00.430932  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_3000.solverstate
I0312 10:35:00.434931  7012 solver.cpp:337] Iteration 3000, Testing net (#0)
I0312 10:35:00.434931  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:35:02.589848  7012 solver.cpp:404]     Test net output #0: accuracy = 0.986501
I0312 10:35:02.589848  7012 solver.cpp:404]     Test net output #1: loss = 0.0569404 (* 1 = 0.0569404 loss)
I0312 10:35:02.607848  7012 solver.cpp:228] Iteration 3000, loss = 0.0859265
I0312 10:35:02.607848  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.97
I0312 10:35:02.607848  7012 solver.cpp:244]     Train net output #1: loss = 0.0859263 (* 1 = 0.0859263 loss)
I0312 10:35:02.607848  7012 sgd_solver.cpp:106] Iteration 3000, lr = 0.1
I0312 10:35:07.230773  7012 solver.cpp:228] Iteration 3100, loss = 0.0437398
I0312 10:35:07.230773  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:35:07.230773  7012 solver.cpp:244]     Train net output #1: loss = 0.0437396 (* 1 = 0.0437396 loss)
I0312 10:35:07.230773  7012 sgd_solver.cpp:106] Iteration 3100, lr = 0.1
I0312 10:35:11.853327  7012 solver.cpp:228] Iteration 3200, loss = 0.0343692
I0312 10:35:11.853327  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:35:11.853327  7012 solver.cpp:244]     Train net output #1: loss = 0.034369 (* 1 = 0.034369 loss)
I0312 10:35:11.853826  7012 sgd_solver.cpp:106] Iteration 3200, lr = 0.1
I0312 10:35:16.474733  7012 solver.cpp:228] Iteration 3300, loss = 0.0352913
I0312 10:35:16.474733  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:35:16.474733  7012 solver.cpp:244]     Train net output #1: loss = 0.0352912 (* 1 = 0.0352912 loss)
I0312 10:35:16.474733  7012 sgd_solver.cpp:106] Iteration 3300, lr = 0.1
I0312 10:35:21.098491  7012 solver.cpp:228] Iteration 3400, loss = 0.0144121
I0312 10:35:21.098491  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:21.098491  7012 solver.cpp:244]     Train net output #1: loss = 0.0144119 (* 1 = 0.0144119 loss)
I0312 10:35:21.098491  7012 sgd_solver.cpp:106] Iteration 3400, lr = 0.1
I0312 10:35:25.716897  7012 solver.cpp:228] Iteration 3500, loss = 0.0153803
I0312 10:35:25.716897  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:25.716897  7012 solver.cpp:244]     Train net output #1: loss = 0.0153802 (* 1 = 0.0153802 loss)
I0312 10:35:25.716897  7012 sgd_solver.cpp:106] Iteration 3500, lr = 0.1
I0312 10:35:30.318186  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_3600.caffemodel
I0312 10:35:30.329686  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_3600.solverstate
I0312 10:35:30.333689  7012 solver.cpp:337] Iteration 3600, Testing net (#0)
I0312 10:35:30.333689  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:35:32.489511  7012 solver.cpp:404]     Test net output #0: accuracy = 0.988901
I0312 10:35:32.489511  7012 solver.cpp:404]     Test net output #1: loss = 0.047502 (* 1 = 0.047502 loss)
I0312 10:35:32.506501  7012 solver.cpp:228] Iteration 3600, loss = 0.0277201
I0312 10:35:32.506501  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:32.506501  7012 solver.cpp:244]     Train net output #1: loss = 0.02772 (* 1 = 0.02772 loss)
I0312 10:35:32.506501  7012 sgd_solver.cpp:106] Iteration 3600, lr = 0.1
I0312 10:35:37.124346  7012 solver.cpp:228] Iteration 3700, loss = 0.041609
I0312 10:35:37.124346  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.98
I0312 10:35:37.124346  7012 solver.cpp:244]     Train net output #1: loss = 0.0416088 (* 1 = 0.0416088 loss)
I0312 10:35:37.124346  7012 sgd_solver.cpp:106] Iteration 3700, lr = 0.1
I0312 10:35:41.746071  7012 solver.cpp:228] Iteration 3800, loss = 0.0228866
I0312 10:35:41.746554  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:41.746554  7012 solver.cpp:244]     Train net output #1: loss = 0.0228865 (* 1 = 0.0228865 loss)
I0312 10:35:41.746554  7012 sgd_solver.cpp:106] Iteration 3800, lr = 0.1
I0312 10:35:46.364958  7012 solver.cpp:228] Iteration 3900, loss = 0.0358537
I0312 10:35:46.364958  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:35:46.364958  7012 solver.cpp:244]     Train net output #1: loss = 0.0358535 (* 1 = 0.0358535 loss)
I0312 10:35:46.364958  7012 sgd_solver.cpp:106] Iteration 3900, lr = 0.1
I0312 10:35:50.980186  7012 solver.cpp:228] Iteration 4000, loss = 0.00753507
I0312 10:35:50.980186  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:50.980186  7012 solver.cpp:244]     Train net output #1: loss = 0.00753493 (* 1 = 0.00753493 loss)
I0312 10:35:50.980186  7012 sgd_solver.cpp:106] Iteration 4000, lr = 0.1
I0312 10:35:55.611696  7012 solver.cpp:228] Iteration 4100, loss = 0.0227464
I0312 10:35:55.611696  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:35:55.611696  7012 solver.cpp:244]     Train net output #1: loss = 0.0227463 (* 1 = 0.0227463 loss)
I0312 10:35:55.611696  7012 sgd_solver.cpp:106] Iteration 4100, lr = 0.1
I0312 10:36:00.219662  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_4200.caffemodel
I0312 10:36:00.234166  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_4200.solverstate
I0312 10:36:00.238165  7012 solver.cpp:337] Iteration 4200, Testing net (#0)
I0312 10:36:00.238165  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:36:02.393255  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9844
I0312 10:36:02.393255  7012 solver.cpp:404]     Test net output #1: loss = 0.0588499 (* 1 = 0.0588499 loss)
I0312 10:36:02.410238  7012 solver.cpp:228] Iteration 4200, loss = 0.0450843
I0312 10:36:02.410238  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.97
I0312 10:36:02.411238  7012 solver.cpp:244]     Train net output #1: loss = 0.0450842 (* 1 = 0.0450842 loss)
I0312 10:36:02.411238  7012 sgd_solver.cpp:106] Iteration 4200, lr = 0.1
I0312 10:36:07.044778  7012 solver.cpp:228] Iteration 4300, loss = 0.0221859
I0312 10:36:07.044778  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:07.044778  7012 solver.cpp:244]     Train net output #1: loss = 0.0221857 (* 1 = 0.0221857 loss)
I0312 10:36:07.044778  7012 sgd_solver.cpp:106] Iteration 4300, lr = 0.1
I0312 10:36:11.669805  7012 solver.cpp:228] Iteration 4400, loss = 0.0137371
I0312 10:36:11.669805  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:11.669805  7012 solver.cpp:244]     Train net output #1: loss = 0.013737 (* 1 = 0.013737 loss)
I0312 10:36:11.669805  7012 sgd_solver.cpp:106] Iteration 4400, lr = 0.1
I0312 10:36:16.303335  7012 solver.cpp:228] Iteration 4500, loss = 0.0235441
I0312 10:36:16.303335  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:36:16.303335  7012 solver.cpp:244]     Train net output #1: loss = 0.023544 (* 1 = 0.023544 loss)
I0312 10:36:16.303335  7012 sgd_solver.cpp:106] Iteration 4500, lr = 0.1
I0312 10:36:20.988984  7012 solver.cpp:228] Iteration 4600, loss = 1.64936
I0312 10:36:20.988984  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.54
I0312 10:36:20.988984  7012 solver.cpp:244]     Train net output #1: loss = 1.64936 (* 1 = 1.64936 loss)
I0312 10:36:20.988984  7012 sgd_solver.cpp:106] Iteration 4600, lr = 0.1
I0312 10:36:25.625506  7012 solver.cpp:228] Iteration 4700, loss = 0.0260263
I0312 10:36:25.625506  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:36:25.625506  7012 solver.cpp:244]     Train net output #1: loss = 0.0260262 (* 1 = 0.0260262 loss)
I0312 10:36:25.625506  7012 sgd_solver.cpp:106] Iteration 4700, lr = 0.1
I0312 10:36:30.235507  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_4800.caffemodel
I0312 10:36:30.247007  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_4800.solverstate
I0312 10:36:30.250507  7012 solver.cpp:337] Iteration 4800, Testing net (#0)
I0312 10:36:30.250507  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:36:32.406591  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9907
I0312 10:36:32.406591  7012 solver.cpp:404]     Test net output #1: loss = 0.0369611 (* 1 = 0.0369611 loss)
I0312 10:36:32.423593  7012 solver.cpp:228] Iteration 4800, loss = 0.0331606
I0312 10:36:32.423593  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:36:32.424592  7012 solver.cpp:244]     Train net output #1: loss = 0.0331605 (* 1 = 0.0331605 loss)
I0312 10:36:32.424592  7012 sgd_solver.cpp:106] Iteration 4800, lr = 0.1
I0312 10:36:37.098214  7012 solver.cpp:228] Iteration 4900, loss = 0.0281572
I0312 10:36:37.098214  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:37.098214  7012 solver.cpp:244]     Train net output #1: loss = 0.0281571 (* 1 = 0.0281571 loss)
I0312 10:36:37.098214  7012 sgd_solver.cpp:106] Iteration 4900, lr = 0.1
I0312 10:36:41.765233  7012 solver.cpp:228] Iteration 5000, loss = 0.019001
I0312 10:36:41.765233  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:41.765233  7012 solver.cpp:244]     Train net output #1: loss = 0.0190009 (* 1 = 0.0190009 loss)
I0312 10:36:41.765233  7012 sgd_solver.cpp:46] MultiStep Status: Iteration 5000, step = 1
I0312 10:36:41.765233  7012 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I0312 10:36:46.410784  7012 solver.cpp:228] Iteration 5100, loss = 0.0402152
I0312 10:36:46.410784  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:36:46.410784  7012 solver.cpp:244]     Train net output #1: loss = 0.0402151 (* 1 = 0.0402151 loss)
I0312 10:36:46.410784  7012 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I0312 10:36:51.047899  7012 solver.cpp:228] Iteration 5200, loss = 0.00562905
I0312 10:36:51.047899  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:51.047899  7012 solver.cpp:244]     Train net output #1: loss = 0.00562894 (* 1 = 0.00562894 loss)
I0312 10:36:51.047899  7012 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I0312 10:36:55.719722  7012 solver.cpp:228] Iteration 5300, loss = 0.00671239
I0312 10:36:55.719722  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:36:55.719722  7012 solver.cpp:244]     Train net output #1: loss = 0.00671228 (* 1 = 0.00671228 loss)
I0312 10:36:55.719722  7012 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I0312 10:37:00.464392  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_5400.caffemodel
I0312 10:37:00.475893  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_5400.solverstate
I0312 10:37:00.479892  7012 solver.cpp:337] Iteration 5400, Testing net (#0)
I0312 10:37:00.479892  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:37:02.673508  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9956
I0312 10:37:02.673508  7012 solver.cpp:404]     Test net output #1: loss = 0.015482 (* 1 = 0.015482 loss)
I0312 10:37:02.692500  7012 solver.cpp:228] Iteration 5400, loss = 0.00522766
I0312 10:37:02.692500  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:02.692500  7012 solver.cpp:244]     Train net output #1: loss = 0.00522755 (* 1 = 0.00522755 loss)
I0312 10:37:02.692500  7012 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
I0312 10:37:07.467710  7012 solver.cpp:228] Iteration 5500, loss = 0.00755123
I0312 10:37:07.467710  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:07.467710  7012 solver.cpp:244]     Train net output #1: loss = 0.00755113 (* 1 = 0.00755113 loss)
I0312 10:37:07.467710  7012 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I0312 10:37:12.233760  7012 solver.cpp:228] Iteration 5600, loss = 0.0119594
I0312 10:37:12.233760  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:12.233760  7012 solver.cpp:244]     Train net output #1: loss = 0.0119593 (* 1 = 0.0119593 loss)
I0312 10:37:12.233760  7012 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I0312 10:37:16.995426  7012 solver.cpp:228] Iteration 5700, loss = 0.021661
I0312 10:37:16.995928  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:37:16.995928  7012 solver.cpp:244]     Train net output #1: loss = 0.0216609 (* 1 = 0.0216609 loss)
I0312 10:37:16.995928  7012 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I0312 10:37:21.759557  7012 solver.cpp:228] Iteration 5800, loss = 0.0039075
I0312 10:37:21.760057  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:21.760057  7012 solver.cpp:244]     Train net output #1: loss = 0.00390739 (* 1 = 0.00390739 loss)
I0312 10:37:21.760057  7012 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I0312 10:37:26.533648  7012 solver.cpp:228] Iteration 5900, loss = 0.00715348
I0312 10:37:26.533648  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:26.533648  7012 solver.cpp:244]     Train net output #1: loss = 0.00715337 (* 1 = 0.00715337 loss)
I0312 10:37:26.533648  7012 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I0312 10:37:31.293303  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_6000.caffemodel
I0312 10:37:31.304785  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_6000.solverstate
I0312 10:37:31.308782  7012 solver.cpp:337] Iteration 6000, Testing net (#0)
I0312 10:37:31.308782  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:37:33.539326  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:37:33.539326  7012 solver.cpp:404]     Test net output #1: loss = 0.0148292 (* 1 = 0.0148292 loss)
I0312 10:37:33.557327  7012 solver.cpp:228] Iteration 6000, loss = 0.00476354
I0312 10:37:33.557327  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:33.557327  7012 solver.cpp:244]     Train net output #1: loss = 0.00476344 (* 1 = 0.00476344 loss)
I0312 10:37:33.557327  7012 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I0312 10:37:38.368845  7012 solver.cpp:228] Iteration 6100, loss = 0.00697212
I0312 10:37:38.368845  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:38.368845  7012 solver.cpp:244]     Train net output #1: loss = 0.00697202 (* 1 = 0.00697202 loss)
I0312 10:37:38.368845  7012 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I0312 10:37:43.149613  7012 solver.cpp:228] Iteration 6200, loss = 0.0107884
I0312 10:37:43.149613  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:43.149613  7012 solver.cpp:244]     Train net output #1: loss = 0.0107883 (* 1 = 0.0107883 loss)
I0312 10:37:43.149613  7012 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I0312 10:37:47.922013  7012 solver.cpp:228] Iteration 6300, loss = 0.0154001
I0312 10:37:47.922513  7012 solver.cpp:244]     Train net output #0: accuracy_training = 0.99
I0312 10:37:47.922513  7012 solver.cpp:244]     Train net output #1: loss = 0.0154 (* 1 = 0.0154 loss)
I0312 10:37:47.922513  7012 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I0312 10:37:52.681658  7012 solver.cpp:228] Iteration 6400, loss = 0.00358864
I0312 10:37:52.681658  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:52.681658  7012 solver.cpp:244]     Train net output #1: loss = 0.00358853 (* 1 = 0.00358853 loss)
I0312 10:37:52.681658  7012 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
I0312 10:37:57.466768  7012 solver.cpp:228] Iteration 6500, loss = 0.00704994
I0312 10:37:57.466768  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:37:57.466768  7012 solver.cpp:244]     Train net output #1: loss = 0.00704983 (* 1 = 0.00704983 loss)
I0312 10:37:57.466768  7012 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I0312 10:38:02.205874  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_6600.caffemodel
I0312 10:38:02.217875  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_6600.solverstate
I0312 10:38:02.221874  7012 solver.cpp:337] Iteration 6600, Testing net (#0)
I0312 10:38:02.221874  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:38:04.409176  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:38:04.409176  7012 solver.cpp:404]     Test net output #1: loss = 0.0145628 (* 1 = 0.0145628 loss)
I0312 10:38:04.427172  7012 solver.cpp:228] Iteration 6600, loss = 0.00437222
I0312 10:38:04.427172  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:04.427172  7012 solver.cpp:244]     Train net output #1: loss = 0.00437211 (* 1 = 0.00437211 loss)
I0312 10:38:04.427172  7012 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I0312 10:38:09.177841  7012 solver.cpp:228] Iteration 6700, loss = 0.00671442
I0312 10:38:09.177841  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:09.177841  7012 solver.cpp:244]     Train net output #1: loss = 0.00671431 (* 1 = 0.00671431 loss)
I0312 10:38:09.177841  7012 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I0312 10:38:13.935935  7012 solver.cpp:228] Iteration 6800, loss = 0.0101195
I0312 10:38:13.935935  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:13.935935  7012 solver.cpp:244]     Train net output #1: loss = 0.0101194 (* 1 = 0.0101194 loss)
I0312 10:38:13.935935  7012 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I0312 10:38:18.708518  7012 solver.cpp:228] Iteration 6900, loss = 0.0126082
I0312 10:38:18.708518  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:18.708518  7012 solver.cpp:244]     Train net output #1: loss = 0.0126081 (* 1 = 0.0126081 loss)
I0312 10:38:18.708518  7012 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I0312 10:38:23.481294  7012 solver.cpp:228] Iteration 7000, loss = 0.00343383
I0312 10:38:23.481294  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:23.481294  7012 solver.cpp:244]     Train net output #1: loss = 0.00343372 (* 1 = 0.00343372 loss)
I0312 10:38:23.481294  7012 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I0312 10:38:28.120404  7012 solver.cpp:228] Iteration 7100, loss = 0.00670469
I0312 10:38:28.120903  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:28.120903  7012 solver.cpp:244]     Train net output #1: loss = 0.00670458 (* 1 = 0.00670458 loss)
I0312 10:38:28.120903  7012 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I0312 10:38:32.766607  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_7200.caffemodel
I0312 10:38:32.779106  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_7200.solverstate
I0312 10:38:32.783107  7012 solver.cpp:337] Iteration 7200, Testing net (#0)
I0312 10:38:32.783107  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:38:34.947214  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:38:34.947214  7012 solver.cpp:404]     Test net output #1: loss = 0.0144392 (* 1 = 0.0144392 loss)
I0312 10:38:34.965214  7012 solver.cpp:228] Iteration 7200, loss = 0.00406623
I0312 10:38:34.965214  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:34.965214  7012 solver.cpp:244]     Train net output #1: loss = 0.00406612 (* 1 = 0.00406612 loss)
I0312 10:38:34.965214  7012 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I0312 10:38:39.606297  7012 solver.cpp:228] Iteration 7300, loss = 0.00639877
I0312 10:38:39.606796  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:39.606796  7012 solver.cpp:244]     Train net output #1: loss = 0.00639867 (* 1 = 0.00639867 loss)
I0312 10:38:39.606796  7012 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I0312 10:38:44.259110  7012 solver.cpp:228] Iteration 7400, loss = 0.00953176
I0312 10:38:44.259110  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:44.259110  7012 solver.cpp:244]     Train net output #1: loss = 0.00953166 (* 1 = 0.00953166 loss)
I0312 10:38:44.259110  7012 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
I0312 10:38:48.905669  7012 solver.cpp:228] Iteration 7500, loss = 0.0106949
I0312 10:38:48.905669  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:48.905669  7012 solver.cpp:244]     Train net output #1: loss = 0.0106948 (* 1 = 0.0106948 loss)
I0312 10:38:48.905669  7012 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I0312 10:38:53.555791  7012 solver.cpp:228] Iteration 7600, loss = 0.00323306
I0312 10:38:53.555791  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:53.555791  7012 solver.cpp:244]     Train net output #1: loss = 0.00323296 (* 1 = 0.00323296 loss)
I0312 10:38:53.555791  7012 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I0312 10:38:58.200050  7012 solver.cpp:228] Iteration 7700, loss = 0.00619685
I0312 10:38:58.200547  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:38:58.200547  7012 solver.cpp:244]     Train net output #1: loss = 0.00619675 (* 1 = 0.00619675 loss)
I0312 10:38:58.200547  7012 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I0312 10:39:02.816082  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_7800.caffemodel
I0312 10:39:02.827081  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_7800.solverstate
I0312 10:39:02.831081  7012 solver.cpp:337] Iteration 7800, Testing net (#0)
I0312 10:39:02.831081  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:39:04.993671  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9958
I0312 10:39:04.993671  7012 solver.cpp:404]     Test net output #1: loss = 0.014395 (* 1 = 0.014395 loss)
I0312 10:39:05.011173  7012 solver.cpp:228] Iteration 7800, loss = 0.00382772
I0312 10:39:05.011173  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:05.011173  7012 solver.cpp:244]     Train net output #1: loss = 0.00382763 (* 1 = 0.00382763 loss)
I0312 10:39:05.011173  7012 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I0312 10:39:09.642707  7012 solver.cpp:228] Iteration 7900, loss = 0.00605201
I0312 10:39:09.642707  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:09.642707  7012 solver.cpp:244]     Train net output #1: loss = 0.00605192 (* 1 = 0.00605192 loss)
I0312 10:39:09.642707  7012 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I0312 10:39:14.276325  7012 solver.cpp:228] Iteration 8000, loss = 0.00904102
I0312 10:39:14.276325  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:14.276325  7012 solver.cpp:244]     Train net output #1: loss = 0.00904093 (* 1 = 0.00904093 loss)
I0312 10:39:14.276325  7012 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I0312 10:39:18.903369  7012 solver.cpp:228] Iteration 8100, loss = 0.00941873
I0312 10:39:18.903369  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:18.903369  7012 solver.cpp:244]     Train net output #1: loss = 0.00941864 (* 1 = 0.00941864 loss)
I0312 10:39:18.903369  7012 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I0312 10:39:23.530925  7012 solver.cpp:228] Iteration 8200, loss = 0.00312416
I0312 10:39:23.530925  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:23.530925  7012 solver.cpp:244]     Train net output #1: loss = 0.00312407 (* 1 = 0.00312407 loss)
I0312 10:39:23.530925  7012 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I0312 10:39:28.164507  7012 solver.cpp:228] Iteration 8300, loss = 0.00579048
I0312 10:39:28.164507  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:28.164507  7012 solver.cpp:244]     Train net output #1: loss = 0.00579039 (* 1 = 0.00579039 loss)
I0312 10:39:28.164507  7012 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I0312 10:39:32.770032  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_8400.caffemodel
I0312 10:39:32.781533  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_8400.solverstate
I0312 10:39:32.785033  7012 solver.cpp:337] Iteration 8400, Testing net (#0)
I0312 10:39:32.785033  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:39:34.943619  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9958
I0312 10:39:34.943619  7012 solver.cpp:404]     Test net output #1: loss = 0.0143612 (* 1 = 0.0143612 loss)
I0312 10:39:34.960621  7012 solver.cpp:228] Iteration 8400, loss = 0.00361615
I0312 10:39:34.961621  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:34.961621  7012 solver.cpp:244]     Train net output #1: loss = 0.00361605 (* 1 = 0.00361605 loss)
I0312 10:39:34.961621  7012 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
I0312 10:39:39.587146  7012 solver.cpp:228] Iteration 8500, loss = 0.00566676
I0312 10:39:39.587146  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:39.587146  7012 solver.cpp:244]     Train net output #1: loss = 0.00566666 (* 1 = 0.00566666 loss)
I0312 10:39:39.587146  7012 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I0312 10:39:44.212723  7012 solver.cpp:228] Iteration 8600, loss = 0.00845005
I0312 10:39:44.212723  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:44.212723  7012 solver.cpp:244]     Train net output #1: loss = 0.00844996 (* 1 = 0.00844996 loss)
I0312 10:39:44.212723  7012 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I0312 10:39:48.836995  7012 solver.cpp:228] Iteration 8700, loss = 0.00852218
I0312 10:39:48.836995  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:48.836995  7012 solver.cpp:244]     Train net output #1: loss = 0.00852208 (* 1 = 0.00852208 loss)
I0312 10:39:48.836995  7012 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I0312 10:39:53.478025  7012 solver.cpp:228] Iteration 8800, loss = 0.00303282
I0312 10:39:53.478025  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:53.478025  7012 solver.cpp:244]     Train net output #1: loss = 0.00303272 (* 1 = 0.00303272 loss)
I0312 10:39:53.478025  7012 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I0312 10:39:58.123129  7012 solver.cpp:228] Iteration 8900, loss = 0.00540399
I0312 10:39:58.123129  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:39:58.123129  7012 solver.cpp:244]     Train net output #1: loss = 0.00540389 (* 1 = 0.00540389 loss)
I0312 10:39:58.123129  7012 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I0312 10:40:02.786070  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_9000.caffemodel
I0312 10:40:02.796571  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_9000.solverstate
I0312 10:40:02.800571  7012 solver.cpp:337] Iteration 9000, Testing net (#0)
I0312 10:40:02.800571  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:40:04.970710  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9958
I0312 10:40:04.970710  7012 solver.cpp:404]     Test net output #1: loss = 0.0143554 (* 1 = 0.0143554 loss)
I0312 10:40:04.988718  7012 solver.cpp:228] Iteration 9000, loss = 0.00342735
I0312 10:40:04.988718  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:04.988718  7012 solver.cpp:244]     Train net output #1: loss = 0.00342725 (* 1 = 0.00342725 loss)
I0312 10:40:04.988718  7012 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I0312 10:40:09.639495  7012 solver.cpp:228] Iteration 9100, loss = 0.00545097
I0312 10:40:09.639495  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:09.639495  7012 solver.cpp:244]     Train net output #1: loss = 0.00545087 (* 1 = 0.00545087 loss)
I0312 10:40:09.639495  7012 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I0312 10:40:14.289917  7012 solver.cpp:228] Iteration 9200, loss = 0.00799627
I0312 10:40:14.289917  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:14.289917  7012 solver.cpp:244]     Train net output #1: loss = 0.00799617 (* 1 = 0.00799617 loss)
I0312 10:40:14.289917  7012 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I0312 10:40:18.958729  7012 solver.cpp:228] Iteration 9300, loss = 0.00786356
I0312 10:40:18.958729  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:18.958729  7012 solver.cpp:244]     Train net output #1: loss = 0.00786346 (* 1 = 0.00786346 loss)
I0312 10:40:18.958729  7012 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I0312 10:40:23.585317  7012 solver.cpp:228] Iteration 9400, loss = 0.00296837
I0312 10:40:23.585317  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:23.585317  7012 solver.cpp:244]     Train net output #1: loss = 0.00296827 (* 1 = 0.00296827 loss)
I0312 10:40:23.585317  7012 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
I0312 10:40:28.206984  7012 solver.cpp:228] Iteration 9500, loss = 0.00508436
I0312 10:40:28.206984  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:28.206984  7012 solver.cpp:244]     Train net output #1: loss = 0.00508427 (* 1 = 0.00508427 loss)
I0312 10:40:28.206984  7012 sgd_solver.cpp:106] Iteration 9500, lr = 0.01
I0312 10:40:32.805136  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_9600.caffemodel
I0312 10:40:32.819136  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_9600.solverstate
I0312 10:40:32.823137  7012 solver.cpp:337] Iteration 9600, Testing net (#0)
I0312 10:40:32.823137  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:40:34.976830  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:40:34.976830  7012 solver.cpp:404]     Test net output #1: loss = 0.0143322 (* 1 = 0.0143322 loss)
I0312 10:40:34.994330  7012 solver.cpp:228] Iteration 9600, loss = 0.00328911
I0312 10:40:34.994330  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:34.994330  7012 solver.cpp:244]     Train net output #1: loss = 0.00328902 (* 1 = 0.00328902 loss)
I0312 10:40:34.994330  7012 sgd_solver.cpp:106] Iteration 9600, lr = 0.01
I0312 10:40:39.615727  7012 solver.cpp:228] Iteration 9700, loss = 0.00538989
I0312 10:40:39.615727  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:39.616227  7012 solver.cpp:244]     Train net output #1: loss = 0.0053898 (* 1 = 0.0053898 loss)
I0312 10:40:39.616227  7012 sgd_solver.cpp:106] Iteration 9700, lr = 0.01
I0312 10:40:44.233963  7012 solver.cpp:228] Iteration 9800, loss = 0.00749326
I0312 10:40:44.233963  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:44.233963  7012 solver.cpp:244]     Train net output #1: loss = 0.00749316 (* 1 = 0.00749316 loss)
I0312 10:40:44.233963  7012 sgd_solver.cpp:106] Iteration 9800, lr = 0.01
I0312 10:40:48.845882  7012 solver.cpp:228] Iteration 9900, loss = 0.00724279
I0312 10:40:48.845882  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:48.845882  7012 solver.cpp:244]     Train net output #1: loss = 0.00724269 (* 1 = 0.00724269 loss)
I0312 10:40:48.845882  7012 sgd_solver.cpp:106] Iteration 9900, lr = 0.01
I0312 10:40:53.462388  7012 solver.cpp:228] Iteration 10000, loss = 0.00291378
I0312 10:40:53.462388  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:53.462388  7012 solver.cpp:244]     Train net output #1: loss = 0.00291368 (* 1 = 0.00291368 loss)
I0312 10:40:53.462388  7012 sgd_solver.cpp:106] Iteration 10000, lr = 0.01
I0312 10:40:58.321061  7012 solver.cpp:228] Iteration 10100, loss = 0.00476044
I0312 10:40:58.321061  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:40:58.321061  7012 solver.cpp:244]     Train net output #1: loss = 0.00476034 (* 1 = 0.00476034 loss)
I0312 10:40:58.321061  7012 sgd_solver.cpp:106] Iteration 10100, lr = 0.01
I0312 10:41:03.150364  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_10200.caffemodel
I0312 10:41:03.173369  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_10200.solverstate
I0312 10:41:03.179375  7012 solver.cpp:337] Iteration 10200, Testing net (#0)
I0312 10:41:03.179375  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:41:05.385741  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:41:05.385741  7012 solver.cpp:404]     Test net output #1: loss = 0.0143538 (* 1 = 0.0143538 loss)
I0312 10:41:05.405742  7012 solver.cpp:228] Iteration 10200, loss = 0.00317364
I0312 10:41:05.405742  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:05.405742  7012 solver.cpp:244]     Train net output #1: loss = 0.00317355 (* 1 = 0.00317355 loss)
I0312 10:41:05.405742  7012 sgd_solver.cpp:106] Iteration 10200, lr = 0.01
I0312 10:41:10.189857  7012 solver.cpp:228] Iteration 10300, loss = 0.00522836
I0312 10:41:10.189857  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:10.189857  7012 solver.cpp:244]     Train net output #1: loss = 0.00522827 (* 1 = 0.00522827 loss)
I0312 10:41:10.189857  7012 sgd_solver.cpp:106] Iteration 10300, lr = 0.01
I0312 10:41:15.006764  7012 solver.cpp:228] Iteration 10400, loss = 0.00698177
I0312 10:41:15.006764  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:15.006764  7012 solver.cpp:244]     Train net output #1: loss = 0.00698167 (* 1 = 0.00698167 loss)
I0312 10:41:15.006764  7012 sgd_solver.cpp:106] Iteration 10400, lr = 0.01
I0312 10:41:19.798159  7012 solver.cpp:228] Iteration 10500, loss = 0.00681876
I0312 10:41:19.798159  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:19.798159  7012 solver.cpp:244]     Train net output #1: loss = 0.00681866 (* 1 = 0.00681866 loss)
I0312 10:41:19.798159  7012 sgd_solver.cpp:106] Iteration 10500, lr = 0.01
I0312 10:41:24.593467  7012 solver.cpp:228] Iteration 10600, loss = 0.00287833
I0312 10:41:24.593467  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:24.593467  7012 solver.cpp:244]     Train net output #1: loss = 0.00287823 (* 1 = 0.00287823 loss)
I0312 10:41:24.593467  7012 sgd_solver.cpp:106] Iteration 10600, lr = 0.01
I0312 10:41:29.318826  7012 solver.cpp:228] Iteration 10700, loss = 0.00443752
I0312 10:41:29.318826  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:29.318826  7012 solver.cpp:244]     Train net output #1: loss = 0.00443742 (* 1 = 0.00443742 loss)
I0312 10:41:29.318826  7012 sgd_solver.cpp:106] Iteration 10700, lr = 0.01
I0312 10:41:34.026175  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_10800.caffemodel
I0312 10:41:34.036676  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_10800.solverstate
I0312 10:41:34.040175  7012 solver.cpp:337] Iteration 10800, Testing net (#0)
I0312 10:41:34.040175  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:41:36.227473  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:41:36.227473  7012 solver.cpp:404]     Test net output #1: loss = 0.0142207 (* 1 = 0.0142207 loss)
I0312 10:41:36.244989  7012 solver.cpp:228] Iteration 10800, loss = 0.00304561
I0312 10:41:36.244989  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:36.244989  7012 solver.cpp:244]     Train net output #1: loss = 0.00304552 (* 1 = 0.00304552 loss)
I0312 10:41:36.244989  7012 sgd_solver.cpp:106] Iteration 10800, lr = 0.01
I0312 10:41:40.960741  7012 solver.cpp:228] Iteration 10900, loss = 0.00506882
I0312 10:41:40.960741  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:40.960741  7012 solver.cpp:244]     Train net output #1: loss = 0.00506873 (* 1 = 0.00506873 loss)
I0312 10:41:40.960741  7012 sgd_solver.cpp:106] Iteration 10900, lr = 0.01
I0312 10:41:45.678619  7012 solver.cpp:228] Iteration 11000, loss = 0.00649581
I0312 10:41:45.678619  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:45.678619  7012 solver.cpp:244]     Train net output #1: loss = 0.00649571 (* 1 = 0.00649571 loss)
I0312 10:41:45.678619  7012 sgd_solver.cpp:106] Iteration 11000, lr = 0.01
I0312 10:41:50.386828  7012 solver.cpp:228] Iteration 11100, loss = 0.00632427
I0312 10:41:50.386828  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:50.386828  7012 solver.cpp:244]     Train net output #1: loss = 0.00632417 (* 1 = 0.00632417 loss)
I0312 10:41:50.386828  7012 sgd_solver.cpp:106] Iteration 11100, lr = 0.01
I0312 10:41:55.081750  7012 solver.cpp:228] Iteration 11200, loss = 0.00286875
I0312 10:41:55.081750  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:55.081750  7012 solver.cpp:244]     Train net output #1: loss = 0.00286866 (* 1 = 0.00286866 loss)
I0312 10:41:55.081750  7012 sgd_solver.cpp:106] Iteration 11200, lr = 0.01
I0312 10:41:59.773733  7012 solver.cpp:228] Iteration 11300, loss = 0.00421596
I0312 10:41:59.774233  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:41:59.774233  7012 solver.cpp:244]     Train net output #1: loss = 0.00421587 (* 1 = 0.00421587 loss)
I0312 10:41:59.774233  7012 sgd_solver.cpp:106] Iteration 11300, lr = 0.01
I0312 10:42:04.441103  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_11400.caffemodel
I0312 10:42:04.453586  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_11400.solverstate
I0312 10:42:04.457584  7012 solver.cpp:337] Iteration 11400, Testing net (#0)
I0312 10:42:04.457584  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:42:06.640354  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9959
I0312 10:42:06.640354  7012 solver.cpp:404]     Test net output #1: loss = 0.0143072 (* 1 = 0.0143072 loss)
I0312 10:42:06.658362  7012 solver.cpp:228] Iteration 11400, loss = 0.0029569
I0312 10:42:06.658362  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:06.658362  7012 solver.cpp:244]     Train net output #1: loss = 0.0029568 (* 1 = 0.0029568 loss)
I0312 10:42:06.658362  7012 sgd_solver.cpp:106] Iteration 11400, lr = 0.01
I0312 10:42:11.341089  7012 solver.cpp:228] Iteration 11500, loss = 0.00505706
I0312 10:42:11.341089  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:11.341089  7012 solver.cpp:244]     Train net output #1: loss = 0.00505697 (* 1 = 0.00505697 loss)
I0312 10:42:11.341089  7012 sgd_solver.cpp:106] Iteration 11500, lr = 0.01
I0312 10:42:16.022873  7012 solver.cpp:228] Iteration 11600, loss = 0.00622664
I0312 10:42:16.022873  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:16.022873  7012 solver.cpp:244]     Train net output #1: loss = 0.00622655 (* 1 = 0.00622655 loss)
I0312 10:42:16.022873  7012 sgd_solver.cpp:106] Iteration 11600, lr = 0.01
I0312 10:42:20.707787  7012 solver.cpp:228] Iteration 11700, loss = 0.00619433
I0312 10:42:20.708287  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:20.708287  7012 solver.cpp:244]     Train net output #1: loss = 0.00619424 (* 1 = 0.00619424 loss)
I0312 10:42:20.708287  7012 sgd_solver.cpp:106] Iteration 11700, lr = 0.01
I0312 10:42:25.386335  7012 solver.cpp:228] Iteration 11800, loss = 0.00287735
I0312 10:42:25.386335  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:25.386335  7012 solver.cpp:244]     Train net output #1: loss = 0.00287726 (* 1 = 0.00287726 loss)
I0312 10:42:25.386335  7012 sgd_solver.cpp:106] Iteration 11800, lr = 0.01
I0312 10:42:30.072592  7012 solver.cpp:228] Iteration 11900, loss = 0.00399856
I0312 10:42:30.073091  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:30.073091  7012 solver.cpp:244]     Train net output #1: loss = 0.00399847 (* 1 = 0.00399847 loss)
I0312 10:42:30.073091  7012 sgd_solver.cpp:106] Iteration 11900, lr = 0.01
I0312 10:42:34.735469  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_12000.caffemodel
I0312 10:42:34.749469  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_12000.solverstate
I0312 10:42:34.753469  7012 solver.cpp:337] Iteration 12000, Testing net (#0)
I0312 10:42:34.753969  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:42:36.931273  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9958
I0312 10:42:36.931273  7012 solver.cpp:404]     Test net output #1: loss = 0.0141929 (* 1 = 0.0141929 loss)
I0312 10:42:36.948272  7012 solver.cpp:228] Iteration 12000, loss = 0.00286251
I0312 10:42:36.948272  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:36.948272  7012 solver.cpp:244]     Train net output #1: loss = 0.00286241 (* 1 = 0.00286241 loss)
I0312 10:42:36.948272  7012 sgd_solver.cpp:106] Iteration 12000, lr = 0.01
I0312 10:42:41.627274  7012 solver.cpp:228] Iteration 12100, loss = 0.00503025
I0312 10:42:41.627274  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:41.627274  7012 solver.cpp:244]     Train net output #1: loss = 0.00503015 (* 1 = 0.00503015 loss)
I0312 10:42:41.627274  7012 sgd_solver.cpp:106] Iteration 12100, lr = 0.01
I0312 10:42:46.308194  7012 solver.cpp:228] Iteration 12200, loss = 0.00580498
I0312 10:42:46.308194  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:46.308194  7012 solver.cpp:244]     Train net output #1: loss = 0.00580488 (* 1 = 0.00580488 loss)
I0312 10:42:46.308194  7012 sgd_solver.cpp:106] Iteration 12200, lr = 0.01
I0312 10:42:50.989619  7012 solver.cpp:228] Iteration 12300, loss = 0.00575981
I0312 10:42:50.989619  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:50.989619  7012 solver.cpp:244]     Train net output #1: loss = 0.00575972 (* 1 = 0.00575972 loss)
I0312 10:42:50.989619  7012 sgd_solver.cpp:106] Iteration 12300, lr = 0.01
I0312 10:42:55.676447  7012 solver.cpp:228] Iteration 12400, loss = 0.00288708
I0312 10:42:55.676447  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:42:55.676447  7012 solver.cpp:244]     Train net output #1: loss = 0.00288699 (* 1 = 0.00288699 loss)
I0312 10:42:55.676447  7012 sgd_solver.cpp:106] Iteration 12400, lr = 0.01
I0312 10:43:00.344105  7012 solver.cpp:228] Iteration 12500, loss = 0.00384845
I0312 10:43:00.344105  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:00.344105  7012 solver.cpp:244]     Train net output #1: loss = 0.00384836 (* 1 = 0.00384836 loss)
I0312 10:43:00.344105  7012 sgd_solver.cpp:46] MultiStep Status: Iteration 12500, step = 2
I0312 10:43:00.344105  7012 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I0312 10:43:04.936514  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_12600.caffemodel
I0312 10:43:04.947515  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_12600.solverstate
I0312 10:43:04.951514  7012 solver.cpp:337] Iteration 12600, Testing net (#0)
I0312 10:43:04.951514  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:43:07.106670  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:43:07.106670  7012 solver.cpp:404]     Test net output #1: loss = 0.0136749 (* 1 = 0.0136749 loss)
I0312 10:43:07.124228  7012 solver.cpp:228] Iteration 12600, loss = 0.00286724
I0312 10:43:07.124228  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:07.124228  7012 solver.cpp:244]     Train net output #1: loss = 0.00286715 (* 1 = 0.00286715 loss)
I0312 10:43:07.124228  7012 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I0312 10:43:11.761200  7012 solver.cpp:228] Iteration 12700, loss = 0.00665431
I0312 10:43:11.761200  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:11.761200  7012 solver.cpp:244]     Train net output #1: loss = 0.00665422 (* 1 = 0.00665422 loss)
I0312 10:43:11.761200  7012 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I0312 10:43:16.366333  7012 solver.cpp:228] Iteration 12800, loss = 0.0058377
I0312 10:43:16.366333  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:16.366333  7012 solver.cpp:244]     Train net output #1: loss = 0.0058376 (* 1 = 0.0058376 loss)
I0312 10:43:16.366333  7012 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I0312 10:43:20.987586  7012 solver.cpp:228] Iteration 12900, loss = 0.00453879
I0312 10:43:20.987586  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:20.987586  7012 solver.cpp:244]     Train net output #1: loss = 0.0045387 (* 1 = 0.0045387 loss)
I0312 10:43:20.987586  7012 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I0312 10:43:25.594159  7012 solver.cpp:228] Iteration 13000, loss = 0.0026289
I0312 10:43:25.594159  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:25.594159  7012 solver.cpp:244]     Train net output #1: loss = 0.00262881 (* 1 = 0.00262881 loss)
I0312 10:43:25.594159  7012 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I0312 10:43:30.198170  7012 solver.cpp:228] Iteration 13100, loss = 0.00369319
I0312 10:43:30.198170  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:30.198170  7012 solver.cpp:244]     Train net output #1: loss = 0.0036931 (* 1 = 0.0036931 loss)
I0312 10:43:30.198170  7012 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I0312 10:43:34.786948  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_13200.caffemodel
I0312 10:43:34.797935  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_13200.solverstate
I0312 10:43:34.801934  7012 solver.cpp:337] Iteration 13200, Testing net (#0)
I0312 10:43:34.801934  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:43:36.957597  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:43:36.958096  7012 solver.cpp:404]     Test net output #1: loss = 0.0136689 (* 1 = 0.0136689 loss)
I0312 10:43:36.975595  7012 solver.cpp:228] Iteration 13200, loss = 0.00279219
I0312 10:43:36.975595  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:36.975595  7012 solver.cpp:244]     Train net output #1: loss = 0.00279209 (* 1 = 0.00279209 loss)
I0312 10:43:36.975595  7012 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I0312 10:43:41.585902  7012 solver.cpp:228] Iteration 13300, loss = 0.00569189
I0312 10:43:41.585902  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:41.585902  7012 solver.cpp:244]     Train net output #1: loss = 0.0056918 (* 1 = 0.0056918 loss)
I0312 10:43:41.585902  7012 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I0312 10:43:46.198426  7012 solver.cpp:228] Iteration 13400, loss = 0.00546972
I0312 10:43:46.198426  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:46.198426  7012 solver.cpp:244]     Train net output #1: loss = 0.00546962 (* 1 = 0.00546962 loss)
I0312 10:43:46.198426  7012 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I0312 10:43:50.806918  7012 solver.cpp:228] Iteration 13500, loss = 0.00457342
I0312 10:43:50.806918  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:50.806918  7012 solver.cpp:244]     Train net output #1: loss = 0.00457333 (* 1 = 0.00457333 loss)
I0312 10:43:50.806918  7012 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I0312 10:43:55.417712  7012 solver.cpp:228] Iteration 13600, loss = 0.00264765
I0312 10:43:55.417712  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:43:55.417712  7012 solver.cpp:244]     Train net output #1: loss = 0.00264756 (* 1 = 0.00264756 loss)
I0312 10:43:55.417712  7012 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I0312 10:44:00.028025  7012 solver.cpp:228] Iteration 13700, loss = 0.00367545
I0312 10:44:00.028025  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:00.028025  7012 solver.cpp:244]     Train net output #1: loss = 0.00367536 (* 1 = 0.00367536 loss)
I0312 10:44:00.028025  7012 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I0312 10:44:04.612853  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_13800.caffemodel
I0312 10:44:04.628476  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_13800.solverstate
I0312 10:44:04.644104  7012 solver.cpp:337] Iteration 13800, Testing net (#0)
I0312 10:44:04.644104  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:44:06.797289  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:44:06.797780  7012 solver.cpp:404]     Test net output #1: loss = 0.0136841 (* 1 = 0.0136841 loss)
I0312 10:44:06.799789  7012 solver.cpp:228] Iteration 13800, loss = 0.00280092
I0312 10:44:06.799789  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:06.799789  7012 solver.cpp:244]     Train net output #1: loss = 0.00280083 (* 1 = 0.00280083 loss)
I0312 10:44:06.799789  7012 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I0312 10:44:11.430881  7012 solver.cpp:228] Iteration 13900, loss = 0.00547023
I0312 10:44:11.430881  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:11.430881  7012 solver.cpp:244]     Train net output #1: loss = 0.00547014 (* 1 = 0.00547014 loss)
I0312 10:44:11.430881  7012 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I0312 10:44:16.037777  7012 solver.cpp:228] Iteration 14000, loss = 0.00536255
I0312 10:44:16.037777  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:16.037777  7012 solver.cpp:244]     Train net output #1: loss = 0.00536245 (* 1 = 0.00536245 loss)
I0312 10:44:16.037777  7012 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I0312 10:44:20.644429  7012 solver.cpp:228] Iteration 14100, loss = 0.0045859
I0312 10:44:20.644429  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:20.644429  7012 solver.cpp:244]     Train net output #1: loss = 0.0045858 (* 1 = 0.0045858 loss)
I0312 10:44:20.644429  7012 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I0312 10:44:25.251816  7012 solver.cpp:228] Iteration 14200, loss = 0.00265333
I0312 10:44:25.251816  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:25.251816  7012 solver.cpp:244]     Train net output #1: loss = 0.00265324 (* 1 = 0.00265324 loss)
I0312 10:44:25.251816  7012 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I0312 10:44:29.867379  7012 solver.cpp:228] Iteration 14300, loss = 0.00365009
I0312 10:44:29.867379  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:29.867379  7012 solver.cpp:244]     Train net output #1: loss = 0.00365 (* 1 = 0.00365 loss)
I0312 10:44:29.867379  7012 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I0312 10:44:34.461534  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_14400.caffemodel
I0312 10:44:34.473034  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_14400.solverstate
I0312 10:44:34.476536  7012 solver.cpp:337] Iteration 14400, Testing net (#0)
I0312 10:44:34.476536  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:44:36.631362  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:44:36.631362  7012 solver.cpp:404]     Test net output #1: loss = 0.0136925 (* 1 = 0.0136925 loss)
I0312 10:44:36.648862  7012 solver.cpp:228] Iteration 14400, loss = 0.00280587
I0312 10:44:36.648862  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:36.648862  7012 solver.cpp:244]     Train net output #1: loss = 0.00280578 (* 1 = 0.00280578 loss)
I0312 10:44:36.648862  7012 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I0312 10:44:41.250249  7012 solver.cpp:228] Iteration 14500, loss = 0.00536724
I0312 10:44:41.250249  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:41.250249  7012 solver.cpp:244]     Train net output #1: loss = 0.00536715 (* 1 = 0.00536715 loss)
I0312 10:44:41.250249  7012 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I0312 10:44:45.868530  7012 solver.cpp:228] Iteration 14600, loss = 0.00530401
I0312 10:44:45.868530  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:45.868530  7012 solver.cpp:244]     Train net output #1: loss = 0.00530392 (* 1 = 0.00530392 loss)
I0312 10:44:45.868530  7012 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I0312 10:44:50.478099  7012 solver.cpp:228] Iteration 14700, loss = 0.00459754
I0312 10:44:50.478099  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:50.478099  7012 solver.cpp:244]     Train net output #1: loss = 0.00459745 (* 1 = 0.00459745 loss)
I0312 10:44:50.478099  7012 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I0312 10:44:55.098942  7012 solver.cpp:228] Iteration 14800, loss = 0.00265743
I0312 10:44:55.098942  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:55.098942  7012 solver.cpp:244]     Train net output #1: loss = 0.00265734 (* 1 = 0.00265734 loss)
I0312 10:44:55.098942  7012 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I0312 10:44:59.704013  7012 solver.cpp:228] Iteration 14900, loss = 0.00362674
I0312 10:44:59.704013  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:44:59.704013  7012 solver.cpp:244]     Train net output #1: loss = 0.00362665 (* 1 = 0.00362665 loss)
I0312 10:44:59.704013  7012 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I0312 10:45:04.298070  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_15000.caffemodel
I0312 10:45:04.309070  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_15000.solverstate
I0312 10:45:04.312572  7012 solver.cpp:337] Iteration 15000, Testing net (#0)
I0312 10:45:04.313088  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:45:06.467753  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9962
I0312 10:45:06.467753  7012 solver.cpp:404]     Test net output #1: loss = 0.0136979 (* 1 = 0.0136979 loss)
I0312 10:45:06.483366  7012 solver.cpp:228] Iteration 15000, loss = 0.00280469
I0312 10:45:06.483366  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:06.483366  7012 solver.cpp:244]     Train net output #1: loss = 0.0028046 (* 1 = 0.0028046 loss)
I0312 10:45:06.483366  7012 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I0312 10:45:11.113212  7012 solver.cpp:228] Iteration 15100, loss = 0.00530196
I0312 10:45:11.113212  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:11.113212  7012 solver.cpp:244]     Train net output #1: loss = 0.00530187 (* 1 = 0.00530187 loss)
I0312 10:45:11.113212  7012 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I0312 10:45:15.732923  7012 solver.cpp:228] Iteration 15200, loss = 0.00525611
I0312 10:45:15.733428  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:15.733428  7012 solver.cpp:244]     Train net output #1: loss = 0.00525602 (* 1 = 0.00525602 loss)
I0312 10:45:15.733428  7012 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I0312 10:45:20.369855  7012 solver.cpp:228] Iteration 15300, loss = 0.00460982
I0312 10:45:20.369855  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:20.369855  7012 solver.cpp:244]     Train net output #1: loss = 0.00460973 (* 1 = 0.00460973 loss)
I0312 10:45:20.369855  7012 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I0312 10:45:25.014987  7012 solver.cpp:228] Iteration 15400, loss = 0.00266122
I0312 10:45:25.014987  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:25.014987  7012 solver.cpp:244]     Train net output #1: loss = 0.00266113 (* 1 = 0.00266113 loss)
I0312 10:45:25.014987  7012 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I0312 10:45:29.632928  7012 solver.cpp:228] Iteration 15500, loss = 0.00360707
I0312 10:45:29.632928  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:29.632928  7012 solver.cpp:244]     Train net output #1: loss = 0.00360698 (* 1 = 0.00360698 loss)
I0312 10:45:29.632928  7012 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I0312 10:45:34.245580  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_15600.caffemodel
I0312 10:45:34.245580  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_15600.solverstate
I0312 10:45:34.261209  7012 solver.cpp:337] Iteration 15600, Testing net (#0)
I0312 10:45:34.261209  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:45:36.412607  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:45:36.412607  7012 solver.cpp:404]     Test net output #1: loss = 0.013705 (* 1 = 0.013705 loss)
I0312 10:45:36.434736  7012 solver.cpp:228] Iteration 15600, loss = 0.0028032
I0312 10:45:36.434736  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:36.434736  7012 solver.cpp:244]     Train net output #1: loss = 0.00280311 (* 1 = 0.00280311 loss)
I0312 10:45:36.434736  7012 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I0312 10:45:41.092191  7012 solver.cpp:228] Iteration 15700, loss = 0.00524946
I0312 10:45:41.092191  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:41.092191  7012 solver.cpp:244]     Train net output #1: loss = 0.00524937 (* 1 = 0.00524937 loss)
I0312 10:45:41.092191  7012 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I0312 10:45:45.766109  7012 solver.cpp:228] Iteration 15800, loss = 0.00520936
I0312 10:45:45.766109  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:45.766109  7012 solver.cpp:244]     Train net output #1: loss = 0.00520927 (* 1 = 0.00520927 loss)
I0312 10:45:45.766109  7012 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I0312 10:45:50.450453  7012 solver.cpp:228] Iteration 15900, loss = 0.0046181
I0312 10:45:50.450953  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:50.450953  7012 solver.cpp:244]     Train net output #1: loss = 0.004618 (* 1 = 0.004618 loss)
I0312 10:45:50.450953  7012 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I0312 10:45:55.099566  7012 solver.cpp:228] Iteration 16000, loss = 0.00266601
I0312 10:45:55.099566  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:55.099566  7012 solver.cpp:244]     Train net output #1: loss = 0.00266592 (* 1 = 0.00266592 loss)
I0312 10:45:55.099566  7012 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I0312 10:45:59.731403  7012 solver.cpp:228] Iteration 16100, loss = 0.00358774
I0312 10:45:59.731403  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:45:59.731403  7012 solver.cpp:244]     Train net output #1: loss = 0.00358765 (* 1 = 0.00358765 loss)
I0312 10:45:59.731403  7012 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I0312 10:46:04.327249  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_16200.caffemodel
I0312 10:46:04.327249  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_16200.solverstate
I0312 10:46:04.342876  7012 solver.cpp:337] Iteration 16200, Testing net (#0)
I0312 10:46:04.342876  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:46:06.500813  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:46:06.500813  7012 solver.cpp:404]     Test net output #1: loss = 0.0136933 (* 1 = 0.0136933 loss)
I0312 10:46:06.505810  7012 solver.cpp:228] Iteration 16200, loss = 0.00279734
I0312 10:46:06.505810  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:06.505810  7012 solver.cpp:244]     Train net output #1: loss = 0.00279725 (* 1 = 0.00279725 loss)
I0312 10:46:06.505810  7012 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I0312 10:46:11.183043  7012 solver.cpp:228] Iteration 16300, loss = 0.00519997
I0312 10:46:11.183043  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:11.183043  7012 solver.cpp:244]     Train net output #1: loss = 0.00519988 (* 1 = 0.00519988 loss)
I0312 10:46:11.183043  7012 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I0312 10:46:15.841080  7012 solver.cpp:228] Iteration 16400, loss = 0.00516553
I0312 10:46:15.841080  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:15.841080  7012 solver.cpp:244]     Train net output #1: loss = 0.00516544 (* 1 = 0.00516544 loss)
I0312 10:46:15.841080  7012 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I0312 10:46:20.484838  7012 solver.cpp:228] Iteration 16500, loss = 0.00463703
I0312 10:46:20.484838  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:20.484838  7012 solver.cpp:244]     Train net output #1: loss = 0.00463694 (* 1 = 0.00463694 loss)
I0312 10:46:20.484838  7012 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I0312 10:46:25.119824  7012 solver.cpp:228] Iteration 16600, loss = 0.00267
I0312 10:46:25.119824  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:25.119824  7012 solver.cpp:244]     Train net output #1: loss = 0.00266991 (* 1 = 0.00266991 loss)
I0312 10:46:25.119824  7012 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I0312 10:46:29.745944  7012 solver.cpp:228] Iteration 16700, loss = 0.00357336
I0312 10:46:29.745944  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:29.745944  7012 solver.cpp:244]     Train net output #1: loss = 0.00357328 (* 1 = 0.00357328 loss)
I0312 10:46:29.746443  7012 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I0312 10:46:34.350596  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_16800.caffemodel
I0312 10:46:34.362097  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_16800.solverstate
I0312 10:46:34.365597  7012 solver.cpp:337] Iteration 16800, Testing net (#0)
I0312 10:46:34.365597  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:46:36.519709  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:46:36.519709  7012 solver.cpp:404]     Test net output #1: loss = 0.0137049 (* 1 = 0.0137049 loss)
I0312 10:46:36.535331  7012 solver.cpp:228] Iteration 16800, loss = 0.00279203
I0312 10:46:36.535331  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:36.535331  7012 solver.cpp:244]     Train net output #1: loss = 0.00279194 (* 1 = 0.00279194 loss)
I0312 10:46:36.535331  7012 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I0312 10:46:41.147135  7012 solver.cpp:228] Iteration 16900, loss = 0.00515919
I0312 10:46:41.147135  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:41.147135  7012 solver.cpp:244]     Train net output #1: loss = 0.0051591 (* 1 = 0.0051591 loss)
I0312 10:46:41.147135  7012 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I0312 10:46:45.759063  7012 solver.cpp:228] Iteration 17000, loss = 0.00513031
I0312 10:46:45.759563  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:45.759563  7012 solver.cpp:244]     Train net output #1: loss = 0.00513022 (* 1 = 0.00513022 loss)
I0312 10:46:45.759563  7012 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I0312 10:46:50.370415  7012 solver.cpp:228] Iteration 17100, loss = 0.00463825
I0312 10:46:50.370915  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:50.370915  7012 solver.cpp:244]     Train net output #1: loss = 0.00463817 (* 1 = 0.00463817 loss)
I0312 10:46:50.370915  7012 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I0312 10:46:54.978315  7012 solver.cpp:228] Iteration 17200, loss = 0.00267489
I0312 10:46:54.978315  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:54.978315  7012 solver.cpp:244]     Train net output #1: loss = 0.00267481 (* 1 = 0.00267481 loss)
I0312 10:46:54.978315  7012 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I0312 10:46:59.590190  7012 solver.cpp:228] Iteration 17300, loss = 0.00356016
I0312 10:46:59.590190  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:46:59.590190  7012 solver.cpp:244]     Train net output #1: loss = 0.00356007 (* 1 = 0.00356007 loss)
I0312 10:46:59.590190  7012 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I0312 10:47:04.185910  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_17400.caffemodel
I0312 10:47:04.201556  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_17400.solverstate
I0312 10:47:04.201556  7012 solver.cpp:337] Iteration 17400, Testing net (#0)
I0312 10:47:04.201556  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:47:06.357628  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:47:06.357628  7012 solver.cpp:404]     Test net output #1: loss = 0.0136974 (* 1 = 0.0136974 loss)
I0312 10:47:06.383755  7012 solver.cpp:228] Iteration 17400, loss = 0.00279215
I0312 10:47:06.383755  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:06.383755  7012 solver.cpp:244]     Train net output #1: loss = 0.00279206 (* 1 = 0.00279206 loss)
I0312 10:47:06.383755  7012 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I0312 10:47:11.003901  7012 solver.cpp:228] Iteration 17500, loss = 0.00512752
I0312 10:47:11.003901  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:11.003901  7012 solver.cpp:244]     Train net output #1: loss = 0.00512743 (* 1 = 0.00512743 loss)
I0312 10:47:11.003901  7012 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I0312 10:47:15.610852  7012 solver.cpp:228] Iteration 17600, loss = 0.00509291
I0312 10:47:15.610852  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:15.610852  7012 solver.cpp:244]     Train net output #1: loss = 0.00509282 (* 1 = 0.00509282 loss)
I0312 10:47:15.610852  7012 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I0312 10:47:20.244429  7012 solver.cpp:228] Iteration 17700, loss = 0.00465269
I0312 10:47:20.244429  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:20.244429  7012 solver.cpp:244]     Train net output #1: loss = 0.0046526 (* 1 = 0.0046526 loss)
I0312 10:47:20.244429  7012 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I0312 10:47:24.860662  7012 solver.cpp:228] Iteration 17800, loss = 0.00267916
I0312 10:47:24.860662  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:24.860662  7012 solver.cpp:244]     Train net output #1: loss = 0.00267908 (* 1 = 0.00267908 loss)
I0312 10:47:24.860662  7012 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I0312 10:47:29.488836  7012 solver.cpp:228] Iteration 17900, loss = 0.00354726
I0312 10:47:29.488836  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:29.488836  7012 solver.cpp:244]     Train net output #1: loss = 0.00354717 (* 1 = 0.00354717 loss)
I0312 10:47:29.488836  7012 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I0312 10:47:34.088240  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_18000.caffemodel
I0312 10:47:34.099241  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_18000.solverstate
I0312 10:47:34.103241  7012 solver.cpp:337] Iteration 18000, Testing net (#0)
I0312 10:47:34.103241  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:47:36.252702  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:47:36.252702  7012 solver.cpp:404]     Test net output #1: loss = 0.0137099 (* 1 = 0.0137099 loss)
I0312 10:47:36.268328  7012 solver.cpp:228] Iteration 18000, loss = 0.00278834
I0312 10:47:36.268328  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:36.268328  7012 solver.cpp:244]     Train net output #1: loss = 0.00278825 (* 1 = 0.00278825 loss)
I0312 10:47:36.268328  7012 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I0312 10:47:40.876701  7012 solver.cpp:228] Iteration 18100, loss = 0.00509053
I0312 10:47:40.876701  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:40.876701  7012 solver.cpp:244]     Train net output #1: loss = 0.00509044 (* 1 = 0.00509044 loss)
I0312 10:47:40.876701  7012 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I0312 10:47:45.488800  7012 solver.cpp:228] Iteration 18200, loss = 0.00506114
I0312 10:47:45.488800  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:45.488800  7012 solver.cpp:244]     Train net output #1: loss = 0.00506106 (* 1 = 0.00506106 loss)
I0312 10:47:45.488800  7012 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I0312 10:47:50.102763  7012 solver.cpp:228] Iteration 18300, loss = 0.00465428
I0312 10:47:50.102763  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:50.102763  7012 solver.cpp:244]     Train net output #1: loss = 0.00465419 (* 1 = 0.00465419 loss)
I0312 10:47:50.102763  7012 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I0312 10:47:54.721705  7012 solver.cpp:228] Iteration 18400, loss = 0.00268475
I0312 10:47:54.721705  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:54.721705  7012 solver.cpp:244]     Train net output #1: loss = 0.00268467 (* 1 = 0.00268467 loss)
I0312 10:47:54.721705  7012 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I0312 10:47:59.407980  7012 solver.cpp:228] Iteration 18500, loss = 0.00353663
I0312 10:47:59.407980  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:47:59.407980  7012 solver.cpp:244]     Train net output #1: loss = 0.00353654 (* 1 = 0.00353654 loss)
I0312 10:47:59.407980  7012 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I0312 10:48:04.048960  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_18600.caffemodel
I0312 10:48:04.068591  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_18600.solverstate
I0312 10:48:04.071091  7012 solver.cpp:337] Iteration 18600, Testing net (#0)
I0312 10:48:04.071091  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:48:06.241674  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:48:06.241674  7012 solver.cpp:404]     Test net output #1: loss = 0.0137064 (* 1 = 0.0137064 loss)
I0312 10:48:06.257302  7012 solver.cpp:228] Iteration 18600, loss = 0.00278346
I0312 10:48:06.257302  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:06.257302  7012 solver.cpp:244]     Train net output #1: loss = 0.00278338 (* 1 = 0.00278338 loss)
I0312 10:48:06.257302  7012 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I0312 10:48:10.893784  7012 solver.cpp:228] Iteration 18700, loss = 0.00506838
I0312 10:48:10.893784  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:10.893784  7012 solver.cpp:244]     Train net output #1: loss = 0.0050683 (* 1 = 0.0050683 loss)
I0312 10:48:10.893784  7012 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I0312 10:48:15.518616  7012 solver.cpp:228] Iteration 18800, loss = 0.00502799
I0312 10:48:15.518616  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:15.518616  7012 solver.cpp:244]     Train net output #1: loss = 0.00502791 (* 1 = 0.00502791 loss)
I0312 10:48:15.518616  7012 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I0312 10:48:20.141978  7012 solver.cpp:228] Iteration 18900, loss = 0.00466744
I0312 10:48:20.141978  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:20.141978  7012 solver.cpp:244]     Train net output #1: loss = 0.00466735 (* 1 = 0.00466735 loss)
I0312 10:48:20.141978  7012 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I0312 10:48:24.755336  7012 solver.cpp:228] Iteration 19000, loss = 0.00268855
I0312 10:48:24.755336  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:24.755336  7012 solver.cpp:244]     Train net output #1: loss = 0.00268846 (* 1 = 0.00268846 loss)
I0312 10:48:24.755336  7012 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I0312 10:48:29.379557  7012 solver.cpp:228] Iteration 19100, loss = 0.00352574
I0312 10:48:29.379557  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:29.379557  7012 solver.cpp:244]     Train net output #1: loss = 0.00352565 (* 1 = 0.00352565 loss)
I0312 10:48:29.379557  7012 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I0312 10:48:33.973912  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_19200.caffemodel
I0312 10:48:33.979979  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_19200.solverstate
I0312 10:48:33.979979  7012 solver.cpp:337] Iteration 19200, Testing net (#0)
I0312 10:48:33.979979  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:48:36.144794  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:48:36.144794  7012 solver.cpp:404]     Test net output #1: loss = 0.0137091 (* 1 = 0.0137091 loss)
I0312 10:48:36.148205  7012 solver.cpp:228] Iteration 19200, loss = 0.00278271
I0312 10:48:36.148205  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:36.148205  7012 solver.cpp:244]     Train net output #1: loss = 0.00278263 (* 1 = 0.00278263 loss)
I0312 10:48:36.148205  7012 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I0312 10:48:40.781090  7012 solver.cpp:228] Iteration 19300, loss = 0.00503478
I0312 10:48:40.781090  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:40.781090  7012 solver.cpp:244]     Train net output #1: loss = 0.0050347 (* 1 = 0.0050347 loss)
I0312 10:48:40.781090  7012 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I0312 10:48:45.411236  7012 solver.cpp:228] Iteration 19400, loss = 0.00500256
I0312 10:48:45.411236  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:45.411236  7012 solver.cpp:244]     Train net output #1: loss = 0.00500248 (* 1 = 0.00500248 loss)
I0312 10:48:45.411236  7012 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I0312 10:48:50.040359  7012 solver.cpp:228] Iteration 19500, loss = 0.00466724
I0312 10:48:50.040359  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:50.040359  7012 solver.cpp:244]     Train net output #1: loss = 0.00466716 (* 1 = 0.00466716 loss)
I0312 10:48:50.040359  7012 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I0312 10:48:54.661012  7012 solver.cpp:228] Iteration 19600, loss = 0.00269507
I0312 10:48:54.661512  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:54.661512  7012 solver.cpp:244]     Train net output #1: loss = 0.00269499 (* 1 = 0.00269499 loss)
I0312 10:48:54.661512  7012 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I0312 10:48:59.285454  7012 solver.cpp:228] Iteration 19700, loss = 0.00351454
I0312 10:48:59.285454  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:48:59.285454  7012 solver.cpp:244]     Train net output #1: loss = 0.00351445 (* 1 = 0.00351445 loss)
I0312 10:48:59.285454  7012 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I0312 10:49:03.880946  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_19800.caffemodel
I0312 10:49:03.896574  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_19800.solverstate
I0312 10:49:03.896574  7012 solver.cpp:337] Iteration 19800, Testing net (#0)
I0312 10:49:03.896574  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:49:06.051977  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:49:06.051977  7012 solver.cpp:404]     Test net output #1: loss = 0.0137068 (* 1 = 0.0137068 loss)
I0312 10:49:06.070103  7012 solver.cpp:228] Iteration 19800, loss = 0.00277819
I0312 10:49:06.070103  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:06.070103  7012 solver.cpp:244]     Train net output #1: loss = 0.00277811 (* 1 = 0.00277811 loss)
I0312 10:49:06.070103  7012 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I0312 10:49:10.709409  7012 solver.cpp:228] Iteration 19900, loss = 0.00501139
I0312 10:49:10.709409  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:10.709409  7012 solver.cpp:244]     Train net output #1: loss = 0.00501131 (* 1 = 0.00501131 loss)
I0312 10:49:10.709409  7012 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I0312 10:49:15.329910  7012 solver.cpp:228] Iteration 20000, loss = 0.0049749
I0312 10:49:15.330410  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:15.330410  7012 solver.cpp:244]     Train net output #1: loss = 0.00497482 (* 1 = 0.00497482 loss)
I0312 10:49:15.330410  7012 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I0312 10:49:19.938010  7012 solver.cpp:228] Iteration 20100, loss = 0.00467723
I0312 10:49:19.938010  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:19.938010  7012 solver.cpp:244]     Train net output #1: loss = 0.00467715 (* 1 = 0.00467715 loss)
I0312 10:49:19.938010  7012 sgd_solver.cpp:106] Iteration 20100, lr = 0.001
I0312 10:49:24.564142  7012 solver.cpp:228] Iteration 20200, loss = 0.00269803
I0312 10:49:24.564142  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:24.564142  7012 solver.cpp:244]     Train net output #1: loss = 0.00269795 (* 1 = 0.00269795 loss)
I0312 10:49:24.564142  7012 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I0312 10:49:29.199630  7012 solver.cpp:228] Iteration 20300, loss = 0.00350253
I0312 10:49:29.199630  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:29.199630  7012 solver.cpp:244]     Train net output #1: loss = 0.00350245 (* 1 = 0.00350245 loss)
I0312 10:49:29.199630  7012 sgd_solver.cpp:106] Iteration 20300, lr = 0.001
I0312 10:49:33.795128  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_20400.caffemodel
I0312 10:49:33.809132  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_20400.solverstate
I0312 10:49:33.813133  7012 solver.cpp:337] Iteration 20400, Testing net (#0)
I0312 10:49:33.813133  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:49:35.961999  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:49:35.961999  7012 solver.cpp:404]     Test net output #1: loss = 0.0137085 (* 1 = 0.0137085 loss)
I0312 10:49:35.977623  7012 solver.cpp:228] Iteration 20400, loss = 0.002777
I0312 10:49:35.977623  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:35.977623  7012 solver.cpp:244]     Train net output #1: loss = 0.00277692 (* 1 = 0.00277692 loss)
I0312 10:49:35.977623  7012 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I0312 10:49:40.617499  7012 solver.cpp:228] Iteration 20500, loss = 0.00498715
I0312 10:49:40.617499  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:40.617499  7012 solver.cpp:244]     Train net output #1: loss = 0.00498707 (* 1 = 0.00498707 loss)
I0312 10:49:40.617499  7012 sgd_solver.cpp:106] Iteration 20500, lr = 0.001
I0312 10:49:45.228108  7012 solver.cpp:228] Iteration 20600, loss = 0.00494742
I0312 10:49:45.228108  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:45.228108  7012 solver.cpp:244]     Train net output #1: loss = 0.00494734 (* 1 = 0.00494734 loss)
I0312 10:49:45.228108  7012 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I0312 10:49:49.857476  7012 solver.cpp:228] Iteration 20700, loss = 0.00467803
I0312 10:49:49.857476  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:49.857476  7012 solver.cpp:244]     Train net output #1: loss = 0.00467795 (* 1 = 0.00467795 loss)
I0312 10:49:49.857476  7012 sgd_solver.cpp:106] Iteration 20700, lr = 0.001
I0312 10:49:54.471112  7012 solver.cpp:228] Iteration 20800, loss = 0.0027041
I0312 10:49:54.471112  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:54.471112  7012 solver.cpp:244]     Train net output #1: loss = 0.00270402 (* 1 = 0.00270402 loss)
I0312 10:49:54.471112  7012 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I0312 10:49:59.085882  7012 solver.cpp:228] Iteration 20900, loss = 0.00349087
I0312 10:49:59.085882  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:49:59.085882  7012 solver.cpp:244]     Train net output #1: loss = 0.00349078 (* 1 = 0.00349078 loss)
I0312 10:49:59.085882  7012 sgd_solver.cpp:106] Iteration 20900, lr = 0.001
I0312 10:50:03.718740  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_21000.caffemodel
I0312 10:50:03.729230  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_21000.solverstate
I0312 10:50:03.732730  7012 solver.cpp:337] Iteration 21000, Testing net (#0)
I0312 10:50:03.733232  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:50:05.875990  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:50:05.875990  7012 solver.cpp:404]     Test net output #1: loss = 0.0136987 (* 1 = 0.0136987 loss)
I0312 10:50:05.891623  7012 solver.cpp:228] Iteration 21000, loss = 0.00277325
I0312 10:50:05.891623  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:05.891623  7012 solver.cpp:244]     Train net output #1: loss = 0.00277317 (* 1 = 0.00277317 loss)
I0312 10:50:05.891623  7012 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I0312 10:50:10.517757  7012 solver.cpp:228] Iteration 21100, loss = 0.0049637
I0312 10:50:10.517757  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:10.517757  7012 solver.cpp:244]     Train net output #1: loss = 0.00496362 (* 1 = 0.00496362 loss)
I0312 10:50:10.517757  7012 sgd_solver.cpp:106] Iteration 21100, lr = 0.001
I0312 10:50:15.138974  7012 solver.cpp:228] Iteration 21200, loss = 0.00492601
I0312 10:50:15.138974  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:15.138974  7012 solver.cpp:244]     Train net output #1: loss = 0.00492594 (* 1 = 0.00492594 loss)
I0312 10:50:15.138974  7012 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I0312 10:50:19.757287  7012 solver.cpp:228] Iteration 21300, loss = 0.00468659
I0312 10:50:19.757287  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:19.757287  7012 solver.cpp:244]     Train net output #1: loss = 0.00468651 (* 1 = 0.00468651 loss)
I0312 10:50:19.757287  7012 sgd_solver.cpp:106] Iteration 21300, lr = 0.001
I0312 10:50:24.372097  7012 solver.cpp:228] Iteration 21400, loss = 0.0027078
I0312 10:50:24.372097  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:24.372592  7012 solver.cpp:244]     Train net output #1: loss = 0.00270772 (* 1 = 0.00270772 loss)
I0312 10:50:24.372592  7012 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I0312 10:50:28.987164  7012 solver.cpp:228] Iteration 21500, loss = 0.00348088
I0312 10:50:28.987678  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:28.987678  7012 solver.cpp:244]     Train net output #1: loss = 0.0034808 (* 1 = 0.0034808 loss)
I0312 10:50:28.987678  7012 sgd_solver.cpp:106] Iteration 21500, lr = 0.001
I0312 10:50:33.587388  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_21600.caffemodel
I0312 10:50:33.598387  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_21600.solverstate
I0312 10:50:33.601888  7012 solver.cpp:337] Iteration 21600, Testing net (#0)
I0312 10:50:33.601888  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:50:35.757485  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9964
I0312 10:50:35.757485  7012 solver.cpp:404]     Test net output #1: loss = 0.0137122 (* 1 = 0.0137122 loss)
I0312 10:50:35.759485  7012 solver.cpp:228] Iteration 21600, loss = 0.00277025
I0312 10:50:35.759485  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:35.775113  7012 solver.cpp:244]     Train net output #1: loss = 0.00277017 (* 1 = 0.00277017 loss)
I0312 10:50:35.775113  7012 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I0312 10:50:40.390993  7012 solver.cpp:228] Iteration 21700, loss = 0.00493676
I0312 10:50:40.390993  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:40.390993  7012 solver.cpp:244]     Train net output #1: loss = 0.00493669 (* 1 = 0.00493669 loss)
I0312 10:50:40.390993  7012 sgd_solver.cpp:106] Iteration 21700, lr = 0.001
I0312 10:50:45.016501  7012 solver.cpp:228] Iteration 21800, loss = 0.0049009
I0312 10:50:45.016501  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:45.016501  7012 solver.cpp:244]     Train net output #1: loss = 0.00490082 (* 1 = 0.00490082 loss)
I0312 10:50:45.016501  7012 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I0312 10:50:49.629559  7012 solver.cpp:228] Iteration 21900, loss = 0.00468242
I0312 10:50:49.629559  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:49.629559  7012 solver.cpp:244]     Train net output #1: loss = 0.00468234 (* 1 = 0.00468234 loss)
I0312 10:50:49.629559  7012 sgd_solver.cpp:106] Iteration 21900, lr = 0.001
I0312 10:50:54.285183  7012 solver.cpp:228] Iteration 22000, loss = 0.00271438
I0312 10:50:54.285183  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:54.285183  7012 solver.cpp:244]     Train net output #1: loss = 0.0027143 (* 1 = 0.0027143 loss)
I0312 10:50:54.285183  7012 sgd_solver.cpp:46] MultiStep Status: Iteration 22000, step = 3
I0312 10:50:54.285183  7012 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0312 10:50:58.970734  7012 solver.cpp:228] Iteration 22100, loss = 0.00348025
I0312 10:50:58.970734  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:50:58.970734  7012 solver.cpp:244]     Train net output #1: loss = 0.00348017 (* 1 = 0.00348017 loss)
I0312 10:50:58.970734  7012 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0312 10:51:03.609380  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_22200.caffemodel
I0312 10:51:03.625020  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_22200.solverstate
I0312 10:51:03.625020  7012 solver.cpp:337] Iteration 22200, Testing net (#0)
I0312 10:51:03.625020  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:51:05.812575  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:51:05.812575  7012 solver.cpp:404]     Test net output #1: loss = 0.013721 (* 1 = 0.013721 loss)
I0312 10:51:05.828200  7012 solver.cpp:228] Iteration 22200, loss = 0.00274374
I0312 10:51:05.828200  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:05.828200  7012 solver.cpp:244]     Train net output #1: loss = 0.00274366 (* 1 = 0.00274366 loss)
I0312 10:51:05.828200  7012 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0312 10:51:10.456241  7012 solver.cpp:228] Iteration 22300, loss = 0.00476537
I0312 10:51:10.456241  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:10.456241  7012 solver.cpp:244]     Train net output #1: loss = 0.00476529 (* 1 = 0.00476529 loss)
I0312 10:51:10.456241  7012 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0312 10:51:15.102548  7012 solver.cpp:228] Iteration 22400, loss = 0.00484092
I0312 10:51:15.102548  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:15.102548  7012 solver.cpp:244]     Train net output #1: loss = 0.00484084 (* 1 = 0.00484084 loss)
I0312 10:51:15.102548  7012 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0312 10:51:19.772897  7012 solver.cpp:228] Iteration 22500, loss = 0.0045421
I0312 10:51:19.772897  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:19.772897  7012 solver.cpp:244]     Train net output #1: loss = 0.00454202 (* 1 = 0.00454202 loss)
I0312 10:51:19.772897  7012 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I0312 10:51:24.433061  7012 solver.cpp:228] Iteration 22600, loss = 0.00271444
I0312 10:51:24.433061  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:24.433061  7012 solver.cpp:244]     Train net output #1: loss = 0.00271436 (* 1 = 0.00271436 loss)
I0312 10:51:24.433061  7012 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I0312 10:51:29.115332  7012 solver.cpp:228] Iteration 22700, loss = 0.00347634
I0312 10:51:29.115332  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:29.115332  7012 solver.cpp:244]     Train net output #1: loss = 0.00347626 (* 1 = 0.00347626 loss)
I0312 10:51:29.115332  7012 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I0312 10:51:33.855320  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_22800.caffemodel
I0312 10:51:33.867804  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_22800.solverstate
I0312 10:51:33.871803  7012 solver.cpp:337] Iteration 22800, Testing net (#0)
I0312 10:51:33.871803  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:51:36.077306  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:51:36.077306  7012 solver.cpp:404]     Test net output #1: loss = 0.013716 (* 1 = 0.013716 loss)
I0312 10:51:36.096303  7012 solver.cpp:228] Iteration 22800, loss = 0.00274613
I0312 10:51:36.096303  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:36.096303  7012 solver.cpp:244]     Train net output #1: loss = 0.00274605 (* 1 = 0.00274605 loss)
I0312 10:51:36.096303  7012 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I0312 10:51:40.846971  7012 solver.cpp:228] Iteration 22900, loss = 0.00476731
I0312 10:51:40.846971  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:40.846971  7012 solver.cpp:244]     Train net output #1: loss = 0.00476722 (* 1 = 0.00476722 loss)
I0312 10:51:40.846971  7012 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I0312 10:51:45.682454  7012 solver.cpp:228] Iteration 23000, loss = 0.00484297
I0312 10:51:45.682454  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:45.682454  7012 solver.cpp:244]     Train net output #1: loss = 0.00484289 (* 1 = 0.00484289 loss)
I0312 10:51:45.682454  7012 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I0312 10:51:50.431171  7012 solver.cpp:228] Iteration 23100, loss = 0.00455214
I0312 10:51:50.431668  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:50.431668  7012 solver.cpp:244]     Train net output #1: loss = 0.00455206 (* 1 = 0.00455206 loss)
I0312 10:51:50.431668  7012 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I0312 10:51:55.179930  7012 solver.cpp:228] Iteration 23200, loss = 0.00271436
I0312 10:51:55.179930  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:55.180430  7012 solver.cpp:244]     Train net output #1: loss = 0.00271428 (* 1 = 0.00271428 loss)
I0312 10:51:55.180430  7012 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I0312 10:51:59.924479  7012 solver.cpp:228] Iteration 23300, loss = 0.00347285
I0312 10:51:59.924974  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:51:59.924974  7012 solver.cpp:244]     Train net output #1: loss = 0.00347277 (* 1 = 0.00347277 loss)
I0312 10:51:59.924974  7012 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I0312 10:52:04.647159  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_23400.caffemodel
I0312 10:52:04.659144  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_23400.solverstate
I0312 10:52:04.662655  7012 solver.cpp:337] Iteration 23400, Testing net (#0)
I0312 10:52:04.662655  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:52:06.858641  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:52:06.858641  7012 solver.cpp:404]     Test net output #1: loss = 0.0137127 (* 1 = 0.0137127 loss)
I0312 10:52:06.876641  7012 solver.cpp:228] Iteration 23400, loss = 0.0027479
I0312 10:52:06.876641  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:06.876641  7012 solver.cpp:244]     Train net output #1: loss = 0.00274782 (* 1 = 0.00274782 loss)
I0312 10:52:06.876641  7012 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I0312 10:52:11.624682  7012 solver.cpp:228] Iteration 23500, loss = 0.00476832
I0312 10:52:11.624682  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:11.624682  7012 solver.cpp:244]     Train net output #1: loss = 0.00476824 (* 1 = 0.00476824 loss)
I0312 10:52:11.624682  7012 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I0312 10:52:16.374002  7012 solver.cpp:228] Iteration 23600, loss = 0.00484416
I0312 10:52:16.374002  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:16.374002  7012 solver.cpp:244]     Train net output #1: loss = 0.00484408 (* 1 = 0.00484408 loss)
I0312 10:52:16.374002  7012 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I0312 10:52:21.118731  7012 solver.cpp:228] Iteration 23700, loss = 0.00455969
I0312 10:52:21.118731  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:21.118731  7012 solver.cpp:244]     Train net output #1: loss = 0.00455961 (* 1 = 0.00455961 loss)
I0312 10:52:21.118731  7012 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I0312 10:52:25.864712  7012 solver.cpp:228] Iteration 23800, loss = 0.00271424
I0312 10:52:25.864712  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:25.864712  7012 solver.cpp:244]     Train net output #1: loss = 0.00271415 (* 1 = 0.00271415 loss)
I0312 10:52:25.864712  7012 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I0312 10:52:30.610425  7012 solver.cpp:228] Iteration 23900, loss = 0.0034698
I0312 10:52:30.610425  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:30.610425  7012 solver.cpp:244]     Train net output #1: loss = 0.00346972 (* 1 = 0.00346972 loss)
I0312 10:52:30.610425  7012 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I0312 10:52:35.333076  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_24000.caffemodel
I0312 10:52:35.344079  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_24000.solverstate
I0312 10:52:35.348079  7012 solver.cpp:337] Iteration 24000, Testing net (#0)
I0312 10:52:35.348079  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:52:37.545567  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:52:37.545567  7012 solver.cpp:404]     Test net output #1: loss = 0.0137101 (* 1 = 0.0137101 loss)
I0312 10:52:37.563565  7012 solver.cpp:228] Iteration 24000, loss = 0.00274943
I0312 10:52:37.563565  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:37.563565  7012 solver.cpp:244]     Train net output #1: loss = 0.00274935 (* 1 = 0.00274935 loss)
I0312 10:52:37.563565  7012 sgd_solver.cpp:106] Iteration 24000, lr = 0.0001
I0312 10:52:42.303841  7012 solver.cpp:228] Iteration 24100, loss = 0.00476893
I0312 10:52:42.303841  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:42.303841  7012 solver.cpp:244]     Train net output #1: loss = 0.00476885 (* 1 = 0.00476885 loss)
I0312 10:52:42.303841  7012 sgd_solver.cpp:106] Iteration 24100, lr = 0.0001
I0312 10:52:47.042923  7012 solver.cpp:228] Iteration 24200, loss = 0.00484448
I0312 10:52:47.042923  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:47.042923  7012 solver.cpp:244]     Train net output #1: loss = 0.0048444 (* 1 = 0.0048444 loss)
I0312 10:52:47.042923  7012 sgd_solver.cpp:106] Iteration 24200, lr = 0.0001
I0312 10:52:51.789556  7012 solver.cpp:228] Iteration 24300, loss = 0.0045658
I0312 10:52:51.789556  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:51.789556  7012 solver.cpp:244]     Train net output #1: loss = 0.00456572 (* 1 = 0.00456572 loss)
I0312 10:52:51.789556  7012 sgd_solver.cpp:106] Iteration 24300, lr = 0.0001
I0312 10:52:56.526739  7012 solver.cpp:228] Iteration 24400, loss = 0.00271413
I0312 10:52:56.526739  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:52:56.526739  7012 solver.cpp:244]     Train net output #1: loss = 0.00271405 (* 1 = 0.00271405 loss)
I0312 10:52:56.526739  7012 sgd_solver.cpp:106] Iteration 24400, lr = 0.0001
I0312 10:53:01.270848  7012 solver.cpp:228] Iteration 24500, loss = 0.00346703
I0312 10:53:01.270848  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:53:01.270848  7012 solver.cpp:244]     Train net output #1: loss = 0.00346695 (* 1 = 0.00346695 loss)
I0312 10:53:01.270848  7012 sgd_solver.cpp:106] Iteration 24500, lr = 0.0001
I0312 10:53:06.000298  7012 solver.cpp:454] Snapshotting to binary proto file examples/mnist/snaps/lenet_iter_24600.caffemodel
I0312 10:53:06.012270  7012 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/snaps/lenet_iter_24600.solverstate
I0312 10:53:06.017284  7012 solver.cpp:337] Iteration 24600, Testing net (#0)
I0312 10:53:06.017284  7012 net.cpp:693] Ignoring source layer accuracy_training
I0312 10:53:08.213270  7012 solver.cpp:404]     Test net output #0: accuracy = 0.9963
I0312 10:53:08.213270  7012 solver.cpp:404]     Test net output #1: loss = 0.0137077 (* 1 = 0.0137077 loss)
I0312 10:53:08.231269  7012 solver.cpp:228] Iteration 24600, loss = 0.00275056
I0312 10:53:08.231269  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:53:08.231269  7012 solver.cpp:244]     Train net output #1: loss = 0.00275048 (* 1 = 0.00275048 loss)
I0312 10:53:08.231269  7012 sgd_solver.cpp:106] Iteration 24600, lr = 0.0001
I0312 10:53:12.985409  7012 solver.cpp:228] Iteration 24700, loss = 0.00476872
I0312 10:53:12.985409  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:53:12.985409  7012 solver.cpp:244]     Train net output #1: loss = 0.00476864 (* 1 = 0.00476864 loss)
I0312 10:53:12.985409  7012 sgd_solver.cpp:106] Iteration 24700, lr = 0.0001
I0312 10:53:17.759517  7012 solver.cpp:228] Iteration 24800, loss = 0.00484412
I0312 10:53:17.759517  7012 solver.cpp:244]     Train net output #0: accuracy_training = 1
I0312 10:53:17.759517  7012 solver.cpp:244]     Train net output #1: loss = 0.00484404 (* 1 = 0.00484404 loss)
I0312 10:53:17.759517  7012 sgd_solver.cpp:106] Iteration 24800, lr = 0.0001
I0312 10:53:22.600214  7012 solver.cpp:228] Iteration 24900, loss = 0.00457051
I0312 10:53:22.600214  7012 solver.cpp:244]     Trai